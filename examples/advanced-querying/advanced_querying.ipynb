<cell id="cell-0"><cell_type>markdown</cell_type># Advanced Querying

This notebook demonstrates advanced querying capabilities in PyDala2, including complex filtering, SQL integration, and performance optimization techniques.</cell id="cell-0">
<cell id="cell-1">import os
import tempfile
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.dataset as pds
import duckdb
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union

# Import PyDala2 components
from pydala.dataset import ParquetDataset
from pydala.table import PydalaTable
from pydala.catalog import Catalog
from pydala.helpers.sql import sql2pyarrow_filter</cell id="cell-1">
<cell id="cell-2"><cell_type>markdown</cell_type>## Create Sample Data

Let's create complex sales data for advanced querying examples.</cell id="cell-2">
<cell id="cell-3">def create_complex_sales_data():
    """Create complex sales data for advanced querying examples."""
    np.random.seed(42)

    # Generate realistic sales data
    n_records = 10000
    start_date = datetime(2023, 1, 1)

    data = {
        'sale_id': range(1, n_records + 1),
        'customer_id': np.random.randint(1, 1001, n_records),
        'product_id': np.random.randint(1, 501, n_records),
        'store_id': np.random.randint(1, 51, n_records),
        'sale_date': [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_records)],
        'sale_amount': np.random.uniform(10, 1000, n_records).round(2),
        'quantity': np.random.randint(1, 10, n_records),
        'discount_percent': np.random.uniform(0, 30, n_records).round(1),
        'payment_method': np.random.choice(['Credit Card', 'Cash', 'Debit Card', 'Digital Wallet'], n_records),
        'sales_rep_id': np.random.randint(1, 101, n_records),
        'region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_records),
        'is_online': np.random.choice([True, False], n_records, p=[0.3, 0.7]),
        'customer_segment': np.random.choice(['Premium', 'Standard', 'Basic'], n_records, p=[0.2, 0.5, 0.3]),
        'return_flag': np.random.choice([0, 1], n_records, p=[0.95, 0.05])  # 5% return rate
    }

    df = pd.DataFrame(data)

    # Calculate derived fields
    df['net_amount'] = df['sale_amount'] * (1 - df['discount_percent'] / 100) * df['quantity']
    df['month'] = df['sale_date'].dt.month
    df['quarter'] = df['sale_date'].dt.quarter
    df['year'] = df['sale_date'].dt.year
    df['day_of_week'] = df['sale_date'].dt.dayofweek

    return df

def create_product_dimension_data():
    """Create product dimension data."""
    np.random.seed(42)

    categories = ['Electronics', 'Clothing', 'Home', 'Sports', 'Books', 'Toys', 'Beauty', 'Food']

    data = {
        'product_id': range(1, 501),
        'product_name': [f'Product_{i}' for i in range(1, 501)],
        'category': np.random.choice(categories, 500),
        'base_price': np.random.uniform(5, 500, 500).round(2),
        'cost': np.random.uniform(2, 250, 500).round(2),
        'supplier_id': np.random.randint(1, 51, 500),
        'launch_date': [datetime(2022, 1, 1) + timedelta(days=np.random.randint(0, 365)) for _ in range(500)],
        'is_active': np.random.choice([True, False], 500, p=[0.9, 0.1])
    }

    return pd.DataFrame(data)

def create_customer_dimension_data():
    """Create customer dimension data."""
    np.random.seed(42)

    data = {
        'customer_id': range(1, 1001),
        'customer_name': [f'Customer_{i}' for i in range(1, 1001)],
        'age': np.random.randint(18, 80, 1000),
        'gender': np.random.choice(['M', 'F'], 1000),
        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], 1000),
        'state': np.random.choice(['NY', 'CA', 'IL', 'TX', 'AZ'], 1000),
        'registration_date': [datetime(2020, 1, 1) + timedelta(days=np.random.randint(0, 1095)) for _ in range(1000)],
        'total_purchases': np.random.randint(0, 100, 1000),
        'avg_order_value': np.random.uniform(25, 500, 1000).round(2)
    }

    return pd.DataFrame(data)

# Create sample data
df_sales = create_complex_sales_data()
df_products = create_product_dimension_data()
df_customers = create_customer_dimension_data()

print(f"Created sales data with {len(df_sales)} records")
print(f"Created products data with {len(df_products)} records")
print(f"Created customers data with {len(df_customers)} records")

df_sales.head()</cell id="cell-3">
<cell id="cell-4"><cell_type>markdown</cell_type>## Example 1: Complex Filtering

Demonstrates complex filtering operations with multiple conditions.</cell id="cell-4">
<cell id="cell-5"># Create temporary directory
temp_dir = tempfile.mkdtemp()
temp_path = Path(temp_dir)

# Create dataset
ds_sales = ParquetDataset.from_pandas(
    df_sales,
    path=temp_path / "sales_data",
    partition_cols=['region', 'year', 'quarter']
)

# Convert to table for querying
table = ds_sales.to_table()</cell id="cell-5">
<cell id="cell-6"># Basic Filtering
print("1.1 Basic Filtering:")
# Simple filters
high_value = table.filter(table.sale_amount > 500)
print(f"Sales over $500: {len(high_value)} records")

online_sales = table.filter(table.is_online == True)
print(f"Online sales: {len(online_sales)} records")</cell id="cell-6">
<cell id="cell-7"># Compound Conditions
print("\n1.2 Compound Conditions:")
# Multiple conditions
complex_filter = table.filter(
    (table.sale_amount > 200) &
    (table.region == 'North') &
    (table.customer_segment == 'Premium') &
    (table.return_flag == 0)
)
print(f"Complex filter result: {len(complex_filter)} records")

# Show sample results
complex_df = complex_filter.to_pandas()
print("Sample of complex filter results:")
complex_df[['sale_amount', 'region', 'customer_segment', 'payment_method']].head()</cell id="cell-7">
<cell id="cell-8"># Date Range Filtering
print("\n1.3 Date Range Filtering:")
# Date filtering
date_filter = table.filter(
    (table.sale_date >= '2023-06-01') &
    (table.sale_date <= '2023-08-31')
)
print(f"Q3 2023 sales: {len(date_filter)} records")

# String Operations
credit_card_sales = table.filter(table.payment_method == 'Credit Card')
print(f"Credit card sales: {len(credit_card_sales)} records")

# Numeric Range Filtering
mid_range_sales = table.filter(
    (table.sale_amount >= 100) &
    (table.sale_amount <= 300)
)
print(f"Sales between $100-$300: {len(mid_range_sales)} records")</cell id="cell-8">
<cell id="cell-9"><cell_type>markdown</cell_type>## Example 2: SQL Integration

Demonstrates SQL integration with DuckDB for powerful queries.</cell id="cell-9">
<cell id="cell-10"># Create datasets for SQL analysis
ds_products = ParquetDataset.from_pandas(df_products, path=temp_path / "products")
ds_customers = ParquetDataset.from_pandas(df_customers, path=temp_path / "customers")

# Connect to DuckDB
con = duckdb.connect()

# Register datasets as DuckDB tables
con.register('sales', df_sales)
con.register('products', df_products)
con.register('customers', df_customers)</cell id="cell-10">
<cell id="cell-11"># Basic SQL Queries
print("2.1 Basic SQL Queries:")

# Basic aggregation
result = con.execute("""
    SELECT
        region,
        COUNT(*) as total_sales,
        SUM(sale_amount) as total_revenue,
        AVG(sale_amount) as avg_sale_amount
    FROM sales
    GROUP BY region
    ORDER BY total_revenue DESC
""").fetchdf()

print("Sales by region:")
result</cell id="cell-11">
<cell id="cell-12"># Complex SQL with Joins
print("2.2 Complex SQL with Joins:")

# Join with dimensions
join_result = con.execute("""
    SELECT
        p.category,
        c.city,
        COUNT(*) as sales_count,
        SUM(s.sale_amount) as total_revenue,
        AVG(s.sale_amount) as avg_sale
    FROM sales s
    JOIN products p ON s.product_id = p.product_id
    JOIN customers c ON s.customer_id = c.customer_id
    WHERE s.sale_date >= '2023-07-01'
    GROUP BY p.category, c.city
    HAVING COUNT(*) > 10
    ORDER BY total_revenue DESC
    LIMIT 10
""").fetchdf()

print("Top category-city combinations (July+ 2023):")
join_result</cell id="cell-12">
<cell id="cell-13"># Window Functions
print("2.3 Window Functions:")

# Window functions
window_result = con.execute("""
    SELECT
        customer_id,
        COUNT(*) as purchase_count,
        SUM(sale_amount) as total_spent,
        AVG(sale_amount) as avg_purchase,
        RANK() OVER (ORDER BY SUM(sale_amount) DESC) as customer_rank
    FROM sales
    GROUP BY customer_id
    ORDER BY total_spent DESC
    LIMIT 10
""").fetchdf()

print("Top 10 customers by total spend:")
window_result</cell id="cell-13">
<cell id="cell-14"># Time Series Analysis
print("2.4 Time Series Analysis:")

# Time-based analysis
time_result = con.execute("""
    SELECT
        DATE_TRUNC('month', sale_date) as month,
        COUNT(*) as sales_count,
        SUM(sale_amount) as monthly_revenue,
        AVG(sale_amount) as avg_sale_amount,
        COUNT(DISTINCT customer_id) as unique_customers
    FROM sales
    GROUP BY DATE_TRUNC('month', sale_date)
    ORDER BY month
""").fetchdf()

print("Monthly sales summary:")
time_result</cell id="cell-14">
<cell id="cell-15"><cell_type>markdown</cell_type>## Example 3: Advanced Aggregations

Demonstrates advanced aggregation techniques and analytics.</cell id="cell-15">
<cell id="cell-16"># Convert to pandas for complex aggregations
df = table.to_pandas()

# Multi-level Grouping
print("3.1 Multi-level Grouping:")

multi_level = df.groupby(['region', 'customer_segment', 'payment_method']).agg({
    'sale_amount': ['sum', 'mean', 'count'],
    'quantity': ['sum', 'mean'],
    'customer_id': 'nunique'
}).round(2)

print("Multi-level aggregation:")
multi_level.head(10)</cell id="cell-16">
<cell id="cell-17"># Pivot Tables
print("3.2 Pivot Tables:")

# Pivot table
pivot_result = df.pivot_table(
    values='sale_amount',
    index='region',
    columns='customer_segment',
    aggfunc='sum',
    fill_value=0
).round(2)

print("Regional sales by customer segment:")
pivot_result</cell id="cell-17">
<cell id="cell-18"># Rolling Aggregations
print("3.3 Rolling Aggregations:")

# Time-based rolling calculations
df_sorted = df.sort_values('sale_date')
df_sorted['rolling_avg_7d'] = df_sorted['sale_amount'].rolling(window=7, min_periods=1).mean()
df_sorted['rolling_sum_30d'] = df_sorted['sale_amount'].rolling(window=30, min_periods=1).sum()

rolling_result = df_sorted.groupby(df_sorted['sale_date'].dt.to_period('M')).agg({
    'sale_amount': 'sum',
    'rolling_avg_7d': 'mean',
    'rolling_sum_30d': 'mean'
}).round(2)

print("Monthly rolling averages:")
rolling_result.head()</cell id="cell-18">
<cell id="cell-19"># Percentile Analysis
print("3.4 Percentile Analysis:")

# Percentile calculations
percentile_result = df.groupby('region')['sale_amount'].agg([
    'count', 'min', 'max', 'mean', 'median',
    ('p25', lambda x: x.quantile(0.25)),
    ('p75', lambda x: x.quantile(0.75)),
    ('p90', lambda x: x.quantile(0.90)),
    ('p95', lambda x: x.quantile(0.95))
]).round(2)

print("Regional sales percentiles:")
percentile_result</cell id="cell-19">
<cell id="cell-20"><cell_type>markdown</cell_type>## Example 4: Performance Optimization

Demonstrates performance optimization techniques.</cell id="cell-20">
<cell id="cell-21"># Column Pruning
print("4.1 Column Pruning:")

# Select only needed columns
start_time = datetime.now()
selected = table.select(['customer_id', 'sale_amount', 'region', 'customer_segment'])
end_time = datetime.now()
print(f"Column selection time: {(end_time - start_time).total_seconds():.3f} seconds")
print(f"Selected {len(selected.columns)} columns from {len(table.columns)}")</cell id="cell-21">
<cell id="cell-22"># Partition Pruning
print("4.2 Partition Pruning:")

# Filter on partition columns
start_time = datetime.now()
north_region = table.filter(table.region == 'North')
end_time = datetime.now()
print(f"Partition filter time: {(end_time - start_time).total_seconds():.3f} seconds")
print(f"North region records: {len(north_region)}")</cell id="cell-22">
<cell id="cell-23"># Efficient Scanning
print("4.3 Efficient Scanning:")

# Use PyArrow scanner for efficient reading
start_time = datetime.now()
scanner = table.to_arrow_scanner(
    columns=['customer_id', 'sale_amount', 'region'],
    filter=pa.dataset.field('sale_amount') > 100
)

# Read first batch
first_batch = next(scanner.to_batches())
end_time = datetime.now()
print(f"Scanner setup time: {(end_time - start_time).total_seconds():.3f} seconds")
print(f"First batch size: {first_batch.num_rows}")</cell id="cell-23">
<cell id="cell-24"># Batch Processing
print("4.4 Batch Processing:")

# Process data in batches
batch_size = 1000
total_rows = 0
batch_count = 0

start_time = datetime.now()
for batch in scanner.to_batches():
    batch_count += 1
    total_rows += batch.num_rows
    if batch_count >= 5:  # Process first 5 batches
        break

end_time = datetime.now()
print(f"Processed {batch_count} batches with {total_rows} total rows")
print(f"Batch processing time: {(end_time - start_time).total_seconds():.3f} seconds")</cell id="cell-24">
<cell id="cell-25"># Predicate Pushdown
print("4.5 Predicate Pushdown:")

# Combine multiple filters efficiently
start_time = datetime.now()
complex_filter = table.filter(
    (table.region == 'North') &
    (table.sale_amount > 200) &
    (table.customer_segment == 'Premium') &
    (table.is_online == True)
)
end_time = datetime.now()
print(f"Complex filter time: {(end_time - start_time).total_seconds():.3f} seconds")
print(f"Complex filter result: {len(complex_filter)} records")</cell id="cell-25">
<cell id="cell-26"><cell_type>markdown</cell_type>## Example 5: Multi-Dataset Queries

Demonstrates querying across multiple datasets.</cell id="cell-26">
<cell id="cell-27"># Create catalog for multi-dataset queries
catalog_path = temp_path / "catalog.yaml"
catalog = Catalog(catalog_path)

# Register datasets
catalog.register_dataset("sales", ds_sales)
catalog.register_dataset("products", ds_products)
catalog.register_dataset("customers", ds_customers)

print(f"Created catalog with {len(catalog.list_datasets())} datasets")</cell id="cell-27">
<cell id="cell-28"># Load datasets for analysis
sales_table = ds_sales.to_table()
products_table = ds_products.to_table()
customers_table = ds_customers.to_table()

# Convert to pandas for joining
sales_df = sales_table.to_pandas()
products_df = products_table.to_pandas()
customers_df = customers_table.to_pandas()

print(f"Sales dataset: {sales_df.shape}")
print(f"Products dataset: {products_df.shape}")
print(f"Customers dataset: {customers_df.shape}")</cell id="cell-28">
<cell id="cell-29"># Manual Joins
print("5.2 Manual Joins:")

# Join sales with products
sales_products = sales_df.merge(
    products_df[['product_id', 'category', 'base_price']],
    on='product_id',
    how='left'
)

# Join with customers
full_data = sales_products.merge(
    customers_df[['customer_id', 'age', 'city', 'customer_segment']],
    on='customer_id',
    how='left'
)

print(f"Joined dataset shape: {full_data.shape}")
print("Sample of joined data:")
full_data[['sale_amount', 'category', 'age', 'city']].head()</cell id="cell-29">
<cell id="cell-30"># Cross-Dataset Filtering
print("5.3 Cross-Dataset Filtering:")

# Example: High-value electronics purchases by premium customers
high_value_electronics = full_data[
    (full_data['category'] == 'Electronics') &
    (full_data['customer_segment'] == 'Premium') &
    (full_data['sale_amount'] > 500)
]

print(f"High-value electronics by premium customers: {len(high_value_electronics)} records")
print(f"Average sale amount: ${high_value_electronics['sale_amount'].mean():.2f}")

high_value_electronics[['sale_amount', 'category', 'age', 'city']].head()</cell id="cell-30">
<cell id="cell-31"># Aggregated Analysis
print("5.4 Aggregated Analysis:")

# Multi-dataset aggregation
category_segment_analysis = full_data.groupby(['category', 'customer_segment']).agg({
    'sale_amount': ['sum', 'mean', 'count'],
    'customer_id': 'nunique',
    'age': 'mean'
}).round(2)

print("Category-Customer Segment Analysis:")
category_segment_analysis.head(10)</cell id="cell-31">
<cell id="cell-32"># Time-based Cross-Dataset Analysis
print("5.5 Time-based Cross-Dataset Analysis:")

# Add time-based analysis
full_data['month'] = full_data['sale_date'].dt.to_period('M')
monthly_trends = full_data.groupby(['month', 'category']).agg({
    'sale_amount': 'sum',
    'customer_id': 'nunique'
}).round(2)

print("Monthly category trends:")
monthly_trends.head(10)</cell id="cell-32">
<cell id="cell-33"><cell_type>markdown</cell_type>## Example 6: Advanced Analytics

Demonstrates advanced analytics and insights.</cell id="cell-33">
<cell id="cell-34"># Customer Analytics
print("6.1 Customer Analytics:")

# Customer lifetime value calculation
customer_ltv = df.groupby('customer_id').agg({
    'sale_amount': 'sum',
    'quantity': 'sum',
    'sale_date': ['min', 'max', 'count']
}).round(2)

# Flatten multi-level columns
customer_ltv.columns = ['total_spent', 'total_quantity', 'first_purchase', 'last_purchase', 'purchase_count']
customer_ltv['customer_lifetime_days'] = (customer_ltv['last_purchase'] - customer_ltv['first_purchase']).dt.days
customer_ltv['avg_purchase_value'] = customer_ltv['total_spent'] / customer_ltv['purchase_count']

print("Customer LTV summary:")
customer_ltv.describe()</cell id="cell-34">
<cell id="cell-35"># Product Performance
print("6.2 Product Performance:")

# Product performance analysis
product_performance = df.groupby('product_id').agg({
    'sale_amount': 'sum',
    'quantity': 'sum',
    'customer_id': 'nunique',
    'sale_id': 'count'
}).round(2)

product_performance.columns = ['total_revenue', 'total_quantity', 'unique_customers', 'total_transactions']
product_performance['avg_transaction_value'] = product_performance['total_revenue'] / product_performance['total_transactions']

print("Top 10 products by revenue:")
product_performance.sort_values('total_revenue', ascending=False).head(10)</cell id="cell-35">
<cell id="cell-36"># Seasonal Analysis
print("6.3 Seasonal Analysis:")

# Seasonal trends
df['month'] = df['sale_date'].dt.month
df['quarter'] = df['sale_date'].dt.quarter
df['day_of_week'] = df['sale_date'].dt.dayofweek

seasonal_analysis = df.groupby('month').agg({
    'sale_amount': 'sum',
    'sale_id': 'count',
    'customer_id': 'nunique'
}).round(2)

seasonal_analysis.columns = ['monthly_revenue', 'transaction_count', 'unique_customers']
seasonal_analysis['avg_transaction_value'] = seasonal_analysis['monthly_revenue'] / seasonal_analysis['transaction_count']

print("Monthly seasonal patterns:")
seasonal_analysis</cell id="cell-36">
<cell id="cell-37"># Geographic Analysis
print("6.4 Geographic Analysis:")

# Regional performance
geo_analysis = df.groupby('region').agg({
    'sale_amount': 'sum',
    'customer_id': 'nunique',
    'sale_id': 'count',
    'quantity': 'sum'
}).round(2)

geo_analysis.columns = ['total_revenue', 'unique_customers', 'transaction_count', 'total_quantity']
geo_analysis['avg_order_value'] = geo_analysis['total_revenue'] / geo_analysis['transaction_count']
geo_analysis['avg_quantity'] = geo_analysis['total_quantity'] / geo_analysis['transaction_count']

print("Regional performance:")
geo_analysis.sort_values('total_revenue', ascending=False)</cell id="cell-37">
<cell id="cell-38"><cell_type>markdown</cell_type>## Summary

This notebook demonstrated comprehensive advanced querying capabilities in PyDala2.</cell id="cell-38">
<cell id="cell-39">print("Key Learnings:")
print("- Complex filtering with compound conditions")
print("- SQL integration with DuckDB for powerful queries")
print("- Advanced aggregations and analytics")
print("- Performance optimization techniques")
print("- Multi-dataset querying capabilities")
print("- Advanced analytics and insights")

print("\nBest Practices:")
print("- Use partition pruning for large datasets")
print("- Leverage column pruning to reduce data transfer")
print("- Use DuckDB for complex analytical queries")
print("- Implement efficient batch processing")
print("- Combine multiple datasets for comprehensive analysis")
print("- Apply performance optimization techniques")

# Clean up
con.close()</cell id="cell-39">
<cell id="cell-40"><cell_type>markdown</cell_type>## Cleanup

Remove temporary files</cell id="cell-40">
<cell id="cell-41">import shutil
shutil.rmtree(temp_dir)
print(f"Cleaned up temporary directory: {temp_dir}")</cell id="cell-41">
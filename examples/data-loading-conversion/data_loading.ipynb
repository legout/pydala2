{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Conversion\n",
    "\n",
    "This notebook demonstrates how to load data from various sources and convert between different formats using PyDala2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pac\n",
    "import pyarrow.json as paj\n",
    "\n",
    "# Import PyDala2 components\n",
    "from pydala.dataset import ParquetDataset, CsvDataset, JsonDataset\n",
    "from pydala.table import PydalaTable\n",
    "from pydala.catalog import Catalog\n",
    "\n",
    "# Optional imports\n",
    "try:\n",
    "    import polars as pl\n",
    "    POLARS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    POLARS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import duckdb\n",
    "    DUCKDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    DUCKDB_AVAILABLE = False\n",
    "\n",
    "print(f\"Library Availability:\")\n",
    "print(f\"  - Polars: {'✓' if POLARS_AVAILABLE else '✗'}\")\n",
    "print(f\"  - DuckDB: {'✓' if DUCKDB_AVAILABLE else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Creating Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_csv_data(path: Path, num_records: int = 1000):\n",
    "    \"\"\"Create sample CSV data for demonstration.\"\"\"\n",
    "    csv_file = path / \"sample_data.csv\"\n",
    "\n",
    "    # Generate sample data\n",
    "    records = []\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "\n",
    "    for i in range(num_records):\n",
    "        record = {\n",
    "            'id': i + 1,\n",
    "            'name': f'Product_{i % 100}',\n",
    "            'category': ['Electronics', 'Clothing', 'Food', 'Books'][i % 4],\n",
    "            'price': round((i % 100) * 1.5 + 10, 2),\n",
    "            'quantity': i % 50 + 1,\n",
    "            'date': (start_date + timedelta(days=i % 365)).strftime('%Y-%m-%d'),\n",
    "            'is_active': i % 2 == 0\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(csv_file, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=records[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(records)\n",
    "\n",
    "    return csv_file\n",
    "\n",
    "def create_sample_json_data(path: Path, num_records: int = 500):\n",
    "    \"\"\"Create sample JSON data for demonstration.\"\"\"\n",
    "    json_file = path / \"sample_data.json\"\n",
    "\n",
    "    # Generate sample nested data\n",
    "    records = []\n",
    "\n",
    "    for i in range(num_records):\n",
    "        record = {\n",
    "            'user_id': f'user_{i:04d}',\n",
    "            'profile': {\n",
    "                'name': f'User {i}',\n",
    "                'age': 20 + (i % 60),\n",
    "                'city': ['New York', 'London', 'Tokyo', 'Paris'][i % 4]\n",
    "            },\n",
    "            'orders': [\n",
    "                {\n",
    "                    'order_id': f'order_{i}_{j}',\n",
    "                    'amount': round((j + 1) * 25.5, 2),\n",
    "                    'items': j + 1\n",
    "                }\n",
    "                for j in range(i % 3 + 1)\n",
    "            ],\n",
    "            'registration_date': (datetime(2023, 1, 1) + timedelta(days=i)).isoformat(),\n",
    "            'active': True\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    # Write to JSON\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Loading from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "temp_path = Path(temp_dir)\n",
    "\n",
    "# Create sample CSV data\n",
    "csv_file = create_sample_csv_data(temp_path, 1000)\n",
    "print(f\"Created CSV file: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Direct CSV loading\n",
    "print(\"1.1 Direct CSV loading...\")\n",
    "csv_dataset = CsvDataset(csv_file)\n",
    "print(f\"Loaded {len(csv_dataset)} records from CSV\")\n",
    "print(f\"Schema: {csv_dataset.schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Convert CSV to Parquet\n",
    "print(\"\\n1.2 Converting CSV to Parquet...\")\n",
    "parquet_path = temp_path / \"converted_data\"\n",
    "parquet_dataset = ParquetDataset.from_csv(\n",
    "    csv_file,\n",
    "    path=parquet_path,\n",
    "    partition_cols=['category']\n",
    ")\n",
    "print(f\"Converted to Parquet with {len(parquet_dataset.files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Using PyDalaTable\n",
    "print(\"\\n1.3 Using PyDalaTable...\")\n",
    "table = csv_dataset.to_table()\n",
    "filtered = table.filter(table.price > 50)\n",
    "print(f\"Filtered to {len(filtered)} records with price > 50\")\n",
    "filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Loading from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample JSON data\n",
    "json_file = create_sample_json_data(temp_path, 500)\n",
    "print(f\"Created JSON file: {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "print(\"2.1 Loading JSON data...\")\n",
    "json_dataset = JsonDataset(json_file)\n",
    "print(f\"Loaded {len(json_dataset)} records from JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore nested structure\n",
    "print(\"\\n2.2 Exploring nested structure...\")\n",
    "table = json_dataset.to_table()\n",
    "print(f\"Columns: {table.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample of nested data\n",
    "import json\n",
    "with open(json_file, 'r') as f:\n",
    "    sample_record = json.load(f)[0]\n",
    "print(\"Sample JSON structure:\")\n",
    "print(json.dumps(sample_record, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Format Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV to Parquet\n",
    "print(\"3.1 CSV to Parquet...\")\n",
    "csv_ds = CsvDataset(csv_file)\n",
    "parquet_path = temp_path / \"parquet_output\"\n",
    "parquet_ds = ParquetDataset.from_dataset(\n",
    "    csv_ds,\n",
    "    path=parquet_path,\n",
    "    compression='zstd'\n",
    ")\n",
    "print(f\"Converted to Parquet with compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet to different formats\n",
    "print(\"\\n3.2 Parquet to other formats...\")\n",
    "table = parquet_ds.to_table()\n",
    "\n",
    "# To Arrow\n",
    "arrow_table = table.to_arrow()\n",
    "print(f\"Converted to Arrow: {type(arrow_table)}\")\n",
    "\n",
    "# To Pandas\n",
    "pandas_df = table.to_pandas()\n",
    "print(f\"Converted to Pandas: {type(pandas_df)}\")\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to different formats\n",
    "csv_output = temp_path / \"output.csv\"\n",
    "pandas_df.to_csv(csv_output, index=False)\n",
    "print(f\"Saved to CSV: {csv_output}\")\n",
    "\n",
    "json_output = temp_path / \"output.json\"\n",
    "pandas_df.to_json(json_output, orient='records', date_format='iso')\n",
    "print(f\"Saved to JSON: {json_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Working with Partitioned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partitioned dataset\n",
    "print(\"4.1 Creating partitioned dataset...\")\n",
    "partitioned_path = temp_path / \"partitioned_data\"\n",
    "partitioned_ds = ParquetDataset.from_csv(\n",
    "    csv_file,\n",
    "    path=partitioned_path,\n",
    "    partition_cols=['category', 'is_active']\n",
    ")\n",
    "\n",
    "print(f\"Created partitioned dataset:\")\n",
    "print(f\" - Partition columns: category, is_active\")\n",
    "print(f\" - Number of files: {len(partitioned_ds.files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore partitions\n",
    "print(\"\\n4.2 Exploring partitions...\")\n",
    "for file_path in partitioned_ds.files[:5]:  # Show first 5\n",
    "    print(f\"   {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query specific partition\n",
    "print(\"\\n4.3 Querying specific partition...\")\n",
    "table = partitioned_ds.to_table()\n",
    "electronics_data = table.filter(table.category == 'Electronics')\n",
    "print(f\"Electronics category: {len(electronics_data)} records\")\n",
    "electronics_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Integration with Other Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base dataset\n",
    "ds = ParquetDataset.from_csv(csv_file, temp_path / \"base_data\")\n",
    "table = ds.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas integration\n",
    "print(\"5.1 Pandas integration...\")\n",
    "pdf = table.to_pandas()\n",
    "pandas_result = pdf.groupby('category')['price'].agg(['mean', 'count'])\n",
    "print(\"Pandas aggregation result:\")\n",
    "pandas_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars integration (if available)\n",
    "if POLARS_AVAILABLE:\n",
    "    print(\"\\n5.2 Polars integration...\")\n",
    "    polars_df = table.to_polars()\n",
    "    polars_result = polars_df.groupby('category').agg([\n",
    "        pl.col('price').mean(),\n",
    "        pl.col('quantity').sum()\n",
    "    ])\n",
    "    print(\"Polars aggregation result:\")\n",
    "    polars_result\n",
    "else:\n",
    "    print(\"\\n5.2 Polars not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB integration (if available)\n",
    "if DUCKDB_AVAILABLE:\n",
    "    print(\"\\n5.3 DuckDB integration...\")\n",
    "    # Register table with DuckDB\n",
    "    con = duckdb.connect()\n",
    "    con.register('pydala_table', table.to_arrow())\n",
    "\n",
    "    # Run SQL query\n",
    "    sql_result = con.execute(\"\"\"\n",
    "        SELECT\n",
    "            category,\n",
    "            AVG(price) as avg_price,\n",
    "            SUM(quantity) as total_quantity,\n",
    "            COUNT(*) as count\n",
    "        FROM pydala_table\n",
    "        GROUP BY category\n",
    "        ORDER BY avg_price DESC\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "    print(\"DuckDB SQL result:\")\n",
    "    display(sql_result)\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"\\n5.3 DuckDB not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Handling Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset\n",
    "print(\"6.1 Creating larger dataset...\")\n",
    "large_csv_file = create_sample_csv_data(temp_path, 10000)  # 10K records\n",
    "large_ds = ParquetDataset.from_csv(large_csv_file, temp_path / \"large_data\")\n",
    "table = large_ds.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scanner for memory-efficient reading\n",
    "scanner = table.to_arrow_scanner(\n",
    "    batch_size=1000,\n",
    "    columns=['id', 'name', 'category', 'price']\n",
    ")\n",
    "\n",
    "# Process in batches\n",
    "batch_count = 0\n",
    "total_processed = 0\n",
    "\n",
    "print(\"Processing in batches...\")\n",
    "for batch in scanner.to_batches():\n",
    "    batch_count += 1\n",
    "    total_processed += batch.num_rows\n",
    "\n",
    "    # Example batch processing\n",
    "    if batch_count <= 3:  # Show first 3 batches\n",
    "        avg_price = batch.column('price').to_pandas().mean()\n",
    "        print(f\"  Batch {batch_count}: {batch.num_rows} rows, avg price: {avg_price:.2f}\")\n",
    "\n",
    "print(f\"\\nProcessed {total_processed} total records in {batch_count} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient filtering\n",
    "print(\"\\n6.3 Memory-efficient filtering...\")\n",
    "filtered_scanner = table.to_arrow_scanner(\n",
    "    filter=pa.dataset.field('price') > 75\n",
    ")\n",
    "\n",
    "# Count without loading all data\n",
    "count = 0\n",
    "for batch in filtered_scanner.to_batches():\n",
    "    count += batch.num_rows\n",
    "\n",
    "print(f\"Records with price > 75: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Cleaned up temporary directory: {temp_dir}\")\n",
    "print(\"\\nAll examples completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
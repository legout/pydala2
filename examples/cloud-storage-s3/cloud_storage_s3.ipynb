<cell id="cell-0"><cell_type>markdown</cell_type># Cloud Storage S3 Integration

This notebook demonstrates cloud storage integration with PyDala2, including S3 dataset operations, cloud storage optimization, and remote data access patterns.</cell id="cell-0">
<cell id="cell-1">import os
import tempfile
import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as pds
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union
import boto3
from botocore.exceptions import ClientError, NoCredentialsError
import time

# Import PyDala2 components
from pydala.dataset import ParquetDataset
from pydala.table import PydalaTable
from pydala.catalog import Catalog
from pydala.filesystem import FileSystem</cell id="cell-1">
<cell id="cell-2"><cell_type>markdown</cell_type>## Create Sample Data</cell id="cell-2">
<cell id="cell-3">def create_sample_cloud_data():
    """Create sample data for cloud storage examples."""
    np.random.seed(42)

    data = {
        'id': range(1, 1001),
        'timestamp': [datetime.now() - timedelta(hours=i) for i in range(1000)],
        'category': np.random.choice(['TypeA', 'TypeB', 'TypeC', 'TypeD'], 1000),
        'value': np.random.uniform(1, 1000, 1000).round(2),
        'metric1': np.random.normal(100, 20, 1000).round(2),
        'metric2': np.random.exponential(50, 1000).round(2),
        'status': np.random.choice(['active', 'inactive', 'pending'], 1000),
        'region': np.random.choice(['us-east-1', 'us-west-2', 'eu-west-1'], 1000),
        'user_id': np.random.randint(1, 101, 1000),
        'session_id': [f"session_{i}" for i in range(1000)],
        'metadata_size': np.random.randint(100, 10000, 1000)
    }

    df = pd.DataFrame(data)
    df['date'] = df['timestamp'].dt.date
    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek

    return df

def check_s3_credentials():
    """Check if S3 credentials are available."""
    try:
        boto3.client('s3').list_buckets()
        return True
    except (NoCredentialsError, ClientError):
        return False

# Check S3 credentials and create sample data
s3_available = check_s3_credentials()
if s3_available:
    print("✓ S3 credentials detected")
else:
    print("ℹ Using simulated S3 operations (local filesystem)")

df = create_sample_cloud_data()
print(f"Created sample data with {len(df)} records")
df.head()</cell id="cell-3">
<cell id="cell-4"><cell_type>markdown</cell_type>## Create Mock S3 Structure</cell id="cell-4">
<cell id="cell-5"># Create mock S3-like structure for demonstration
print("\n=== Creating Mock S3 Structure ===")

with tempfile.TemporaryDirectory() as temp_dir:
    temp_path = Path(temp_dir)

    # Create bucket-like directory structure
    bucket_path = temp_path / "my-bucket"
    bucket_path.mkdir()

    # Create dataset structure
    ds = ParquetDataset.from_pandas(
        df,
        path=bucket_path / "datasets" / "sample_data",
        partition_cols=['region', 'status'],
        row_group_size=100
    )

    print(f"Created mock S3 structure at: {bucket_path}")
    print(f"Dataset files: {len(ds.files)}")
    print(f"Sample file paths:")
    for file_path in ds.files[:5]:  # Show first 5 files
        print(f"  s3://my-bucket/{file_path}")

    # Store paths for later use
    mock_bucket_path = bucket_path
    mock_ds = ds</cell id="cell-5">
<cell id="cell-6"><cell_type>markdown</cell_type>## Example 1: S3 Dataset Operations</cell id="cell-6">
<cell id="cell-7"># Create temporary directory for S3 simulation
temp_dir = tempfile.mkdtemp()
temp_path = Path(temp_dir)

# Create S3-like structure
bucket_path = temp_path / "my-bucket"
bucket_path.mkdir()

print("\n=== Example 1: S3 Dataset Operations ===")
print("Note: This example simulates S3 operations using local filesystem")
print("In real usage, you would use actual S3 paths like:")
print("s3://bucket-name/path/to/data/")

# Create dataset with S3-like path
ds = ParquetDataset.from_pandas(
    df,
    path=bucket_path / "raw-data" / "transactions",
    partition_cols=['region', 'date'],
    row_group_size=200
)

print(f"\n1.1 Dataset Creation:")
print(f" - S3 Path: s3://my-bucket/raw-data/transactions/")
print(f" - Files: {len(ds.files)}")
print(f" - Records: {len(df)}")
print(f" - Partitions: region, date")

print("\n1.2 Dataset Access:")
table = ds.to_table()
print(f" - Table loaded successfully")
print(f" - Shape: {len(table)} rows, {len(table.columns)} columns")</cell id="cell-7">
<cell id="cell-8"># Querying S3 data
print("\n1.3 Querying S3 Data:")

# Simulate querying with filter
active_records = table.filter(table.status == 'active')
print(f" - Active records: {len(active_records)}")

us_east_records = table.filter(table.region == 'us-east-1')
print(f" - us-east-1 records: {len(us_east_records)}")

print("\n1.4 S3 File Operations:")
print("Sample S3 file structure:")
for i, file_path in enumerate(ds.files[:3]):
    s3_path = f"s3://my-bucket/{file_path}"
    size = os.path.getsize(bucket_path / file_path) / 1024  # KB
    print(f"  {i+1}. {s3_path} ({size:.1f} KB)")</cell id="cell-8">
<cell id="cell-9"><cell_type>markdown</cell_type>## Example 2: Cloud Storage Optimization</cell id="cell-9">
<cell id="cell-10"># Create optimized dataset
df = create_sample_cloud_data()

print("\n=== Example 2: Cloud Storage Optimization ===")

print("\n2.1 Partitioning Strategy:")

# Optimal partitioning for cloud access
ds_optimized = ParquetDataset.from_pandas(
    df,
    path=temp_path / "optimized-data",
    partition_cols=['region', 'status', 'date'],
    row_group_size=1000  # Larger row groups for cloud
)

print(f" - Partitioning: region, status, date")
print(f" - Row group size: 1000")
print(f" - Files: {len(ds_optimized.files)}")

print("\n2.2 Column Pruning for Cloud:")
table = ds_optimized.to_table()

# Select only needed columns (reduces data transfer)
start_time = time.time()
selected_columns = table.select(['id', 'timestamp', 'value', 'region', 'status'])
end_time = time.time()

print(f" - Column selection time: {end_time - start_time:.3f}s")
print(f" - Selected {len(selected_columns.columns)} of {len(table.columns)} columns")</cell id="cell-10">
<cell id="cell-11"># Predicate pushdown
print("\n2.3 Predicate Pushdown:")

# Use efficient filtering
start_time = time.time()
filtered_data = table.filter(
    (table.region == 'us-east-1') &
    (table.status == 'active') &
    (table.value > 100)
)
end_time = time.time()

print(f" - Filter time: {end_time - start_time:.3f}s")
print(f" - Filtered records: {len(filtered_data)}")</cell id="cell-11">
<cell id="cell-12"># Batch processing for cloud
print("\n2.4 Batch Processing for Cloud:")

# Process in batches to minimize memory usage
scanner = table.to_arrow_scanner(
    columns=['id', 'timestamp', 'value', 'region'],
    filter=pa.dataset.field('value') > 50
)

batch_count = 0
total_rows = 0
for batch in scanner.to_batches():
    batch_count += 1
    total_rows += batch.num_rows
    if batch_count >= 5:  # Process first 5 batches
        break

print(f" - Processed {batch_count} batches")
print(f" - Total rows: {total_rows}")</cell id="cell-12">
<cell id="cell-13"><cell_type>markdown</cell_type>## Example 3: Remote Access Patterns</cell id="cell-13">
<cell id="cell-14"># Create multiple datasets to simulate remote access
datasets = []
for i in range(3):
    df = create_sample_cloud_data()
    df['batch'] = f"batch_{i}"

    ds = ParquetDataset.from_pandas(
        df,
        path=temp_path / f"batch_{i}",
        partition_cols=['region']
    )
    datasets.append(ds)

print(f"\n=== Example 3: Remote Access Patterns ===")
print(f"\n3.1 Created {len(datasets)} remote datasets")</cell id="cell-14">
<cell id="cell-15"># Sequential access pattern
print("\n3.2 Sequential Access Pattern:")
start_time = time.time()
all_data = []

for ds in datasets:
    table = ds.to_table()
    data = table.filter(table.status == 'active')
    all_data.append(data)

combined_data = pa.concat_tables(all_data)
sequential_time = time.time() - start_time
print(f" - Sequential access time: {sequential_time:.3f}s")
print(f" - Combined records: {combined_data.num_rows}")</cell id="cell-15">
<cell id="cell-16"># Efficient access pattern
print("\n3.3 Efficient Access Pattern:")
start_time = time.time()
efficient_results = []

for ds in datasets:
    # Use scanner for efficient access
    scanner = ds.to_arrow_scanner(
        columns=['id', 'value', 'region', 'status'],
        filter=pa.dataset.field('status') == 'active'
    )

    for batch in scanner.to_batches():
        efficient_results.append(batch)

if efficient_results:
    efficient_data = pa.Table.from_batches(efficient_results)
    efficient_time = time.time() - start_time
    print(f" - Efficient access time: {efficient_time:.3f}s")
    print(f" - Efficient records: {efficient_data.num_rows}")</cell id="cell-16">
<cell id="cell-17"># Caching strategy
print("\n3.4 Caching Strategy:")

# Simulate caching frequently accessed data
print(" - Caching us-east-1 data...")
us_east_data = []
for ds in datasets:
    table = ds.to_table()
    east_data = table.filter(table.region == 'us-east-1')
    us_east_data.append(east_data)

cached_us_east = pa.concat_tables(us_east_data)
print(f" - Cached {len(cached_us_east)} us-east-1 records")

# Access cached data
start_time = time.time()
cached_result = cached_us_east.filter(cached_us_east.value > 200)
cache_time = time.time() - start_time
print(f" - Cached access time: {cache_time:.3f}s")
print(f" - Cached result: {len(cached_result)} records")</cell id="cell-17">
<cell id="cell-18"><cell_type>markdown</cell_type>## Example 4: Performance Considerations</cell id="cell-18">
<cell id="cell-19"># Create test datasets with different configurations
configurations = [
    {"name": "Small Files", "row_group_size": 100, "partitions": ["region"]},
    {"name": "Medium Files", "row_group_size": 500, "partitions": ["region", "status"]},
    {"name": "Large Files", "row_group_size": 1000, "partitions": ["region", "status", "date"]},
]

print(f"\n=== Example 4: Performance Considerations ===")

results = []

for config in configurations:
    print(f"\n4.{len(results)+1} Testing {config['name']}:")

    df = create_sample_cloud_data()

    ds = ParquetDataset.from_pandas(
        df,
        path=temp_path / f"test_{config['name'].lower().replace(' ', '_')}",
        partition_cols=config['partitions'],
        row_group_size=config['row_group_size']
    )

    # Test query performance
    table = ds.to_table()

    # Test 1: Simple filter
    start_time = time.time()
    result1 = table.filter(table.value > 100)
    time1 = time.time() - start_time

    # Test 2: Complex filter
    start_time = time.time()
    result2 = table.filter(
        (table.region == 'us-east-1') &
        (table.status == 'active') &
        (table.value > 200)
    )
    time2 = time.time() - start_time

    # Test 3: Aggregation
    start_time = time.time()
    df_agg = table.to_pandas().groupby('region')['value'].mean()
    time3 = time.time() - start_time

    result = {
        'config': config['name'],
        'files': len(ds.files),
        'simple_filter_time': time1,
        'complex_filter_time': time2,
        'aggregation_time': time3,
        'row_group_size': config['row_group_size'],
        'partition_count': len(config['partitions'])
    }
    results.append(result)

    print(f"   - Files: {len(ds.files)}")
    print(f"   - Simple filter: {time1:.3f}s")
    print(f"   - Complex filter: {time2:.3f}s")
    print(f"   - Aggregation: {time3:.3f}s")</cell id="cell-19">
<cell id="cell-20"># Performance summary
print("\n4.5 Performance Summary:")
print("Configuration | Files | Simple Filter | Complex Filter | Aggregation")
print("-" * 70)
for r in results:
    print(f"{r['config']:<15} | {r['files']:<5} | {r['simple_filter_time']:<13.3f} | {r['complex_filter_time']:<15.3f} | {r['aggregation_time']:<12.3f}")

print("\n4.6 Cloud Storage Best Practices:")
print("   - Use appropriate file sizes (100MB-1GB)")
print("   - Implement proper partitioning strategy")
print("   - Use column pruning and predicate pushdown")
print("   - Consider data locality and region placement")
print("   - Implement caching for frequently accessed data")
print("   - Use compression to reduce storage costs")</cell id="cell-20">
<cell id="cell-21"><cell_type>markdown</cell_type>## Example 5: Error Handling</cell id="cell-21">
<cell id="cell-22">print(f"\n=== Example 5: Error Handling ===")

print("\n5.1 Common Cloud Storage Errors:")

error_examples = [
    "AuthenticationError: Invalid AWS credentials",
    "PermissionError: Access denied to S3 bucket",
    "ConnectionError: Network timeout accessing S3",
    "NotFoundError: S3 object does not exist",
    "ThrottlingError: S3 request rate exceeded"
]

for error in error_examples:
    print(f"   - {error}")

print("\n5.2 Error Handling Patterns:")

# Simulate error handling patterns
def safe_s3_operation(operation_name: str, simulate_error: bool = False):
    """Simulate safe S3 operation with error handling."""
    try:
        if simulate_error:
            raise ConnectionError("Simulated S3 connection error")

        # Simulate successful operation
        print(f"   ✓ {operation_name} completed successfully")
        return True

    except ConnectionError as e:
        print(f"   ✗ {operation_name} failed: {e}")
        print(f"     Retrying with exponential backoff...")
        return False

    except PermissionError as e:
        print(f"   ✗ {operation_name} failed: {e}")
        print(f"     Check IAM permissions and bucket policies")
        return False

    except Exception as e:
        print(f"   ✗ {operation_name} failed: {e}")
        print(f"     Log error and alert monitoring system")
        return False

# Test error handling
safe_s3_operation("List S3 objects", simulate_error=False)
safe_s3_operation("Read S3 object", simulate_error=True)</cell id="cell-22">
<cell id="cell-23"># Retry strategies and monitoring
print("\n5.3 Retry Strategies:")
print("   - Exponential backoff for transient errors")
print("   - Circuit breaker pattern for repeated failures")
print("   - Dead letter queues for failed operations")
print("   - Monitoring and alerting for error rates")

print("\n5.4 Monitoring and Logging:")
print("   - Track S3 request latency and success rates")
print("   - Monitor data transfer costs")
print("   - Log failed operations for debugging")
print("   - Set up alerts for unusual error patterns")</cell id="cell-23">
<cell id="cell-24"><cell_type>markdown</cell_type>## Cleanup

Remove temporary files</cell id="cell-24">
<cell id="cell-25">import shutil
shutil.rmtree(temp_dir)
print(f"Cleaned up temporary directory: {temp_dir}")

print("\n" + "="*50)
print("Cloud storage S3 examples completed!")
print("\nKey Takeaways:")
print("- S3 dataset operations and access patterns")
print("- Cloud storage optimization techniques")
print("- Remote access strategies and caching")
print("- Performance considerations for cloud storage")
print("- Error handling and retry strategies")

if not s3_available:
    print("\nNote: Examples used simulated S3 operations.")
    print("For real S3 access, configure AWS credentials.")</cell id="cell-25">
# Cloud Storage S3

This example demonstrates cloud storage integration with PyDala2, including S3 dataset operations, cloud optimization techniques, and remote access patterns.

## What You'll Learn

- S3 dataset operations
- Cloud storage optimization
- Remote data access patterns
- Performance considerations for cloud storage
- Error handling and retry strategies

## Files

- `cloud_storage_s3.py` - Python script demonstrating S3 operations
- `cloud_storage_s3.ipynb` - Jupyter notebook version
- `cloud_storage_s3.marimo.py` - Marimo notebook version

## Running the Examples

### Python Script
```bash
cd cloud-storage-s3
python cloud_storage_s3.py
```

### Jupyter Notebook
```bash
cd cloud-storage-s3
jupyter notebook cloud_storage_s3.ipynb
```

### Marimo Notebook
```bash
cd cloud-storage-s3
marimo edit cloud_storage_s3.marimo.py
```

## Prerequisites

- PyDala2 installed (`pip install pydala2`)
- AWS CLI configured (for real S3 access)
- boto3 installed (`pip install boto3`)
- Sample data will be generated by the examples

## AWS Credentials Setup

For real S3 access, configure AWS credentials:

```bash
# Using AWS CLI
aws configure

# Or set environment variables
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=us-east-1
```

## Key Concepts Demonstrated

1. **S3 Dataset Operations**: Creating, accessing, and querying S3 datasets
2. **Cloud Optimization**: Partitioning strategies, row group optimization
3. **Remote Access Patterns**: Efficient data access, caching strategies
4. **Performance Considerations**: File size optimization, network latency
5. **Error Handling**: Retry strategies, error recovery patterns

## Advanced Topics

- S3 security and access control
- Cost optimization strategies
- Cross-region data access
- S3 lifecycle policies
- Data transfer acceleration
- Monitoring and logging

## Cloud Storage Best Practices

1. **Data Organization**
   - Use consistent partitioning strategies
   - Implement proper folder structures
   - Use meaningful file naming conventions

2. **Performance Optimization**
   - Optimize file sizes (100MB-1GB recommended)
   - Use appropriate compression (Snappy, Zstandard)
   - Implement data partitioning for query performance

3. **Cost Optimization**
   - Use appropriate storage classes (Standard, Intelligent-Tiering)
   - Implement lifecycle policies for data archiving
   - Monitor data transfer costs

4. **Security Best Practices**
   - Use IAM roles and policies
   - Enable S3 bucket encryption
   - Implement access logging
   - Use bucket policies for access control

## S3 Path Format

PyDala2 supports standard S3 path formats:

```
s3://bucket-name/path/to/data/
s3://bucket-name/dataset-name/
s3://bucket-name/prefix/data.parquet
```

## Error Handling

The examples demonstrate comprehensive error handling:
- Authentication and authorization errors
- Network connectivity issues
- S3 service limits and throttling
- Data consistency and corruption
- Retry strategies and backoff patterns

## Monitoring and Logging

Best practices for cloud storage monitoring:
- Track S3 request latency and success rates
- Monitor data transfer costs
- Log failed operations for debugging
- Set up alerts for unusual error patterns
- Monitor storage usage and growth

## Next Steps

After mastering cloud storage, explore:
- Time Series analysis
- Advanced querying techniques
- Performance optimization
- Metadata management

## Note on Simulation

The examples include both real S3 operations (when credentials are available) and simulated operations using local filesystem for demonstration purposes. This allows you to learn the concepts regardless of AWS access.
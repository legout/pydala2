{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyDala2","text":"<p>PyDala2 is a high-performance Python library for managing Parquet datasets with advanced metadata capabilities, built on Apache Arrow and DuckDB.</p>"},{"location":"#what-is-pydala2","title":"\ud83c\udfaf What is PyDala2?","text":"<p>PyDala2 provides a sophisticated dataset management system that combines: - Automatic optimization of Parquet file layouts - Intelligent metadata management with schema evolution - Multi-engine query processing (PyArrow + DuckDB) - Production-ready features like delta updates and time-based operations</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":""},{"location":"#core-capabilities","title":"\ud83d\ude80 Core Capabilities","text":"<ul> <li>Smart Parquet Management: Automatic file optimization, compaction, and layout management</li> <li>Dual Engine Architecture: Seamlessly switches between PyArrow (fast scans) and DuckDB (complex queries)</li> <li>Advanced Metadata: Automatic schema unification, statistics tracking, and efficient updates</li> <li>Hive Partitioning: Native support for partitioned datasets with automatic inference</li> </ul>"},{"location":"#data-operations","title":"\ud83d\udcca Data Operations","text":"<ul> <li>Automatic Filter Pushdown: Intelligently routes queries to the optimal engine</li> <li>Delta Updates: Merge new data efficiently with change detection</li> <li>Schema Evolution: Handle schema changes without breaking existing data</li> <li>Time-based Operations: Built-in support for time series and temporal data</li> </ul>"},{"location":"#production-features","title":"\ud83c\udfd7\ufe0f Production Features","text":"<ul> <li>Catalog System: Centralized dataset management across namespaces</li> <li>Cloud Storage: Native S3 integration with caching and credential management</li> <li>Optimization Operations: Automatic data type optimization, file compaction, and sorting</li> <li>Robust Error Handling: Automatic schema repair and graceful degradation</li> </ul>"},{"location":"#use-cases","title":"\ud83c\udfaf Use Cases","text":""},{"location":"#data-engineering","title":"Data Engineering","text":"<pre><code># Efficient ETL pipeline\ndataset = ParquetDataset(\"data/sales\")\ndataset.write_to_dataset(\n    new_sales_data,\n    mode=\"delta\",\n    partition_by=[\"date\", \"region\"],\n    sort_by=\"timestamp DESC\"\n)\n</code></pre>"},{"location":"#analytics","title":"Analytics","text":"<pre><code># Complex queries with automatic optimization\nresult = dataset.filter(\"\"\"\n    date &gt; '2023-01-01'\n    AND region IN ('US', 'EU')\n    AND amount &gt; 1000\n\"\"\")\n</code></pre>"},{"location":"#data-lake-management","title":"Data Lake Management","text":"<pre><code># Catalog-based dataset organization\ncatalog = Catalog(\"datalake.yaml\")\nsales = catalog.get_table(\"sales\")\ncustomers = catalog.get_table(\"customers\")\n</code></pre>"},{"location":"#quick-example","title":"\ud83d\ude80 Quick Example","text":"<pre><code>from pydala import ParquetDataset\nimport polars as pl\n\n# Create a dataset (directory auto-created)\ndataset = ParquetDataset(\"data/sales\")\n\n# Write data with automatic optimization\ndata = pl.DataFrame({\n    'id': range(1000),\n    'date': pl.date_range(start=2023-01-01, end=2023-12-31, interval='1d', eager=True).head(1000),\n    'region': ['US', 'EU', 'APAC'] * 333 + ['US'],\n    'amount': [i * 10.5 for i in range(1000)]\n})\n\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"date\", \"region\"],\n    max_rows_per_file=100000,\n    compression=\"zstd\"\n)\n\n# Read and filter - automatic backend selection\nresult = dataset.filter(\"amount &gt; 500 AND region = 'US'\")\nprint(result.collect())\n\n# Use SQL directly\nsql_result = dataset.ddb_con.sql(\"\"\"\n    SELECT region, COUNT(*) as count, AVG(amount) as avg_amount\n    FROM dataset\n    GROUP BY region\n\"\"\").pl()\n</code></pre>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"graph TD     A[PyDala2 Application] --&gt; B[ParquetDataset]     B --&gt; C[PydalaTable]     C --&gt; D[PyArrow Dataset]     C --&gt; E[DuckDB Relation]     B --&gt; F[Metadata Management]     F --&gt; G[_metadata File]     F --&gt; H[_file_metadata File]     B --&gt; I[Optimization]     I --&gt; J[Compaction]     I --&gt; K[Schema Evolution]     I --&gt; L[Delta Updates]     B --&gt; M[Catalog Integration]     M --&gt; N[YAML Configuration]     M --&gt; O[Namespace Management]"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Our comprehensive documentation covers:</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation Guide - Setup and configuration</li> <li>Quick Start - 5-minute tutorial</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Basic Usage - Core operations and patterns</li> <li>Data Operations - Filtering, aggregation, and transformations</li> <li>Performance Optimization - Tuning for production workloads</li> <li>Catalog Management - Organizing datasets</li> <li>Schema Management - Schema evolution and validation</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Core Classes - BaseDataset, Config</li> <li>Dataset Classes - ParquetDataset and variants</li> <li>Table Operations - PydalaTable interface</li> <li>Catalog System - Catalog and namespace management</li> <li>Filesystem - Storage backends and cloud integration</li> <li>Utilities - Helper functions and tools</li> </ul>"},{"location":"#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Deployment Guide - Production deployment</li> <li>Performance Tuning - Advanced optimization</li> <li>Troubleshooting - Common issues and solutions</li> <li>Integration Patterns - Using PyDala2 with other tools</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details.</p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>MIT License</p>"},{"location":"REFACTORING/","title":"Pydala Codebase Refactoring Plan","text":""},{"location":"REFACTORING/#overview","title":"Overview","text":"<p>This document outlines a comprehensive refactoring plan for the Pydala codebase, focusing on improving code quality, reducing complexity, and enhancing maintainability while ensuring full backward compatibility.</p>"},{"location":"REFACTORING/#analysis-summary","title":"Analysis Summary","text":"<p>The codebase analysis revealed several areas for improvement: - Code Quality: Large blocks of commented code, bare exception clauses - Complexity: Methods with high cyclomatic complexity, duplicate code - Type Safety: Missing type hints, insufficient input validation - Maintainability: Magic numbers, scattered configuration</p>"},{"location":"REFACTORING/#priority-1-code-cleanup-and-bug-fixes","title":"Priority 1: Code Cleanup and Bug Fixes","text":""},{"location":"REFACTORING/#1-remove-commented-code-in-miscpy","title":"1. Remove Commented Code in misc.py","text":"<ul> <li>Location: <code>pydala/helpers/misc.py:15-370</code></li> <li>Issue: ~355 lines of commented code (90% of file)</li> <li>Action: Remove all commented code blocks</li> <li>Impact:</li> <li>Reduces file size from ~400 lines to ~45 lines</li> <li>Improves code readability and navigation</li> <li>Eliminates confusion about deprecated functionality</li> </ul>"},{"location":"REFACTORING/#2-fix-critical-bug-in-catalogpy","title":"2. Fix Critical Bug in catalog.py","text":"<ul> <li>Location: <code>pydala/catalog.py:63-66</code></li> <li>Current Code:   <pre><code># Update the catalog with itself?\ncatalog_dict.update(self.to_dict())  # This line is suspicious\n</code></pre></li> <li>Issue: Redundant self-update potentially causing data corruption</li> <li>Fix: Remove or properly document this update logic</li> </ul>"},{"location":"REFACTORING/#3-fix-bare-exception-clause","title":"3. Fix Bare Exception Clause","text":"<ul> <li>Location: <code>pydala/io.py:49</code></li> <li>Current Code:   <pre><code>except Exception:\n    pass\n</code></pre></li> <li>Issue: Masks all errors without logging</li> <li>Fix:   <pre><code>except (OSError, IOError) as e:\n    logger.warning(f\"Failed to process {file_path}: {e}\")\n</code></pre></li> </ul>"},{"location":"REFACTORING/#priority-2-reduce-complexity","title":"Priority 2: Reduce Complexity","text":""},{"location":"REFACTORING/#4-simplify-_get_sort_by-method","title":"4. Simplify _get_sort_by Method","text":"<ul> <li>Location: <code>pydala/table.py:30-93</code></li> <li>Issue: 63-line method with high cyclomatic complexity</li> <li>Action: Extract helper functions:</li> <li><code>_parse_string_sort(value: str) -&gt; SortKey</code></li> <li><code>_parse_callable_sort(value: Callable) -&gt; SortKey</code></li> <li><code>_validate_sort_key(key: SortKey) -&gt; None</code></li> <li>Benefits:</li> <li>Improves testability</li> <li>Reduces cognitive complexity</li> <li>Enables better error messages</li> </ul>"},{"location":"REFACTORING/#5-remove-duplicate-scanner-method","title":"5. Remove Duplicate Scanner Method","text":"<ul> <li>Location: <code>pydala/table.py</code></li> <li>Issue: <code>to_scanner()</code> and <code>scanner()</code> are identical (lines 748-755, 757-764)</li> <li>Action:</li> <li>Keep <code>scanner()</code> as primary method</li> <li>Mark <code>to_scanner()</code> as deprecated with warning</li> <li>Update documentation</li> </ul>"},{"location":"REFACTORING/#6-extract-scanner-parameters","title":"6. Extract Scanner Parameters","text":"<ul> <li>Issue: Scanner configuration duplicated across 10+ methods</li> <li>Action: Create <code>ScannerConfig</code> dataclass:   <pre><code>@dataclass\nclass ScannerConfig:\n    batch_size: int = 131072\n    buffer_size: int = 65536\n    prefetch: int = 2\n    num_threads: int = 4\n</code></pre></li> <li>Files to Update: <code>table.py</code>, <code>dataset.py</code>, <code>scanner.py</code></li> <li>Benefits:</li> <li>Centralized configuration</li> <li>Easier parameter management</li> <li>Consistent defaults across the codebase</li> </ul>"},{"location":"REFACTORING/#priority-3-improve-type-safety","title":"Priority 3: Improve Type Safety","text":""},{"location":"REFACTORING/#7-add-type-hints","title":"7. Add Type Hints","text":"<ul> <li>Focus Areas:</li> <li>All public methods in core modules</li> <li>Complex methods in <code>table.py</code>, <code>dataset.py</code></li> <li>Helper functions in <code>misc.py</code></li> <li>Example:   <pre><code>def head(\n    self,\n    n: int = 5,\n    columns: Optional[List[str]] = None\n) -&gt; \"Table\":\n    ...\n</code></pre></li> </ul>"},{"location":"REFACTORING/#8-add-input-validation","title":"8. Add Input Validation","text":"<ul> <li>Approach:</li> <li>Add parameter validation at public API boundaries</li> <li>Use clear, descriptive error messages</li> <li>Validate types, ranges, and constraints</li> <li>Example:   <pre><code>if not isinstance(n, int) or n &lt; 0:\n    raise ValueError(\"n must be a non-negative integer\")\n</code></pre></li> </ul>"},{"location":"REFACTORING/#priority-4-maintainability-improvements","title":"Priority 4: Maintainability Improvements","text":""},{"location":"REFACTORING/#9-create-constants-file","title":"9. Create Constants File","text":"<ul> <li>New File: <code>pydala/constants.py</code></li> <li>Content:   <pre><code># Performance tuning\nDEFAULT_BATCH_SIZE = 131072\nDEFAULT_BUFFER_SIZE = 65536\nDEFAULT_PREFETCH_COUNT = 2\nDEFAULT_THREAD_COUNT = 4\n\n# Validation\nMAX_COLUMN_NAME_LENGTH = 255\nMIN_PARTITION_SIZE = 1024\n</code></pre></li> <li>Benefits:</li> <li>Single source of truth</li> <li>Easier tuning</li> <li>Better documentation</li> </ul>"},{"location":"REFACTORING/#10-fix-test-infrastructure","title":"10. Fix Test Infrastructure","text":"<ul> <li>Location: <code>tests/test_table.py</code></li> <li>Issues:</li> <li>Missing imports</li> <li>Incomplete test coverage</li> <li>Outdated test cases</li> <li>Action:</li> <li>Fix import statements</li> <li>Add tests for refactored methods</li> <li>Ensure 90%+ code coverage</li> </ul>"},{"location":"REFACTORING/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"REFACTORING/#phase-1-code-cleanup-days-1-2","title":"Phase 1: Code Cleanup (Days 1-2)","text":"<ol> <li>Remove commented code</li> <li>Fix critical bugs</li> <li>Update exception handling</li> </ol>"},{"location":"REFACTORING/#phase-2-complexity-reduction-days-3-5","title":"Phase 2: Complexity Reduction (Days 3-5)","text":"<ol> <li>Extract helper methods</li> <li>Remove duplicate code</li> <li>Create configuration classes</li> </ol>"},{"location":"REFACTORING/#phase-3-type-safety-days-6-7","title":"Phase 3: Type Safety (Days 6-7)","text":"<ol> <li>Add type hints</li> <li>Add input validation</li> <li>Update documentation</li> </ol>"},{"location":"REFACTORING/#phase-4-maintainability-days-8-9","title":"Phase 4: Maintainability (Days 8-9)","text":"<ol> <li>Create constants file</li> <li>Update tests</li> <li>Performance optimization</li> </ol>"},{"location":"REFACTORING/#phase-5-testing-and-documentation-day-10","title":"Phase 5: Testing and Documentation (Day 10)","text":"<ol> <li>Run full test suite</li> <li>Update API documentation</li> <li>Create migration guide</li> </ol>"},{"location":"REFACTORING/#backward-compatibility-guarantees","title":"Backward Compatibility Guarantees","text":"<ol> <li>API Stability: No breaking changes to public APIs</li> <li>Method Signatures: All existing parameters remain supported</li> <li>Return Types: No changes to return types</li> <li>Error Handling: Same exceptions thrown for same conditions</li> <li>Deprecation Path: Deprecated methods will warn but continue to work</li> </ol>"},{"location":"REFACTORING/#risk-assessment","title":"Risk Assessment","text":""},{"location":"REFACTORING/#low-risk","title":"Low Risk","text":"<ul> <li>Removing commented code</li> <li>Adding type hints</li> <li>Creating constants file</li> </ul>"},{"location":"REFACTORING/#medium-risk","title":"Medium Risk","text":"<ul> <li>Extracting helper methods</li> <li>Adding input validation</li> <li>Fixing exception handling</li> </ul>"},{"location":"REFACTORING/#high-risk","title":"High Risk","text":"<ul> <li>Bug fixes in core logic</li> <li>Removing duplicate methods</li> <li>Performance optimizations</li> </ul>"},{"location":"REFACTORING/#success-metrics","title":"Success Metrics","text":"<ol> <li>Code Quality:</li> <li>Reduce cyclomatic complexity by 40%</li> <li>Eliminate all commented code</li> <li> <p>Fix all linting issues</p> </li> <li> <p>Maintainability:</p> </li> <li>Achieve 90%+ test coverage</li> <li>Add type hints to 100% of public methods</li> <li> <p>Reduce code duplication by 30%</p> </li> <li> <p>Performance:</p> </li> <li>No performance regression</li> <li>10% improvement in memory usage for large datasets</li> </ol>"},{"location":"REFACTORING/#rollback-plan","title":"Rollback Plan","text":"<ol> <li>Git tags will be created before each major change</li> <li>Each commit will be atomic and revertable</li> <li>Continuous integration will catch regressions early</li> <li>Feature flags for performance optimizations</li> </ol>"},{"location":"REFACTORING/#next-steps","title":"Next Steps","text":"<ol> <li>Create feature branch for refactoring</li> <li>Set up continuous integration</li> <li>Begin with Phase 1 (Code Cleanup)</li> <li>Regular progress updates to stakeholders</li> <li>Code reviews for each change</li> </ol> <p>This plan provides a structured approach to improving the Pydala codebase while ensuring stability and backward compatibility throughout the process.</p>"},{"location":"REFACTORING_SUMMARY/","title":"Pydala Refactoring Summary","text":"<p>This document summarizes the comprehensive refactoring performed on the pydala codebase to improve code quality, maintainability, and adherence to Python best practices.</p>"},{"location":"REFACTORING_SUMMARY/#overview","title":"Overview","text":"<p>The refactoring focused on enhancing the core modules while maintaining full backward compatibility. All changes were incremental improvements rather than a complete rewrite.</p>"},{"location":"REFACTORING_SUMMARY/#completed-tasks","title":"Completed Tasks","text":""},{"location":"REFACTORING_SUMMARY/#1-critical-bug-fixes","title":"1. Critical Bug Fixes \u2705","text":"<ul> <li>catalog.py:63: Fixed <code>_write_catalog</code> method to properly handle catalog updates and deletions</li> <li>table.py: Fixed <code>_parse_sort_by_string</code> method that was failing tests due to incorrect parsing logic</li> <li>Changed from <code>split()</code> to <code>rsplit()</code> to properly handle field names with spaces</li> <li>Ensured proper handling of sort direction specifications</li> </ul>"},{"location":"REFACTORING_SUMMARY/#2-code-deduplication","title":"2. Code Deduplication \u2705","text":"<ul> <li>Removed duplicate <code>scanner</code> method implementation in <code>table.py</code></li> <li>Consolidated scanner functionality with enhanced validation and <code>ScannerConfig</code> integration</li> <li>Eliminated commented code blocks across multiple modules</li> </ul>"},{"location":"REFACTORING_SUMMARY/#3-type-safety-enhancement","title":"3. Type Safety Enhancement \u2705","text":"<ul> <li>Added comprehensive type hints to all public methods across core modules</li> <li>Improved method signatures with proper return type annotations</li> <li>Enhanced type safety for better IDE support and static analysis</li> </ul>"},{"location":"REFACTORING_SUMMARY/#4-input-validation","title":"4. Input Validation \u2705","text":"<ul> <li>Implemented robust input validation across core modules</li> <li>Added parameter validation in <code>scanner</code> method before applying defaults</li> <li>Enhanced validation in <code>BaseDataset.__init__</code> for path and format parameters</li> <li>Standardized error handling patterns throughout the codebase</li> </ul>"},{"location":"REFACTORING_SUMMARY/#5-test-coverage-improvement","title":"5. Test Coverage Improvement \u2705","text":"<ul> <li>Increased test coverage from ~16% to 79% for <code>table.py</code></li> <li>Added comprehensive test cases covering:</li> <li>Method functionality and edge cases</li> <li>Deprecation warnings</li> <li>Input validation</li> <li>Error handling scenarios</li> <li>Fixed broken tests and improved test reliability</li> </ul>"},{"location":"REFACTORING_SUMMARY/#6-method-complexity-reduction","title":"6. Method Complexity Reduction \u2705","text":"<ul> <li>Refactored <code>write_to_dataset</code> method in <code>io.py</code> from 90 lines to ~50 lines</li> <li>Extracted helper methods for better organization:</li> <li><code>_generate_basename_template()</code></li> <li><code>_should_create_dir()</code></li> <li><code>_create_file_visitor()</code></li> <li><code>_write_dataset_with_retry()</code></li> <li>Improved code readability and maintainability</li> </ul>"},{"location":"REFACTORING_SUMMARY/#7-import-organization","title":"7. Import Organization \u2705","text":"<ul> <li>Standardized imports across all core modules following PEP 8 guidelines</li> <li>Organized imports into clear sections:</li> <li>Standard library imports</li> <li>Third-party imports</li> <li>Local imports</li> <li>Removed redundant imports and cleaned up import statements</li> </ul>"},{"location":"REFACTORING_SUMMARY/#technical-improvements","title":"Technical Improvements","text":""},{"location":"REFACTORING_SUMMARY/#configuration-management","title":"Configuration Management","text":"<ul> <li>Centralized scanner configuration using <code>ScannerConfig</code> dataclass</li> <li>Improved consistency across methods using shared configuration</li> </ul>"},{"location":"REFACTORING_SUMMARY/#error-handling","title":"Error Handling","text":"<ul> <li>Standardized error messages and exception types</li> <li>Enhanced validation logic with meaningful error descriptions</li> <li>Improved handling of edge cases and invalid inputs</li> </ul>"},{"location":"REFACTORING_SUMMARY/#code-quality","title":"Code Quality","text":"<ul> <li>Enhanced code documentation and comments</li> <li>Improved method naming and organization</li> <li>Reduced code duplication and improved maintainability</li> </ul>"},{"location":"REFACTORING_SUMMARY/#files-modified","title":"Files Modified","text":"<ul> <li><code>pydala/table.py</code>: Core table functionality improvements</li> <li><code>pydala/catalog.py</code>: Bug fixes and type hint enhancements</li> <li><code>pydala/io.py</code>: Method refactoring and import standardization</li> <li><code>pydala/dataset.py</code>: Import organization and code cleanup</li> <li><code>pydala/constants.py</code>: ScannerConfig dataclass implementation</li> <li><code>tests/test_table.py</code>: Comprehensive test coverage improvements</li> </ul>"},{"location":"REFACTORING_SUMMARY/#backward-compatibility","title":"Backward Compatibility","text":"<p>All changes maintain full backward compatibility: - Deprecated methods include proper deprecation warnings - Public API remains unchanged - Existing functionality preserved - No breaking changes introduced</p>"},{"location":"REFACTORING_SUMMARY/#test-results","title":"Test Results","text":"<p>All tests pass successfully: - 24 tests passing in <code>test_table.py</code> - Coverage improved from basic to 79% - No regressions detected</p>"},{"location":"REFACTORING_SUMMARY/#next-steps","title":"Next Steps","text":"<p>The codebase is now in a much improved state with: - Better maintainability - Enhanced type safety - Comprehensive test coverage - Standardized code organization - Improved performance and reliability</p> <p>Future enhancements can build upon this solid foundation with confidence in the code quality and stability.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you install PyDala2 and set up your environment for working with Parquet datasets.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>PyDala2 requires Python 3.8 or higher. Ensure you have a recent version of Python installed:</p> <pre><code>python --version\n</code></pre>"},{"location":"getting-started/#install-from-pypi","title":"Install from PyPI","text":"<p>The easiest way to install PyDala2 is using pip:</p> <pre><code>pip install pydala2\n</code></pre>"},{"location":"getting-started/#install-with-optional-dependencies","title":"Install with Optional Dependencies","text":"<p>PyDala2 has several optional dependencies for enhanced functionality:</p> <pre><code># Install with all optional dependencies\npip install pydala2[all]\n\n# Install with specific dependencies\npip install pydala2[s3]        # For S3 storage support\npip install pydala2[azure]     # For Azure Blob Storage\npip install pydala2[gcs]       # For Google Cloud Storage\n</code></pre>"},{"location":"getting-started/#install-from-source","title":"Install from Source","text":"<p>For development or to get the latest features:</p> <pre><code>git clone https://github.com/yourusername/pydala2.git\ncd pydala2\npip install -e .\n</code></pre>"},{"location":"getting-started/#basic-setup","title":"Basic Setup","text":""},{"location":"getting-started/#your-first-dataset","title":"Your First Dataset","text":"<p>Let's create a simple dataset and perform basic operations:</p> <pre><code>import polars as pl\nfrom pydala import ParquetDataset\n\n# Create some sample data\ndata = pl.DataFrame({\n    'id': range(1, 6),\n    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'age': [25, 30, 35, 28, 32],\n    'city': ['New York', 'London', 'Paris', 'Tokyo', 'Sydney'],\n    'salary': [50000, 60000, 70000, 55000, 65000]\n})\n\n# Create a dataset (directory auto-created)\ndataset = ParquetDataset(\"data/employees\")\n\n# Write the data with optimization\ndataset.write_to_dataset(\n    data,\n    max_rows_per_file=1000,\n    compression=\"zstd\"\n)\n\n# Read it back\nresult = dataset.table.pl.collect()\nprint(result)\n</code></pre>"},{"location":"getting-started/#understanding-the-pydala2-architecture","title":"Understanding the PyDala2 Architecture","text":"<p>PyDala2 uses a dual-engine architecture:</p> <ol> <li>PyArrow Dataset: For efficient columnar operations and file scanning</li> <li>DuckDB: For SQL queries and complex aggregations</li> </ol> <p>The library automatically chooses the best engine for your operations:</p> <pre><code># Simple scan - uses PyArrow\ndata = dataset.table.pl.collect()\n\n# Complex filter with LIKE - automatically uses DuckDB\nfiltered = dataset.filter(\"name LIKE 'A%' OR city IN ('London', 'Paris')\")\n\n# SQL operations - uses DuckDB directly\nsql_result = dataset.ddb_con.sql(\"\"\"\n    SELECT city, AVG(salary) as avg_salary\n    FROM dataset\n    GROUP BY city\n    ORDER BY avg_salary DESC\n\"\"\").pl()\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":""},{"location":"getting-started/#environment-variables","title":"Environment Variables","text":"<p>PyDala2 can be configured using environment variables:</p> <pre><code># S3 credentials (if using S3)\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=us-east-1\n\n# Cache directory\nexport PYDALA_CACHE_DIR=/tmp/pydala_cache\n\n# Logging level\nexport PYDALA_LOG_LEVEL=INFO\n</code></pre>"},{"location":"getting-started/#dataset-configuration","title":"Dataset Configuration","text":"<p>Configuration is handled at the dataset level:</p> <pre><code># Dataset with caching enabled\ndataset = ParquetDataset(\n    \"data/large_dataset\",\n    cached=True,\n    cache_storage=\"/tmp/pydala_cache\"\n)\n\n# S3 dataset\ns3_dataset = ParquetDataset(\n    \"s3://my-bucket/data\",\n    bucket=\"my-bucket\",\n    key=\"your-access-key\",\n    secret=\"your-secret-key\"\n)\n\n# Partitioned dataset\npartitioned_dataset = ParquetDataset(\n    \"data/partitioned\",\n    partitioning=\"hive\"  # or [\"year\", \"month\"]\n)\n</code></pre>"},{"location":"getting-started/#catalog-setup","title":"Catalog Setup","text":"<p>The catalog system helps you manage multiple datasets through a YAML configuration:</p> <pre><code>from pydala import Catalog\n\n# Create catalog YAML\ncatalog_config = \"\"\"\ntables:\n  production:\n    sales:\n      path: /data/sales\n      format: parquet\n      options:\n        partitioning: hive\n        cached: true\n    customers:\n      path: /data/customers\n      format: parquet\n  staging:\n    temp_data:\n      path: /staging/temp\n      format: parquet\n\"\"\"\n\n# Write catalog file\nwith open(\"catalog.yaml\", \"w\") as f:\n    f.write(catalog_config)\n\n# Load catalog\ncatalog = Catalog(\"catalog.yaml\", namespace=\"production\")\n\n# Access datasets\nsales = catalog.get_table(\"sales\")\ncustomers = catalog.get_table(\"customers\")\n\n# Both datasets are registered in DuckDB\nresult = sales.ddb_con.sql(\"\"\"\n    SELECT s.*, c.city\n    FROM sales s\n    JOIN customers c ON s.customer_id = c.id\n    WHERE s.date &gt;= '2023-01-01'\n\"\"\")\n</code></pre>"},{"location":"getting-started/#working-with-different-data-formats","title":"Working with Different Data Formats","text":"<p>PyDala2 accepts multiple data formats for writing:</p> <pre><code># Polars DataFrame (recommended)\ndataset.write_to_dataset(pl_df)\n\n# PyArrow Table\nimport pyarrow as pa\ndataset.write_to_dataset(pa_table)\n\n# Pandas DataFrame\ndataset.write_to_dataset(pd_df)\n\n# DuckDB relation\ndataset.write_to_dataset(duckdb_relation)\n</code></pre>"},{"location":"getting-started/#metadata-management","title":"Metadata Management","text":"<p>PyDala2 automatically manages metadata:</p> <pre><code># After writing, metadata files are created:\n# - _metadata: Combined Parquet metadata for all files\n# - _file_metadata: Per-file statistics (JSON, brotli compressed)\n\n# View dataset information\nprint(f\"Dataset name: {dataset.name}\")\nprint(f\"Partition columns: {dataset.partition_names}\")\nprint(f\"Row count: {dataset.count_rows()}\")\n\n# Update metadata (after manual file changes)\ndataset.update_metadata()\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have PyDala2 installed and configured:</p> <ol> <li>Learn basic operations in the User Guide</li> <li>Explore advanced features in the Performance Guide</li> <li>**Check the API Reference](api/core.md) for detailed documentation</li> <li>See real-world examples in the Integration Patterns</li> </ol>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#common-issues","title":"Common Issues","text":"<p>ImportError: No module named 'pydala' - Use <code>from pydala import ParquetDataset</code> (not pydala2) - Ensure PyDala2 is installed correctly</p> <p>Permission denied when writing datasets - Check file permissions for the target directory - PyDala2 auto-creates directories if needed</p> <p>DuckDB connection issues - DuckDB connection is automatically created - Use <code>dataset.ddb_con</code> to access it directly</p>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<ul> <li>Check the Troubleshooting Guide</li> <li>Browse existing issues on GitHub</li> <li>Create a new issue with detailed error information</li> </ul> <pre><code># Enable debug logging for troubleshooting\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Your code here\n</code></pre>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Get up and running with PyDala2 in minutes with this quick start guide.</p>"},{"location":"quick-start/#installation","title":"Installation","text":"<pre><code>pip install pydala2\n</code></pre>"},{"location":"quick-start/#basic-operations","title":"Basic Operations","text":""},{"location":"quick-start/#1-create-and-write-data","title":"1. Create and Write Data","text":"<pre><code>import polars as pl\nfrom pydala import ParquetDataset\n\n# Sample data\ndata = pl.DataFrame({\n    'id': range(1, 1001),\n    'name': [f'User_{i}' for i in range(1, 1001)],\n    'value': [i * 2 for i in range(1, 1001)],\n    'category': ['A', 'B', 'C', 'D'] * 250\n})\n\n# Create dataset (directory auto-created)\ndataset = ParquetDataset(\"quickstart/data\")\n\n# Write data with optimization\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"category\"],\n    max_rows_per_file=250000,\n    compression=\"zstd\"\n)\n</code></pre>"},{"location":"quick-start/#2-read-and-filter-data","title":"2. Read and Filter Data","text":"<pre><code>from pydala import ParquetDataset\n\ndataset = ParquetDataset(\"quickstart/data\")\n\n# Read all data as Polars DataFrame\nall_data = dataset.table.pl.collect()\n\n# Filter data - automatic backend selection\nfiltered = dataset.filter(\"value &gt; 100 AND category IN ('A', 'C')\")\nprint(filtered.collect())\n\n# Use SQL directly via DuckDB connection\nsql_result = dataset.ddb_con.sql(\"\"\"\n    SELECT category, AVG(value) as avg_value\n    FROM dataset\n    GROUP BY category\n\"\"\").pl()\n</code></pre>"},{"location":"quick-start/#3-work-with-different-data-formats","title":"3. Work with Different Data Formats","text":"<pre><code># PyArrow Table\narrow_table = dataset.table.arrow()\n\n# DuckDB operations\nduckdb_result = dataset.ddb_con.sql(\"\"\"\n    SELECT COUNT(*) as count\n    FROM dataset\n    WHERE value &gt; 500\n\"\"\").pl()\n\n# Convert to Pandas if needed\npandas_df = filtered.collect().to_pandas()\n</code></pre>"},{"location":"quick-start/#4-catalog-management","title":"4. Catalog Management","text":"<pre><code>from pydala import Catalog\n\n# Create catalog from YAML\ncatalog = Catalog(\"catalog.yaml\")\n\n# Register datasets\ncatalog.load_parquet(\"main_data\", \"quickstart/data\")\n\n# Get dataset from catalog\ndataset = catalog.get_table(\"main_data\")\n\n# Dataset is automatically registered in DuckDB\nresult = dataset.ddb_con.sql(\"SELECT category, COUNT(*) FROM dataset GROUP BY category\")\n</code></pre>"},{"location":"quick-start/#advanced-features","title":"Advanced Features","text":""},{"location":"quick-start/#delta-updates","title":"Delta Updates","text":"<pre><code># Add new data efficiently\nnew_data = pl.DataFrame({\n    'id': range(1001, 1501),\n    'name': [f'User_{i}' for i in range(1001, 1501)],\n    'value': [i * 2 for i in range(1001, 1501)],\n    'category': ['A', 'B', 'C', 'D'] * 125\n})\n\n# Delta mode merges with existing data\ndataset.write_to_dataset(\n    new_data,\n    mode=\"delta\",\n    partition_by=[\"category\"]\n)\n</code></pre>"},{"location":"quick-start/#schema-evolution","title":"Schema Evolution","text":"<pre><code># Add new column\ndata_with_new_col = data.with_columns(\n    pl.col(\"value\").alias(\"double_value\") * 2\n)\n\n# Allow schema changes\ndataset.write_to_dataset(\n    data_with_new_col,\n    mode=\"append\",\n    alter_schema=True\n)\n</code></pre>"},{"location":"quick-start/#optimization-operations","title":"Optimization Operations","text":"<pre><code># Compact small files\ndataset.compact_partitions(\n    max_rows_per_file=500000,\n    sort_by=\"id\"\n)\n\n# Optimize data types\ndataset.optimize_dtypes()\n\n# Update metadata for better performance\ndataset.update_metadata()\n</code></pre>"},{"location":"quick-start/#time-based-operations","title":"Time-based Operations","text":"<pre><code># Time series data\ndates = pl.date_range(start=2023-01-01, end=2023-12-31, interval=\"1d\", eager=True)\ntime_data = pl.DataFrame({\n    'timestamp': dates[:1000],\n    'value': range(1000),\n    'metric': ['cpu', 'memory', 'disk'] * 333 + ['cpu']\n})\n\n# Write with time-based partitioning\ntime_dataset = ParquetDataset(\"metrics/data\")\ntime_dataset.write_to_dataset(\n    time_data,\n    partition_by=[\"metric\"],\n    timestamp_column=\"timestamp\"\n)\n\n# Time-based compaction\ntime_dataset.compact_by_timeperiod(\n    interval=\"1 month\",\n    timestamp_column=\"timestamp\"\n)\n</code></pre>"},{"location":"quick-start/#complete-example","title":"Complete Example","text":"<p>Here's a complete workflow example:</p> <pre><code>import polars as pl\nfrom datetime import datetime\nfrom pydala import ParquetDataset, Catalog\n\n# 1. Generate sample data\ndates = pl.date_range('2023-01-01', periods=100, interval='1d', eager=True)\ndata = pl.DataFrame({\n    'date': dates,\n    'product_id': range(100),\n    'sales': [100 + i * 10 for i in range(100)],\n    'region': ['North', 'South', 'East', 'West'] * 25\n})\n\n# 2. Create and write dataset\ndataset = ParquetDataset(\"sales_data\")\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"region\"],\n    max_rows_per_file=50,\n    compression=\"zstd\"\n)\n\n# 3. Query with filtering\nresult = dataset.filter(\"\"\"\n    date &gt;= '2023-02-01'\n    AND region IN ('North', 'South')\n\"\"\")\n\n# 4. Aggregation via DuckDB\nagg_result = dataset.ddb_con.sql(\"\"\"\n    SELECT\n        region,\n        DATE_TRUNC('month', date) as month,\n        SUM(sales) as total_sales,\n        COUNT(*) as order_count\n    FROM dataset\n    WHERE date &gt;= '2023-02-01'\n    GROUP BY region, month\n    ORDER BY total_sales DESC\n\"\"\").pl()\n\nprint(agg_result)\n\n# 5. Optimize the dataset\ndataset.compact_partitions(\n    max_rows_per_file=100,\n    sort_by=\"date DESC\"\n)\n</code></pre>"},{"location":"quick-start/#working-with-cloud-storage","title":"Working with Cloud Storage","text":"<pre><code># S3 dataset\ns3_dataset = ParquetDataset(\n    \"s3://my-bucket/sales-data\",\n    bucket=\"my-bucket\",\n    key=\"your-access-key\",\n    secret=\"your-secret-key\",\n    cached=True,  # Enable local caching\n    cache_storage=\"/tmp/s3-cache\"\n)\n\n# Write to S3\ns3_dataset.write_to_dataset(data, partition_by=[\"region\"])\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcda User Guide - Learn advanced operations</li> <li>\u26a1 Performance Guide - Optimize your workflows</li> <li>\ud83d\udd27 API Reference - Detailed API documentation</li> <li>\ud83c\udfd7\ufe0f Deployment Guide - Production best practices</li> </ul>"},{"location":"quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"quick-start/#partitioning-strategies","title":"Partitioning Strategies","text":"<pre><code># Date-based partitioning\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"year\", \"month\", \"day\"]\n)\n\n# Mixed partitioning\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"region\", \"category\"]\n)\n\n# Hive-style partitioning\ndataset = ParquetDataset(\n    \"data/hive\",\n    partitioning=\"hive\"  # Automatically detects from paths\n)\n</code></pre>"},{"location":"quick-start/#large-dataset-processing","title":"Large Dataset Processing","text":"<pre><code># Process in batches using DuckDB\nbatch_size = 100000\ntotal_rows = dataset.ddb_con.sql(\"SELECT COUNT(*) FROM dataset\").fetchone()[0]\n\nfor offset in range(0, total_rows, batch_size):\n    batch = dataset.ddb_con.sql(f\"\"\"\n        SELECT * FROM dataset\n        LIMIT {batch_size} OFFSET {offset}\n    \"\"\").pl()\n    process_batch(batch)\n</code></pre>"},{"location":"advanced/deployment/","title":"Deployment Guide","text":"<p>This guide covers deployment considerations and best practices for running PyDala2 in production environments.</p>"},{"location":"advanced/deployment/#architecture-overview","title":"Architecture Overview","text":""},{"location":"advanced/deployment/#single-node-deployment","title":"Single Node Deployment","text":"graph TD     A[Application] --&gt; B[PyDala2]     B --&gt; C[Local Storage]     B --&gt; D[Remote StorageS3/GCS/Azure]     B --&gt; E[DatabaseDuckDB]"},{"location":"advanced/deployment/#distributed-deployment","title":"Distributed Deployment","text":"graph TD     A[Load Balancer]     A --&gt; B[App Server 1]     A --&gt; C[App Server 2]     A --&gt; D[App Server N]     B --&gt; E[PyDala2]     C --&gt; E     D --&gt; E     E --&gt; F[Shared Storage]     E --&gt; G[Metadata Store]"},{"location":"advanced/deployment/#system-requirements","title":"System Requirements","text":""},{"location":"advanced/deployment/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>Memory: 4GB RAM minimum</li> <li>Storage: 10GB free space</li> <li>CPU: 2 cores minimum</li> </ul>"},{"location":"advanced/deployment/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>Memory: 16GB RAM or more</li> <li>Storage: SSD with 100GB+ free space</li> <li>CPU: 4+ cores</li> <li>Network: Gigabit Ethernet for cloud storage</li> </ul>"},{"location":"advanced/deployment/#installation","title":"Installation","text":""},{"location":"advanced/deployment/#production-installation","title":"Production Installation","text":"<pre><code># Create virtual environment\npython -m venv pydala2_env\nsource pydala2_env/bin/activate\n\n# Install with all dependencies\npip install pydala2[all]\n\n# Or install specific components\npip install pydala2[polars,duckdb,pandas]\n</code></pre>"},{"location":"advanced/deployment/#docker-deployment","title":"Docker Deployment","text":"<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gcc \\\n    g++ \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application\nCOPY . .\n\n# Set environment variables\nENV PYDALA2_CACHE_DIR=/cache\nENV PYDALA2_LOG_LEVEL=INFO\n\n# Create cache directory\nRUN mkdir -p /cache\n\n# Run application\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"advanced/deployment/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pydala2-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: pydala2\n  template:\n    metadata:\n      labels:\n        app: pydala2\n    spec:\n      containers:\n      - name: app\n        image: pydala2-app:latest\n        resources:\n          requests:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2\"\n        env:\n        - name: PYDALA2_CACHE_DIR\n          value: \"/cache\"\n        - name: PYDALA2_MAX_MEMORY\n          value: \"4294967296\"\n        volumeMounts:\n        - name: cache-volume\n          mountPath: /cache\n      volumes:\n      - name: cache-volume\n        emptyDir:\n          sizeLimit: 10Gi\n</code></pre>"},{"location":"advanced/deployment/#configuration","title":"Configuration","text":""},{"location":"advanced/deployment/#environment-variables","title":"Environment Variables","text":"<pre><code># Core configuration\nexport PYDALA2_DEFAULT_BACKEND=polars\nexport PYDALA2_CACHE_ENABLED=true\nexport PYDALA2_CACHE_DIR=/tmp/pydala2_cache\nexport PYDALA2_CACHE_MAX_SIZE=4294967296  # 4GB\nexport PYDALA2_LOG_LEVEL=INFO\n\n# Performance tuning\nexport PYDALA2_MAX_MEMORY=8589934592  # 8GB\nexport PYDALA2_N_WORKERS=4\nexport PYDALA2_IO_THREADS=8\n\n# Cloud storage\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=us-east-1\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n</code></pre>"},{"location":"advanced/deployment/#configuration-file","title":"Configuration File","text":"<pre><code># config.py\nfrom pydala import set_config\n\n# Production configuration\nPROD_CONFIG = {\n    'default_backend': 'polars',\n    'cache_enabled': True,\n    'cache_max_size': 8 * 1024 * 1024 * 1024,  # 8GB\n    'cache_ttl': 3600,  # 1 hour\n    'max_memory': 16 * 1024 * 1024 * 1024,  # 16GB\n    'n_workers': 8,\n    'io_threads': 16,\n    'compression': 'zstd',\n    'compression_level': 3,\n    'row_group_size': 1000000,\n    'validate_schema': True,\n    'enable_profiling': True,\n    'log_level': 'INFO'\n}\n\n# Apply configuration\nset_config(PROD_CONFIG)\n</code></pre>"},{"location":"advanced/deployment/#storage-configuration","title":"Storage Configuration","text":""},{"location":"advanced/deployment/#local-storage","title":"Local Storage","text":"<pre><code># High-performance local storage\nfrom pydala import ParquetDataset\n\ndataset = ParquetDataset(\n    \"/data/production\",\n    cached=True,\n    cache_storage=\"/fast_ssd/cache\",\n    filesystem_kwargs={\n        'auto_mkdir': True,\n        'use_listings_cache': True,\n        'listings_expiry_time': 300\n    }\n)\n</code></pre>"},{"location":"advanced/deployment/#s3-configuration","title":"S3 Configuration","text":"<pre><code># Production S3 setup\nfrom pydala import ParquetDataset, FileSystem\n\n# Optimized S3 filesystem\ns3_fs = FileSystem(\n    protocol=\"s3\",\n    bucket=\"production-data\",\n    cached=True,\n    cache_storage=\"/s3_cache\",\n    client_kwargs={\n        \"region_name\": \"us-east-1\",\n        \"endpoint_url\": \"https://s3.us-east-1.amazonaws.com\"\n    },\n    config_kwargs={\n        \"max_pool_connections\": 50,\n        \"retry_mode\": \"adaptive\"\n    }\n)\n\ndataset = ParquetDataset(\n    \"s3://production-data/sales\",\n    filesystem=s3_fs,\n    cached=True\n)\n</code></pre>"},{"location":"advanced/deployment/#gcs-configuration","title":"GCS Configuration","text":"<pre><code># GCS with service account\ngcs_fs = FileSystem(\n    protocol=\"gcs\",\n    bucket=\"production-data\",\n    cached=True,\n    cache_storage=\"/gcs_cache\",\n    token=\"/secrets/gcp-service-account.json\",\n    project=\"my-project\"\n)\n</code></pre>"},{"location":"advanced/deployment/#performance-optimization","title":"Performance Optimization","text":""},{"location":"advanced/deployment/#caching-strategy","title":"Caching Strategy","text":"<pre><code># Multi-level caching\nimport os\nfrom pydala import set_config\n\n# Configure tiered caching\nset_config({\n    'cache_enabled': True,\n    'cache_max_size': 8 * 1024 * 1024 * 1024,  # 8GB total\n    'cache_storage': {\n        'level1': '/dev/shm/pydala2',  # Memory cache\n        'level2': '/fast_ssd/cache',   # SSD cache\n        'level3': '/slow_hdd/cache'    # HDD cache\n    }\n})\n</code></pre>"},{"location":"advanced/deployment/#connection-pooling","title":"Connection Pooling","text":"<pre><code># Reuse connections across operations\nfrom pydala import Catalog, ParquetDataset\nimport duckdb\n\n# Shared DuckDB connection\nddb_con = duckdb.connect()\n\n# Configure for high concurrency\nddb_con.execute(\"SET threads=8\")\nddb_con.execute(\"SET memory_limit='8GB'\")\nddb_con.execute(\"SET enable_object_cache=true\")\n\n# Use in multiple datasets\ncatalog = Catalog(\"catalog.yaml\", ddb_con=ddb_con)\ndataset1 = ParquetDataset(\"data1\", ddb_con=ddb_con)\ndataset2 = ParquetDataset(\"data2\", ddb_con=ddb_con)\n</code></pre>"},{"location":"advanced/deployment/#memory-management","title":"Memory Management","text":"<pre><code># Monitor and manage memory usage\nimport psutil\nfrom pydala import get_memory_usage\n\ndef check_memory():\n    \"\"\"Check memory usage and take action if needed.\"\"\"\n    mem = get_memory_usage()\n    usage_percent = mem['used'] / mem['total'] * 100\n\n    if usage_percent &gt; 90:\n        print(f\"High memory usage: {usage_percent:.1f}%\")\n        # Clear caches\n        clear_all_caches()\n        # Trigger garbage collection\n        import gc\n        gc.collect()\n\ndef clear_all_caches():\n    \"\"\"Clear all PyDala2 caches.\"\"\"\n    from pydala.filesystem import clear_cache\n    clear_cache()\n</code></pre>"},{"location":"advanced/deployment/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"advanced/deployment/#structured-logging","title":"Structured Logging","text":"<pre><code># Configure structured logging\nimport logging\nimport json\nfrom datetime import datetime\n\nclass JSONFormatter(logging.Formatter):\n    def format(self, record):\n        log_entry = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'level': record.levelname,\n            'message': record.getMessage(),\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n        if hasattr(record, 'extra'):\n            log_entry.update(record.extra)\n        return json.dumps(log_entry)\n\n# Set up logger\nlogger = logging.getLogger('pydala2')\nhandler = logging.StreamHandler()\nhandler.setFormatter(JSONFormatter())\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n</code></pre>"},{"location":"advanced/deployment/#metrics-collection","title":"Metrics Collection","text":"<pre><code># Custom metrics collection\nimport time\nfrom collections import defaultdict\n\nclass MetricsCollector:\n    def __init__(self):\n        self.metrics = defaultdict(list)\n\n    def time_operation(self, operation_name):\n        \"\"\"Decorator to time operations.\"\"\"\n        def decorator(func):\n            def wrapper(*args, **kwargs):\n                start_time = time.time()\n                try:\n                    result = func(*args, **kwargs)\n                    success = True\n                except Exception as e:\n                    success = False\n                    raise e\n                finally:\n                    duration = time.time() - start_time\n                    self.metrics[operation_name].append({\n                        'duration': duration,\n                        'success': success,\n                        'timestamp': time.time()\n                    })\n                return result\n            return wrapper\n        return decorator\n\n    def get_stats(self, operation_name):\n        \"\"\"Get statistics for an operation.\"\"\"\n        durations = [m['duration'] for m in self.metrics[operation_name]]\n        if not durations:\n            return None\n\n        return {\n            'count': len(durations),\n            'avg': sum(durations) / len(durations),\n            'min': min(durations),\n            'max': max(durations),\n            'p95': sorted(durations)[int(len(durations) * 0.95)]\n        }\n\n# Usage\nmetrics = MetricsCollector()\n\n@metrics.time_operation('dataset_read')\ndef read_data():\n    return dataset.read()\n</code></pre>"},{"location":"advanced/deployment/#health-checks","title":"Health Checks","text":"<pre><code># Health check endpoint\nfrom fastapi import FastAPI, HTTPException\nfrom pydala import Catalog, ParquetDataset\n\napp = FastAPI()\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    try:\n        # Check basic functionality\n        catalog = Catalog(\"catalog.yaml\")\n        dataset = ParquetDataset(\"test_dataset\")\n\n        # Test read operation\n        if dataset.exists():\n            count = dataset.count_rows()\n\n        return {\n            \"status\": \"healthy\",\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"version\": \"1.0.0\"\n        }\n    except Exception as e:\n        raise HTTPException(\n            status_code=503,\n            detail=f\"Service unhealthy: {str(e)}\"\n        )\n</code></pre>"},{"location":"advanced/deployment/#security","title":"Security","text":""},{"location":"advanced/deployment/#access-control","title":"Access Control","text":"<pre><code># Implement access control\nclass AccessControl:\n    def __init__(self):\n        self.permissions = {}\n\n    def check_access(self, user, resource, action):\n        \"\"\"Check if user has permission for action on resource.\"\"\"\n        key = f\"{user}:{resource}\"\n        return self.permissions.get(key, {}).get(action, False)\n\n    def grant_permission(self, user, resource, action):\n        \"\"\"Grant permission to user.\"\"\"\n        key = f\"{user}:{resource}\"\n        if key not in self.permissions:\n            self.permissions[key] = {}\n        self.permissions[key][action] = True\n\n# Usage\naccess_control = AccessControl()\naccess_control.grant_permission(\"analyst1\", \"sales_data\", \"read\")\n\n# Check before operation\nif access_control.check_access(current_user, \"sales_data\", \"read\"):\n    data = catalog.get_dataset(\"sales_data\").read()\n</code></pre>"},{"location":"advanced/deployment/#data-encryption","title":"Data Encryption","text":"<pre><code># Client-side encryption\nfrom cryptography.fernet import Fernet\nimport base64\n\nclass DataEncryptor:\n    def __init__(self, key=None):\n        if key:\n            self.key = key\n        else:\n            self.key = Fernet.generate_key()\n        self.cipher = Fernet(self.key)\n\n    def encrypt_data(self, data):\n        \"\"\"Encrypt data.\"\"\"\n        if isinstance(data, str):\n            data = data.encode()\n        return self.cipher.encrypt(data)\n\n    def decrypt_data(self, encrypted_data):\n        \"\"\"Decrypt data.\"\"\"\n        decrypted = self.cipher.decrypt(encrypted_data)\n        return decrypted.decode()\n\n# Usage in dataset operations\nencryptor = DataEncryptor()\n\n# Encrypt sensitive columns before writing\ndef write_encrypted(dataset, data, sensitive_columns):\n    for col in sensitive_columns:\n        data[col] = data[col].apply(encryptor.encrypt_data)\n    dataset.write(data)\n\n# Decrypt after reading\ndef read_decrypted(dataset, sensitive_columns):\n    data = dataset.read()\n    for col in sensitive_columns:\n        data[col] = data[col].apply(encryptor.decrypt_data)\n    return data\n</code></pre>"},{"location":"advanced/deployment/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"advanced/deployment/#dataset-backup","title":"Dataset Backup","text":"<pre><code># Backup strategy\nimport shutil\nfrom datetime import datetime\nimport tarfile\n\ndef backup_dataset(dataset_path, backup_dir):\n    \"\"\"Create a compressed backup of a dataset.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_name = f\"backup_{timestamp}.tar.gz\"\n    backup_path = os.path.join(backup_dir, backup_name)\n\n    with tarfile.open(backup_path, \"w:gz\") as tar:\n        tar.add(dataset_path, arcname=os.path.basename(dataset_path))\n\n    return backup_path\n\ndef restore_dataset(backup_path, restore_dir):\n    \"\"\"Restore a dataset from backup.\"\"\"\n    with tarfile.open(backup_path, \"r:gz\") as tar:\n        tar.extractall(restore_dir)\n</code></pre>"},{"location":"advanced/deployment/#catalog-backup","title":"Catalog Backup","text":"<pre><code># Backup catalog configuration\nimport yaml\nfrom pathlib import Path\n\ndef backup_catalog(catalog_path, backup_dir):\n    \"\"\"Backup catalog configuration.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_path = Path(backup_dir) / f\"catalog_backup_{timestamp}.yaml\"\n\n    # Read original catalog\n    with open(catalog_path, 'r') as f:\n        catalog_data = yaml.safe_load(f)\n\n    # Add backup metadata\n    catalog_data['_backup_metadata'] = {\n        'timestamp': timestamp,\n        'version': '1.0'\n    }\n\n    # Write backup\n    with open(backup_path, 'w') as f:\n        yaml.safe_dump(catalog_data, f, default_flow_style=False)\n\n    return str(backup_path)\n</code></pre>"},{"location":"advanced/deployment/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"advanced/deployment/#horizontal-scaling","title":"Horizontal Scaling","text":"<pre><code># Distributed catalog\nfrom pydala import Catalog\n\nclass DistributedCatalog:\n    def __init__(self, catalog_urls):\n        self.catalogs = [Catalog(url) for url in catalog_urls]\n\n    def get_dataset(self, name):\n        \"\"\"Get dataset from any available catalog.\"\"\"\n        for catalog in self.catalogs:\n            try:\n                return catalog.get_dataset(name)\n            except:\n                continue\n        raise ValueError(f\"Dataset {name} not found in any catalog\")\n\n# Usage\ncatalog_urls = [\n    \"http://catalog1:8000/catalog.yaml\",\n    \"http://catalog2:8000/catalog.yaml\",\n    \"http://catalog3:8000/catalog.yaml\"\n]\ndist_catalog = DistributedCatalog(catalog_urls)\n</code></pre>"},{"location":"advanced/deployment/#load-balancing","title":"Load Balancing","text":"<pre><code># Round-robin dataset access\nimport itertools\n\nclass LoadBalancedDataset:\n    def __init__(self, dataset_paths):\n        self.datasets = [\n            ParquetDataset(path)\n            for path in dataset_paths\n        ]\n        self.cycle = itertools.cycle(self.datasets)\n\n    def read(self, **kwargs):\n        \"\"\"Read from next dataset in rotation.\"\"\"\n        dataset = next(self.cycle)\n        return dataset.read(**kwargs)\n\n# Usage\nlb_dataset = LoadBalancedDataset([\n    \"/data/replica1/sales\",\n    \"/data/replica2/sales\",\n    \"/data/replica3/sales\"\n])\n</code></pre>"},{"location":"advanced/deployment/#deployment-checklist","title":"Deployment Checklist","text":"<ul> <li> Configure appropriate caching strategy</li> <li> Set up monitoring and logging</li> <li> Implement security controls</li> <li> Create backup procedures</li> <li> Test disaster recovery</li> <li> Configure alerting</li> <li> Document deployment process</li> <li> Set up CI/CD pipeline</li> <li> Perform load testing</li> <li> Optimize for specific workload</li> </ul>"},{"location":"advanced/integration/","title":"Integration Patterns","text":"<p>This guide covers common integration patterns and use cases for PyDala2 with other tools and frameworks.</p>"},{"location":"advanced/integration/#data-pipeline-integration","title":"Data Pipeline Integration","text":""},{"location":"advanced/integration/#apache-airflow","title":"Apache Airflow","text":"<pre><code># Airflow DAG with PyDala2\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime, timedelta\nfrom pydala import ParquetDataset, Catalog\n\ndef process_daily_sales():\n    \"\"\"Process daily sales data.\"\"\"\n    # Create dataset\n    dataset = ParquetDataset(\"s3://sales-bucket/daily\")\n\n    # Read yesterday's data\n    yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    data = dataset.filter(f\"date = '{yesterday}'\").table.to_polars()\n\n    # Process data\n    processed = (\n        data\n        .filter(pl.col(\"status\") == \"completed\")\n        .with_columns([\n            (pl.col(\"amount\") * 0.1).alias(\"tax\"),\n            (pl.col(\"amount\") * 1.1).alias(\"total\")\n        ])\n    )\n\n    # Write to processed bucket\n    processed_dataset = ParquetDataset(\"s3://processed-bucket/sales\")\n    processed_dataset.write_to_dataset(processed)\n\ndef update_dashboard():\n    \"\"\"Update dashboard with latest metrics.\"\"\"\n    catalog = Catalog(\"s3://metadata-bucket/catalog.yaml\")\n\n    # Calculate daily metrics\n    metrics = catalog.ddb_con.sql(\"\"\"\n        SELECT\n            date,\n            COUNT(*) as orders,\n            SUM(amount) as revenue,\n            AVG(amount) as avg_order\n        FROM daily_sales\n        WHERE date = CURRENT_DATE - INTERVAL '1 day'\n        GROUP BY date\n    \"\"\").to_arrow()\n\n    # Update dashboard database\n    dashboard_db = ParquetDataset(\"s3://dashboard-bucket/metrics\")\n    dashboard_db.write_to_dataset(metrics)\n\nwith DAG(\n    'daily_sales_processing',\n    start_date=datetime(2023, 1, 1),\n    schedule_interval='0 6 * * *',  # Daily at 6 AM\n    catchup=False\n) as dag:\n    process_task = PythonOperator(\n        task_id='process_daily_sales',\n        python_callable=process_daily_sales\n    )\n\n    update_task = PythonOperator(\n        task_id='update_dashboard',\n        python_callable=update_dashboard\n    )\n\n    process_task &gt;&gt; update_task\n</code></pre>"},{"location":"advanced/integration/#prefect","title":"Prefect","text":"<pre><code># Prefect flow with PyDala2\nfrom prefect import flow, task\nfrom prefect.context import get_run_context\nfrom pydala import ParquetDataset\n\n@task\ndef extract_data(source_path: str, date_filter: str):\n    \"\"\"Extract data from source.\"\"\"\n    dataset = ParquetDataset(source_path)\n    return dataset.filter(f\"date &gt;= '{date_filter}'\").table.to_polars()\n\n@task\ndef transform_data(raw_data):\n    \"\"\"Transform raw data.\"\"\"\n    return (\n        raw_data\n        .filter(pl.col(\"status\").is_in([\"completed\", \"shipped\"]))\n        .group_by([\"customer_id\", \"product_category\"])\n        .agg([\n            pl.count(\"order_id\").alias(\"order_count\"),\n            pl.sum(\"amount\").alias(\"total_spent\"),\n            pl.col(\"order_date\").max().alias(\"last_order_date\")\n        ])\n    )\n\n@task\ndef load_data(transformed_data, target_path: str):\n    \"\"\"Load transformed data.\"\"\"\n    dataset = ParquetDataset(target_path)\n    dataset.write(transformed_data, mode='append')\n\n@flow(name=\"ETL Pipeline\")\ndef etl_pipeline(source_path: str, target_path: str, start_date: str):\n    \"\"\"Main ETL pipeline.\"\"\"\n    # Get flow run context\n    context = get_run_context()\n    run_id = context.flow_run.id\n\n    # Extract\n    raw_data = extract_data(source_path, start_date)\n\n    # Transform\n    transformed_data = transform_data(raw_data)\n\n    # Load\n    load_data(transformed_data, target_path)\n\n    return f\"Processed {len(transformed_data)} customer records\"\n</code></pre>"},{"location":"advanced/integration/#machine-learning-integration","title":"Machine Learning Integration","text":""},{"location":"advanced/integration/#feature-store-integration","title":"Feature Store Integration","text":"<pre><code># Feature store with PyDala2\nimport pyarrow as pa\nfrom pydala import ParquetDataset, Catalog\n\nclass FeatureStore:\n    \"\"\"Simple feature store implementation.\"\"\"\n\n    def __init__(self, catalog_path: str):\n        self.catalog = Catalog(catalog_path)\n\n    def create_feature_view(self, name: str, query: str):\n        \"\"\"Create a feature view.\"\"\"\n        # Register feature view in catalog\n        self.catalog._set_table_params(\n            f\"feature_views.{name}\",\n            query=query,\n            type=\"feature_view\",\n            created_at=datetime.now().isoformat()\n        )\n\n    def get_features(self, view_name: str, entity_ids: list, feature_date: str):\n        \"\"\"Get features for entities.\"\"\"\n        query = self.catalog.get(f\"feature_views.{view_name}\").query\n\n        # Execute query with parameters\n        return self.catalog.query(\n            query,\n            parameters={\n                \"entity_ids\": entity_ids,\n                \"feature_date\": feature_date\n            }\n        )\n\n# Usage\nfs = FeatureStore(\"s3://feature-store/catalog.yaml\")\n\n# Create feature view\nfs.create_feature_view(\n    \"customer_features\",\n    \"\"\"\n    SELECT\n        customer_id,\n        COUNT(DISTINCT order_id) as order_count_30d,\n        SUM(amount) as total_spent_30d,\n        AVG(amount) as avg_order_value_30d,\n        DATEDIFF('day', MAX(order_date), ?) as days_since_last_order\n    FROM sales\n    WHERE order_date &gt;= DATE_SUB(?, INTERVAL 30 DAY)\n      AND order_date &lt;= ?\n    GROUP BY customer_id\n    \"\"\"\n)\n\n# Get features for ML\nfeatures = fs.get_features(\n    \"customer_features\",\n    entity_ids=[1, 2, 3, 4, 5],\n    feature_date=\"2023-12-01\"\n)\n</code></pre>"},{"location":"advanced/integration/#model-training-integration","title":"Model Training Integration","text":"<pre><code># PyDala2 with scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport joblib\n\ndef train_churn_model(dataset_path: str):\n    \"\"\"Train churn prediction model.\"\"\"\n    # Load data\n    dataset = ParquetDataset(dataset_path)\n    data = dataset.read(backend=\"pandas\")\n\n    # Prepare features\n    features = data.drop(['customer_id', 'churned'], axis=1)\n    labels = data['churned']\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        features, labels, test_size=0.2, random_state=42\n    )\n\n    # Train model\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n\n    # Evaluate\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Model accuracy: {accuracy:.2f}\")\n\n    # Save model and metadata\n    model_path = f\"models/churn_model_v{datetime.now().strftime('%Y%m%d')}.pkl\"\n    joblib.dump(model, model_path)\n\n    # Save training metadata\n    metadata = {\n        \"model_path\": model_path,\n        \"accuracy\": accuracy,\n        \"training_date\": datetime.now().isoformat(),\n        \"features\": list(features.columns),\n        \"n_samples\": len(data)\n    }\n\n    metadata_dataset = ParquetDataset(\"models/metadata\")\n    metadata_dataset.write(\n        pd.DataFrame([metadata]),\n        mode=\"append\"\n    )\n\n    return model, accuracy\n</code></pre>"},{"location":"advanced/integration/#batch-prediction","title":"Batch Prediction","text":"<pre><code># Batch prediction with PyDala2\ndef batch_predict(model_path: str, input_path: str, output_path: str):\n    \"\"\"Run batch predictions.\"\"\"\n    # Load model\n    model = joblib.load(model_path)\n\n    # Load input data\n    input_dataset = ParquetDataset(input_path)\n    data = input_dataset.read(backend=\"pandas\")\n\n    # Prepare features\n    features = data.drop(['customer_id'], axis=1)\n\n    # Make predictions\n    predictions = model.predict_proba(features)[:, 1]\n    results = data[['customer_id']].copy()\n    results['churn_probability'] = predictions\n\n    # Save results\n    output_dataset = ParquetDataset(output_path)\n    output_dataset.write(results, mode='overwrite')\n\n    # Generate summary statistics\n    summary = {\n        \"prediction_date\": datetime.now().isoformat(),\n        \"total_predictions\": len(results),\n        \"high_risk_customers\": int((predictions &gt; 0.7).sum()),\n        \"avg_churn_probability\": float(predictions.mean())\n    }\n\n    return summary\n</code></pre>"},{"location":"advanced/integration/#web-application-integration","title":"Web Application Integration","text":""},{"location":"advanced/integration/#fastapi-integration","title":"FastAPI Integration","text":"<pre><code># FastAPI with PyDala2\nfrom fastapi import FastAPI, HTTPException, Query\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydala import Catalog, ParquetDataset\nimport polars as pl\n\napp = FastAPI()\n\n# Initialize catalog\ncatalog = Catalog(\"catalog.yaml\")\n\nclass SalesSummary(BaseModel):\n    date: str\n    total_sales: float\n    total_orders: int\n    avg_order_value: float\n\n@app.get(\"/api/sales/summary\", response_model=List[SalesSummary])\nasync def get_sales_summary(\n    start_date: str = Query(..., description=\"Start date (YYYY-MM-DD)\"),\n    end_date: str = Query(..., description=\"End date (YYYY-MM-DD)\"),\n    category: Optional[str] = Query(None, description=\"Product category\")\n):\n    \"\"\"Get sales summary for date range.\"\"\"\n    try:\n        # Build query\n        base_query = \"\"\"\n            SELECT\n                DATE_TRUNC('day', order_date) as date,\n                COUNT(*) as total_orders,\n                SUM(amount) as total_sales,\n                SUM(amount) / COUNT(*) as avg_order_value\n            FROM sales\n            WHERE order_date BETWEEN ? AND ?\n        \"\"\"\n        params = [start_date, end_date]\n\n        if category:\n            base_query += \" AND category = ?\"\n            params.append(category)\n\n        base_query += \" GROUP BY date ORDER BY date\"\n\n        # Execute query\n        results = catalog.query(base_query, parameters=params)\n\n        # Convert to response model\n        return [\n            SalesSummary(\n                date=str(row[\"date\"]),\n                total_sales=float(row[\"total_sales\"]),\n                total_orders=int(row[\"total_orders\"]),\n                avg_order_value=float(row[\"avg_order_value\"])\n            )\n            for row in results\n        ]\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/products/{product_id}/analytics\")\nasync def get_product_analytics(product_id: int):\n    \"\"\"Get analytics for a specific product.\"\"\"\n    try:\n        # Get product dataset\n        dataset = catalog.get_dataset(\"product_analytics\")\n\n        # Calculate analytics\n        analytics = dataset.read(\n            backend=\"polars\",\n            filters=f\"product_id = {product_id}\"\n        ).select([\n            pl.col(\"product_id\"),\n            pl.col(\"product_name\"),\n            pl.col(\"category\"),\n            pl.count(\"order_id\").alias(\"total_orders\"),\n            pl.sum(\"amount\").alias(\"total_revenue\"),\n            pl.mean(\"rating\").alias(\"avg_rating\"),\n            pl.col(\"order_date\").max().alias(\"last_ordered\")\n        ])\n\n        if len(analytics) == 0:\n            raise HTTPException(status_code=404, detail=\"Product not found\")\n\n        return analytics.to_dict(as_series=False)[0]\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"advanced/integration/#streamlit-dashboard","title":"Streamlit Dashboard","text":"<pre><code># Streamlit dashboard with PyDala2\nimport streamlit as st\nimport plotly.express as px\nfrom pydala import Catalog, ParquetDataset\nimport pandas as pd\n\n# Initialize catalog\n@st.cache_resource\ndef load_catalog():\n    return Catalog(\"catalog.yaml\")\n\ncatalog = load_catalog()\n\n# Page title\nst.title(\"Sales Analytics Dashboard\")\n\n# Sidebar filters\nst.sidebar.header(\"Filters\")\n\n# Date range\ndate_col, _ = st.columns([2, 1])\nstart_date = st.sidebar.date_input(\"Start Date\", pd.to_datetime(\"2023-01-01\"))\nend_date = st.sidebar.date_input(\"End Date\", pd.to_datetime(\"2023-12-31\"))\n\n# Category filter\ncategories = catalog.query(\"SELECT DISTINCT category FROM sales ORDER BY category\")\nselected_category = st.sidebar.selectbox(\n    \"Category\",\n    options=[\"All\"] + [row[\"category\"] for row in categories]\n)\n\n# Load data based on filters\n@st.cache_data(ttl=3600)  # Cache for 1 hour\ndef load_sales_data(start_date, end_date, category):\n    \"\"\"Load sales data with filters.\"\"\"\n    base_query = \"\"\"\n        SELECT\n            order_date,\n            category,\n            product_name,\n            amount,\n            customer_id\n        FROM sales\n        WHERE order_date BETWEEN ? AND ?\n    \"\"\"\n    params = [start_date, end_date]\n\n    if category != \"All\":\n        base_query += \" AND category = ?\"\n        params.append(category)\n\n    return catalog.query(base_query, parameters=params)\n\n# Load data\nsales_data = load_sales_data(start_date, end_date, selected_category)\n\n# Convert to DataFrame for plotting\ndf = pd.DataFrame(sales_data)\n\n# Key metrics\nst.header(\"Key Metrics\")\ncol1, col2, col3, col4 = st.columns(4)\n\nwith col1:\n    st.metric(\"Total Revenue\", f\"${df['amount'].sum():,.0f}\")\n\nwith col2:\n    st.metric(\"Total Orders\", f\"{len(df):,}\")\n\nwith col3:\n    st.metric(\"Average Order\", f\"${df['amount'].mean():.2f}\")\n\nwith col4:\n    st.metric(\"Unique Customers\", f\"{df['customer_id'].nunique():,}\")\n\n# Sales over time\nst.header(\"Sales Trend\")\ndf['order_date'] = pd.to_datetime(df['order_date'])\ndaily_sales = df.groupby(df['order_date'].dt.date)['amount'].sum().reset_index()\n\nfig = px.line(\n    daily_sales,\n    x=\"order_date\",\n    y=\"amount\",\n    title=\"Daily Sales\"\n)\nst.plotly_chart(fig, use_container_width=True)\n\n# Sales by category\nst.header(\"Sales by Category\")\ncategory_sales = df.groupby('category')['amount'].sum().reset_index()\n\nfig = px.bar(\n    category_sales,\n    x=\"category\",\n    y=\"amount\",\n    title=\"Sales by Category\"\n)\nst.plotly_chart(fig, use_container_width=True)\n\n# Top products\nst.header(\"Top Products\")\ntop_products = df.groupby('product_name')['amount'].sum().nlargest(10).reset_index()\n\nfig = px.bar(\n    top_products,\n    x=\"amount\",\n    y=\"product_name\",\n    orientation=\"h\",\n    title=\"Top 10 Products by Revenue\"\n)\nst.plotly_chart(fig, use_container_width=True)\n</code></pre>"},{"location":"advanced/integration/#database-integration","title":"Database Integration","text":""},{"location":"advanced/integration/#postgresql-integration","title":"PostgreSQL Integration","text":"<pre><code># Sync with PostgreSQL\nimport psycopg2\nfrom psycopg2.extras import execute_batch\nfrom pydala import ParquetDataset\n\ndef sync_to_postgres(dataset_path: str, table_name: str, db_config: dict):\n    \"\"\"Sync PyDala2 dataset to PostgreSQL.\"\"\"\n    # Connect to PostgreSQL\n    conn = psycopg2.connect(**db_config)\n    cursor = conn.cursor()\n\n    # Load dataset\n    dataset = ParquetDataset(dataset_path)\n    data = dataset.read(backend=\"pandas\")\n\n    # Create table if not exists\n    create_table_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n        {', '.join([f'{col} {get_pg_type(dtype)}' for col, dtype in data.dtypes.items()])}\n    )\n    \"\"\"\n    cursor.execute(create_table_sql)\n\n    # Prepare insert statement\n    columns = ', '.join(data.columns)\n    placeholders = ', '.join(['%s'] * len(data.columns))\n    insert_sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n\n    # Insert data in batches\n    batch_size = 1000\n    for i in range(0, len(data), batch_size):\n        batch = data.iloc[i:i + batch_size]\n        execute_batch(\n            cursor,\n            insert_sql,\n            [tuple(row) for row in batch.itertuples(index=False)]\n        )\n        conn.commit()\n        print(f\"Inserted {min(i + batch_size, len(data))} rows\")\n\n    cursor.close()\n    conn.close()\n\ndef get_pg_type(dtype):\n    \"\"\"Map pandas dtype to PostgreSQL type.\"\"\"\n    if dtype == 'int64':\n        return 'BIGINT'\n    elif dtype == 'float64':\n        return 'DOUBLE PRECISION'\n    elif dtype == 'bool':\n        return 'BOOLEAN'\n    elif dtype == 'datetime64[ns]':\n        return 'TIMESTAMP'\n    else:\n        return 'TEXT'\n</code></pre>"},{"location":"advanced/integration/#bigquery-integration","title":"BigQuery Integration","text":"<pre><code># BigQuery integration\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\ndef export_to_bigquery(dataset_path: str, project_id: str, dataset_id: str, table_id: str):\n    \"\"\"Export dataset to BigQuery.\"\"\"\n    # Initialize BigQuery client\n    credentials = service_account.Credentials.from_service_account_file(\n        'service-account.json'\n    )\n    client = bigquery.Client(credentials=credentials, project=project_id)\n\n    # Load dataset\n    dataset = ParquetDataset(dataset_path)\n    data = dataset.read(backend=\"pandas\")\n\n    # Set up job config\n    job_config = bigquery.LoadJobConfig(\n        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n        source_format=bigquery.SourceFormat.PARQUET\n    )\n\n    # Get table reference\n    table_ref = client.dataset(dataset_id).table(table_id)\n\n    # Load data\n    job = client.load_table_from_dataframe(\n        data,\n        table_ref,\n        job_config=job_config\n    )\n\n    # Wait for job to complete\n    job.result()\n    print(f\"Loaded {job.output_rows} rows into {dataset_id}.{table_id}\")\n</code></pre>"},{"location":"advanced/integration/#real-time-integration","title":"Real-time Integration","text":""},{"location":"advanced/integration/#kafka-integration","title":"Kafka Integration","text":"<pre><code># Kafka producer with PyDala2\nfrom kafka import KafkaProducer\nimport json\nfrom pydala import ParquetDataset\n\ndef stream_to_kafka(dataset_path: str, topic: str, bootstrap_servers: str):\n    \"\"\"Stream dataset to Kafka.\"\"\"\n    # Initialize producer\n    producer = KafkaProducer(\n        bootstrap_servers=bootstrap_servers,\n        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n    )\n\n    # Load dataset\n    dataset = ParquetDataset(dataset_path)\n    data = dataset.read(backend=\"pandas\")\n\n    # Stream records\n    for _, row in data.iterrows():\n        message = row.to_dict()\n        producer.send(topic, value=message)\n\n    # Flush producer\n    producer.flush()\n    print(f\"Streamed {len(data)} records to topic {topic}\")\n\n# Kafka consumer\nfrom kafka import KafkaConsumer\n\ndef consume_from_kafka_to_dataset(topic: str, bootstrap_servers: str, output_path: str):\n    \"\"\"Consume from Kafka and save to dataset.\"\"\"\n    # Initialize consumer\n    consumer = KafkaConsumer(\n        topic,\n        bootstrap_servers=bootstrap_servers,\n        auto_offset_reset='earliest',\n        value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n    )\n\n    # Collect messages\n    records = []\n    for message in consumer:\n        records.append(message.value)\n\n        # Batch save every 1000 records\n        if len(records) &gt;= 1000:\n            df = pd.DataFrame(records)\n            dataset = ParquetDataset(output_path)\n            dataset.write(df, mode='append')\n            records = []\n\n    # Save remaining records\n    if records:\n        df = pd.DataFrame(records)\n        dataset = ParquetDataset(output_path)\n        dataset.write(df, mode='append')\n</code></pre>"},{"location":"advanced/integration/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"advanced/integration/#prometheus-integration","title":"Prometheus Integration","text":"<pre><code># Prometheus metrics for PyDala2\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\nimport time\n\n# Define metrics\nREAD_OPERATIONS = Counter(\n    'pydala2_read_operations_total',\n    'Total read operations',\n    ['dataset', 'backend']\n)\n\nREAD_DURATION = Histogram(\n    'pydala2_read_duration_seconds',\n    'Read operation duration',\n    ['dataset', 'backend']\n)\n\nDATASET_SIZE = Gauge(\n    'pydala2_dataset_size_bytes',\n    'Dataset size in bytes',\n    ['dataset']\n)\n\n# Monitor dataset operations\nclass MonitoredDataset:\n    def __init__(self, dataset_path: str):\n        self.dataset = ParquetDataset(dataset_path)\n        self.dataset_path = dataset_path\n\n    def read(self, **kwargs):\n        \"\"\"Monitored read operation.\"\"\"\n        backend = kwargs.get('backend', 'default')\n\n        # Record start time\n        start_time = time.time()\n\n        try:\n            # Perform read\n            result = self.dataset.read(**kwargs)\n\n            # Record metrics\n            READ_OPERATIONS.labels(\n                dataset=self.dataset_path,\n                backend=backend\n            ).inc()\n\n            READ_DURATION.labels(\n                dataset=self.dataset_path,\n                backend=backend\n            ).observe(time.time() - start_time)\n\n            return result\n\n        except Exception as e:\n            # Increment error counter (not shown)\n            raise e\n\n    def update_size_gauge(self):\n        \"\"\"Update dataset size gauge.\"\"\"\n        size = sum(\n            os.path.getsize(f)\n            for f in self.dataset.files\n        )\n        DATASET_SIZE.labels(dataset=self.dataset_path).set(size)\n\n# Start metrics server\nstart_http_server(8000)\n</code></pre>"},{"location":"advanced/integration/#logging-integration","title":"Logging Integration","text":"<pre><code># Structured logging with ELK stack\nimport logging\nimport json\nfrom datetime import datetime\nfrom pydala import ParquetDataset\n\nclass JSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter for ELK stack.\"\"\"\n    def format(self, record):\n        log_entry = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'level': record.levelname,\n            'message': record.getMessage(),\n            'logger': record.name,\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n\n        # Add extra fields if present\n        if hasattr(record, 'dataset'):\n            log_entry['dataset'] = record.dataset\n        if hasattr(record, 'operation'):\n            log_entry['operation'] = record.operation\n        if hasattr(record, 'duration'):\n            log_entry['duration_ms'] = record.duration * 1000\n\n        return json.dumps(log_entry)\n\n# Configure logger\nlogger = logging.getLogger('pydala2')\nhandler = logging.StreamHandler()\nhandler.setFormatter(JSONFormatter())\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n\n# Usage with extra context\ndef monitored_read(dataset_path: str, **kwargs):\n    start_time = time.time()\n    logger.info(\n        \"Starting read operation\",\n        extra={\n            'dataset': dataset_path,\n            'operation': 'read'\n        }\n    )\n\n    try:\n        dataset = ParquetDataset(dataset_path)\n        result = dataset.read(**kwargs)\n\n        logger.info(\n            \"Completed read operation\",\n            extra={\n                'dataset': dataset_path,\n                'operation': 'read',\n                'duration': time.time() - start_time,\n                'rows_returned': len(result)\n            }\n        )\n\n        return result\n\n    except Exception as e:\n        logger.error(\n            \"Read operation failed\",\n            extra={\n                'dataset': dataset_path,\n                'operation': 'read',\n                'duration': time.time() - start_time,\n                'error': str(e)\n            }\n        )\n        raise\n</code></pre>"},{"location":"advanced/integration/#best-practices","title":"Best Practices","text":"<ol> <li>Use connection pooling for database operations</li> <li>Implement retry logic for network operations</li> <li>Cache frequently accessed data to reduce I/O</li> <li>Monitor performance with appropriate metrics</li> <li>Handle errors gracefully with proper logging</li> <li>Use async patterns for I/O-bound operations</li> <li>Batch operations for better performance</li> <li>Validate data before processing</li> <li>Document integrations clearly</li> <li>Test integrations thoroughly</li> </ol>"},{"location":"advanced/performance-tuning/","title":"Performance Tuning","text":"<p>This guide covers advanced performance tuning techniques for PyDala2.</p>"},{"location":"advanced/performance-tuning/#performance-architecture","title":"Performance Architecture","text":"graph TD     A[PyDala2 Application] --&gt; B[Backend Selection]     B --&gt; C[Polars]     B --&gt; D[DuckDB]     B --&gt; E[PyArrow]     B --&gt; F[Pandas]     A --&gt; G[Memory Management]     G --&gt; H[Caching]     G --&gt; I[Chunked Processing]     A --&gt; J[I/O Optimization]     J --&gt; K[Compression]     J --&gt; L[Partitioning]     J --&gt; M[Parallel I/O]"},{"location":"advanced/performance-tuning/#backend-optimization","title":"Backend Optimization","text":""},{"location":"advanced/performance-tuning/#polars-performance","title":"Polars Performance","text":"<p>Polars is generally the fastest backend for data manipulation tasks.</p>"},{"location":"advanced/performance-tuning/#lazy-evaluation","title":"Lazy Evaluation","text":"<pre><code># Always use lazy evaluation for complex operations\nlazy_df = dataset.table.to_polars(lazy=True)  # Returns LazyFrame by default\n\n# Build up the query plan\nresult = (\n    lazy_df\n    .filter(pl.col(\"date\") &gt;= \"2023-01-01\")\n    .group_by([\"category\", \"region\"])\n    .agg([\n        pl.count(\"id\").alias(\"count\"),\n        pl.sum(\"amount\").alias(\"total_amount\"),\n        pl.mean(\"amount\").alias(\"avg_amount\"),\n        pl.col(\"amount\").quantile(0.95).alias(\"p95_amount\")\n    ])\n    .filter(pl.col(\"count\") &gt; 10)\n    .sort(\"total_amount\", descending=True)\n    .collect()  # Execute here\n)\n\n# Or use the shortcut property\nresult = (\n    dataset.t.pl  # Direct access to LazyFrame\n    .filter(pl.col(\"date\") &gt;= \"2023-01-01\")\n    .group_by([\"category\", \"region\"])\n    .agg([\n        pl.count(\"id\").alias(\"count\"),\n        pl.sum(\"amount\").alias(\"total_amount\")\n    ])\n    .collect()\n)\n</code></pre>"},{"location":"advanced/performance-tuning/#query-optimization","title":"Query Optimization","text":"<pre><code># Optimized aggregation patterns\ndef optimized_aggregation(df):\n    \"\"\"Optimized aggregation with proper types.\"\"\"\n    return (\n        df\n        # Filter early to reduce data size\n        .filter(pl.col(\"status\") == \"completed\")\n        # Cast types early if needed\n        .with_columns([\n            pl.col(\"amount\").cast(pl.Float64),\n            pl.col(\"date\").cast(pl.Date)\n        ])\n        # Group by cardinality order (low to high)\n        .group_by([\"region\", \"category\", \"subcategory\"])\n        .agg([\n            # Use built-in aggregations when possible\n            pl.count(\"id\").alias(\"orders\"),\n            pl.sum(\"amount\").alias(\"revenue\"),\n            # Use expressions for complex calcs\n            (pl.col(\"amount\") / pl.count(\"id\")).alias(\"avg_order\")\n        ])\n    )\n\n# Use expressions for multiple calculations\nresult = (\n    df\n    .with_columns([\n        # Single pass for multiple calculations\n        (pl.col(\"amount\") * 0.1).alias(\"tax\"),\n        (pl.col(\"amount\") * 1.1).alias(\"total\"),\n        pl.col(\"amount\").rank().alias(\"amount_rank\")\n    ])\n)\n</code></pre>"},{"location":"advanced/performance-tuning/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Use PyDala2's built-in data type optimization\ndataset.optimize.optimize_dtypes()\n\n# Or manually optimize in Polars\ndef optimize_dtypes_polars(df):\n    \"\"\"Optimize Polars DataFrame memory usage.\"\"\"\n    return (\n        df\n        # Downcast numeric types\n        .with_columns([\n            pl.col(\"id\").cast(pl.Int32),\n            pl.col(\"small_int\").cast(pl.Int16),\n            pl.col(\"price\").cast(pl.Float32),\n            # Use categorical for low cardinality strings\n            pl.col(\"category\").cast(pl.Categorical),\n            # Use dates instead of timestamps\n            pl.col(\"date_only\").cast(pl.Date)\n        ])\n    )\n\n# Apply optimization to a LazyFrame\nlazy_df = dataset.table.pl\noptimized_df = lazy_df.with_columns([\n    pl.col(\"amount\").cast(pl.Float32),\n    pl.col(\"category\").cast(pl.Categorical)\n])\nresult = optimized_df.collect()\n</code></pre>"},{"location":"advanced/performance-tuning/#duckdb-optimization","title":"DuckDB Optimization","text":"<p>DuckDB excels at SQL queries and joins.</p>"},{"location":"advanced/performance-tuning/#query-optimization_1","title":"Query Optimization","text":"<pre><code># Use DuckDB connection directly for SQL queries\ndef get_category_summary(dataset, category):\n    \"\"\"Get summary for a specific category.\"\"\"\n    return dataset.ddb_con.sql(\"\"\"\n        SELECT region, SUM(amount) as total\n        FROM dataset\n        WHERE category = ?\n        GROUP BY region\n    \"\"\", parameters=[category]).to_arrow()\n\n# Optimize joins with proper indexing hints\njoin_result = dataset.ddb_con.sql(\"\"\"\n    SELECT /*+ HASH_JOIN(a, b) */\n        a.*,\n        b.customer_name,\n        b.segment\n    FROM dataset a\n    JOIN customers b ON a.customer_id = b.id\n    WHERE a.date &gt;= '2023-01-01'\n\"\"\").to_arrow()\n</code></pre>"},{"location":"advanced/performance-tuning/#window-function-optimization","title":"Window Function Optimization","text":"<pre><code># Efficient window functions\nwindow_result = dataset.ddb_con.sql(\"\"\"\n    SELECT\n        order_id,\n        customer_id,\n        order_date,\n        amount,\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id\n            ORDER BY order_date\n        ) as order_sequence,\n        RANK() OVER (\n            PARTITION BY customer_id\n            ORDER BY amount DESC\n        ) as amount_rank,\n        SUM(amount) OVER (\n            PARTITION BY customer_id\n            ORDER BY order_date\n            RANGE BETWEEN INTERVAL 30 DAY PRECEDING AND CURRENT ROW\n        ) as rolling_30d_sum\n    FROM dataset\n\"\"\").to_arrow()\n</code></pre>"},{"location":"advanced/performance-tuning/#pyarrow-optimization","title":"PyArrow Optimization","text":"<p>PyArrow provides the best performance for columnar operations and Arrow ecosystem integration.</p>"},{"location":"advanced/performance-tuning/#scanner-optimization","title":"Scanner Optimization","text":"<pre><code># Use scanner for efficient filtering\nscanner = dataset.table.scanner(\n    columns=['id', 'date', 'amount', 'category'],\n    filter=(\n        (pc.field('date') &gt;= pc.scalar('2023-01-01')) &amp;\n        (pc.field('amount') &gt; pc.scalar(100))\n    ),\n    batch_size=65536,  # Optimal batch size\n    use_threads=True\n)\n\n# Process in batches\nfor batch in scanner.to_batches():\n    process_batch(batch)\n\n# Or use the shortcut\nfor batch in dataset.t.scanner(columns=['id', 'amount']).to_batches():\n    process_batch(batch)\n</code></pre>"},{"location":"advanced/performance-tuning/#memory-mapping","title":"Memory Mapping","text":"<pre><code># Use memory mapping for large datasets\nimport pyarrow as pa\nimport pyarrow.memory_map as pmmap\n\n# Memory map dataset\nmmap_dataset = pds.dataset(\n    \"large_dataset.parquet\",\n    format=\"parquet\",\n    memory_map=True\n)\n\n# Create memory mapped table\ntable = mmap_dataset.to_table()\n</code></pre>"},{"location":"advanced/performance-tuning/#memory-management","title":"Memory Management","text":""},{"location":"advanced/performance-tuning/#advanced-caching-strategies","title":"Advanced Caching Strategies","text":"<pre><code>class TieredCache:\n    \"\"\"Multi-level cache implementation.\"\"\"\n\n    def __init__(self):\n        self.l1_cache = {}  # In-memory\n        self.l2_cache = {}  # SSD\n        self.l1_max_size = 100 * 1024 * 1024  # 100MB\n        self.l2_max_size = 10 * 1024 * 1024 * 1024  # 10GB\n\n    def get(self, key):\n        \"\"\"Get from cache, checking L1 then L2.\"\"\"\n        # Check L1 (memory)\n        if key in self.l1_cache:\n            return self.l1_cache[key]\n\n        # Check L2 (SSD)\n        if key in self.l2_cache:\n            value = self._load_from_l2(key)\n            # Promote to L1 if space\n            self._add_to_l1(key, value)\n            return value\n\n        return None\n\n    def put(self, key, value):\n        \"\"\"Add to cache, managing tiers.\"\"\"\n        if self._l1_size + len(str(value)) &lt; self.l1_max_size:\n            self._add_to_l1(key, value)\n        else:\n            self._add_to_l2(key, value)\n\n    def _add_to_l1(self, key, value):\n        \"\"\"Add to L1 cache with LRU eviction.\"\"\"\n        while self._l1_size + len(str(value)) &gt; self.l1_max_size:\n            # Evict oldest\n            oldest_key = next(iter(self.l1_cache))\n            del self.l1_cache[oldest_key]\n        self.l1_cache[key] = value\n</code></pre>"},{"location":"advanced/performance-tuning/#memory-profiling","title":"Memory Profiling","text":"<pre><code>import tracemalloc\nimport time\n\ndef profile_memory_usage(func):\n    \"\"\"Decorator to profile memory usage.\"\"\"\n    def wrapper(*args, **kwargs):\n        # Start memory tracing\n        tracemalloc.start()\n        snapshot1 = tracemalloc.take_snapshot()\n\n        # Execute function\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n\n        # Take memory snapshot\n        snapshot2 = tracemalloc.take_snapshot()\n\n        # Calculate differences\n        stats = snapshot2.compare_to(snapshot1, 'lineno')\n        total = sum(stat.size_diff for stat in stats)\n\n        print(f\"Function {func.__name__}:\")\n        print(f\"  Execution time: {end_time - start_time:.2f}s\")\n        print(f\"  Memory allocated: {total / 1024 / 1024:.2f} MB\")\n\n        # Show top memory allocations\n        for stat in stats[:10]:\n            print(f\"    {stat.filename}:{stat.lineno}: {stat.size_diff / 1024:.1f} KB\")\n\n        return result\n    return wrapper\n\n# Usage\n@profile_memory_usage\ndef process_large_dataset():\n    return dataset.table.to_polars()\n</code></pre>"},{"location":"advanced/performance-tuning/#io-optimization","title":"I/O Optimization","text":""},{"location":"advanced/performance-tuning/#advanced-partitioning-strategies","title":"Advanced Partitioning Strategies","text":"<pre><code>def optimal_partitioning(df, target_size_mb=100):\n    \"\"\"Calculate optimal partitioning strategy.\"\"\"\n    total_size_mb = len(df) * df.memory_usage(deep=True).sum() / 1024 / 1024\n    estimated_rows = len(df)\n\n    # Calculate partitions needed\n    num_partitions = max(1, int(total_size_mb / target_size_mb))\n\n    # Analyze columns for good partitioning\n    partition_candidates = {}\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            # String columns\n            cardinality = df[col].nunique()\n            if cardinality &lt; 1000:  # Low cardinality\n                partition_candidates[col] = cardinality\n        elif df[col].dtype in ['int64', 'int32']:\n            # Integer columns\n            unique_vals = df[col].nunique()\n            if unique_vals &lt; 100:  # Very low cardinality\n                partition_candidates[col] = unique_vals\n\n    # Select best partitioning columns\n    sorted_cols = sorted(partition_candidates.items(), key=lambda x: x[1])\n    partition_cols = [col for col, _ in sorted_cols[:3]]  # Max 3 levels\n\n    return partition_cols\n\n# Usage\ndf = dataset.table.df  # Export to pandas\npartition_cols = optimal_partitioning(df, target_size_mb=100)\ndataset.write_to_dataset(data=df, partition_cols=partition_cols)\n</code></pre>"},{"location":"advanced/performance-tuning/#compression-optimization","title":"Compression Optimization","text":"<pre><code># Benchmark different compression settings\ndef benchmark_compression(dataset, sample_data):\n    \"\"\"Benchmark different compression options.\"\"\"\n    compressions = [\n        ('uncompressed', None),\n        ('snappy', None),\n        ('gzip', 6),\n        ('zstd', 3),\n        ('zstd', 6),\n        ('brotli', 4),\n        ('lz4', None)\n    ]\n\n    results = []\n\n    for name, level in compressions:\n        # Write with compression\n        start_time = time.time()\n        dataset.write_to_dataset(\n            data=sample_data,\n            compression=name,\n            compression_level=level\n        )\n        write_time = time.time() - start_time\n\n        # Measure size\n        size = sum(\n            os.path.getsize(f)\n            for f in dataset.files\n        )\n\n        # Measure read performance\n        start_time = time.time()\n        data = dataset.table.to_polars()\n        read_time = time.time() - start_time\n\n        results.append({\n            'compression': name,\n            'level': level,\n            'size_mb': size / 1024 / 1024,\n            'write_time': write_time,\n            'read_time': read_time,\n            'ratio': (len(sample_data) * 8) / size if size &gt; 0 else 0\n        })\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"advanced/performance-tuning/#parallel-io","title":"Parallel I/O","text":"<pre><code># Parallel file operations\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef parallel_read_files(file_paths, max_workers=4):\n    \"\"\"Read multiple files in parallel.\"\"\"\n    def read_file(path):\n        return pd.read_parquet(path)\n\n    results = []\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all tasks\n        future_to_path = {\n            executor.submit(read_file, path): path\n            for path in file_paths\n        }\n\n        # Collect results as they complete\n        for future in as_completed(future_to_path):\n            path = future_to_path[future]\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                print(f\"Error reading {path}: {e}\")\n\n    return pd.concat(results, ignore_index=True)\n</code></pre>"},{"location":"advanced/performance-tuning/#query-optimization_2","title":"Query Optimization","text":""},{"location":"advanced/performance-tuning/#predicate-pushdown","title":"Predicate Pushdown","text":"<pre><code># PyDala2 automatically applies predicate pushdown\n# Just use the filter method with your conditions\ndef optimized_filters(dataset, filters):\n    \"\"\"Apply filters efficiently.\"\"\"\n    # PyDala2 automatically optimizes filter order\n    # and pushes down predicates to the datasource\n    combined = \" AND \".join(filters)\n    result = dataset.filter(combined)\n\n    # Export the filtered result\n    return result.table.to_polars()\n</code></pre>"},{"location":"advanced/performance-tuning/#column-pruning","title":"Column Pruning","text":"<pre><code># Select only needed columns\ndef efficient_column_selection(dataset, required_columns):\n    \"\"\"Select columns efficiently.\"\"\"\n    # Check if columns exist\n    available_columns = set(dataset.columns)\n    selected_columns = [\n        col for col in required_columns\n        if col in available_columns\n    ]\n\n    if not selected_columns:\n        raise ValueError(\"No valid columns selected\")\n\n    # Use column parameter in export method\n    return dataset.table.to_polars(columns=selected_columns)\n\n    # Or with shortcut\n    return dataset.t.pl.select(selected_columns).collect()\n</code></pre>"},{"location":"advanced/performance-tuning/#join-optimization","title":"Join Optimization","text":"<pre><code># Optimize join strategies\ndef optimized_join(left_ds, right_ds, join_key, join_type='inner'):\n    \"\"\"Optimize dataset join.\"\"\"\n    # Determine optimal strategy based on data sizes\n    left_size = left_ds.count_rows()\n    right_size = right_ds.count_rows()\n\n    if left_size &lt; 1000000 and right_size &lt; 1000000:\n        # Small datasets - broadcast join\n        result = catalog.query(f\"\"\"\n            SELECT l.*, r.*\n            FROM left_ds l\n            {join_type.upper()} JOIN right_ds r\n            ON l.{join_key} = r.{join_key}\n        \"\"\")\n    elif left_size / right_size &gt; 100:\n        # Right side much smaller - broadcast right\n        result = right_ds.read(backend=\"polars\").join(\n            left_ds.read(backend=\"polars\"),\n            on=join_key,\n            how=join_type\n        )\n    else:\n        # Both large - use partitioned join\n        # Partition on join key\n        for partition_value in get_unique_values(left_ds, join_key):\n            left_part = left_ds.read(filters=f\"{join_key} = {partition_value}\")\n            right_part = right_ds.read(filters=f\"{join_key} = {partition_value}\")\n            # Process partition\n            process_join_partition(left_part, right_part)\n</code></pre>"},{"location":"advanced/performance-tuning/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"advanced/performance-tuning/#query-plan-analysis","title":"Query Plan Analysis","text":"<pre><code># Analyze Polars query plan\ndef analyze_polars_plan(lazy_df):\n    \"\"\"Analyze and optimize Polars lazy plan.\"\"\"\n    # Get the optimized plan\n    plan = lazy_df.describe_optimized_plan()\n\n    print(\"Optimized Plan:\")\n    print(plan)\n\n    # Look for optimization opportunities\n    if \"FILTER\" in plan:\n        print(\"\u2713 Predicate pushdown detected\")\n\n    if \"PROJECTION\" in plan:\n        print(\"\u2713 Column pruning detected\")\n\n    if \"AGGREGATE\" in plan:\n        print(\"\u2713 Aggregation pushdown detected\")\n\n    return lazy_df\n\n# Usage\nlazy_df = dataset.read(backend=\"polars\", lazy=True)\noptimized_df = analyze_polars_plan(\n    lazy_df.filter(pl.col(\"amount\") &gt; 100)\n)\n</code></pre>"},{"location":"advanced/performance-tuning/#custom-aggregations","title":"Custom Aggregations","text":"<pre><code># Efficient custom aggregations\ndef efficient_rolling_aggregation(dataset, window_col, value_col, window_size):\n    \"\"\"Efficient rolling aggregation using window functions.\"\"\"\n    return dataset.ddb_con.sql(f\"\"\"\n        SELECT\n            {window_col},\n            {value_col},\n            AVG({value_col}) OVER (\n                ORDER BY {window_col}\n                ROWS BETWEEN {window_size} PRECEDING AND CURRENT ROW\n            ) as rolling_avg,\n            SUM({value_col}) OVER (\n                ORDER BY {window_col}\n                ROWS BETWEEN {window_size} PRECEDING AND CURRENT ROW\n            ) as rolling_sum\n        FROM dataset\n        ORDER BY {window_col}\n    \"\"\").to_arrow()\n</code></pre>"},{"location":"advanced/performance-tuning/#materialized-views","title":"Materialized Views","text":"<pre><code>class MaterializedView:\n    \"\"\"Simple materialized view implementation.\"\"\"\n\n    def __init__(self, base_dataset, view_path, query):\n        self.base_dataset = base_dataset\n        self.view_path = view_path\n        self.query = query\n        self.view_dataset = ParquetDataset(view_path)\n\n    def refresh(self):\n        \"\"\"Refresh materialized view.\"\"\"\n        # Execute query\n        result = self.base_dataset.ddb_con.sql(self.query).to_arrow()\n\n        # Write to view\n        self.view_dataset.write_to_dataset(data=result)\n\n    def read(self):\n        \"\"\"Read from materialized view.\"\"\"\n        return self.view_dataset.table.to_polars()\n\n# Usage\nmv = MaterializedView(\n    base_dataset=dataset,\n    view_path=\"data/monthly_summary\",\n    query=\"\"\"\n        SELECT\n            DATE_TRUNC('month', order_date) as month,\n            category,\n            COUNT(*) as orders,\n            SUM(amount) as revenue\n        FROM dataset\n        GROUP BY month, category\n    \"\"\"\n)\n\n# Refresh daily\nmv.refresh()\n\n# Query from view\nsummary = mv.read()  # This is a custom method on MaterializedView class\n</code></pre>"},{"location":"advanced/performance-tuning/#performance-checklist","title":"Performance Checklist","text":"<ul> <li> Use lazy evaluation for complex operations</li> <li> Choose appropriate backend for workload</li> <li> Enable and configure caching</li> <li> Optimize partitioning strategy</li> <li> Use appropriate compression</li> <li> Process data in chunks for large datasets</li> <li> Apply filters early</li> <li> Select only needed columns</li> <li> Optimize join strategies</li> <li> Monitor memory usage</li> <li> Use streaming for very large datasets</li> <li> Profile and optimize slow queries</li> <li> Consider materialized views for frequent queries</li> <li> Tune parallel processing parameters</li> <li> Monitor disk I/O performance</li> </ul>"},{"location":"advanced/troubleshooting/","title":"Troubleshooting","text":"<p>This guide helps you diagnose and resolve common issues when using PyDala2.</p>"},{"location":"advanced/troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"advanced/troubleshooting/#installation-problems","title":"Installation Problems","text":""},{"location":"advanced/troubleshooting/#importerror-no-module-named-pydala","title":"ImportError: No module named 'pydala'","text":"<p>Symptoms: <pre><code>&gt;&gt;&gt; import pydala\nModuleNotFoundError: No module named 'pydala'\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check installation: <pre><code>pip list | grep pydala\n</code></pre></p> </li> <li> <p>Install or reinstall: <pre><code># Install from PyPI\npip install pydala2\n\n# Or if developing from source\npip install -e .\n\n# With all dependencies\npip install pydala2[all]\n</code></pre></p> </li> <li> <p>Check Python environment: <pre><code>which python\npython --version\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#permission-denied-errors","title":"Permission denied errors","text":"<p>Symptoms: <pre><code>PermissionError: [Errno 13] Permission denied: '/data/dataset'\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check directory permissions: <pre><code>ls -la /data/\n</code></pre></p> </li> <li> <p>Change ownership or permissions: <pre><code># Change ownership\nsudo chown -R user:group /data/dataset\n\n# Or change permissions\nchmod 755 /data/dataset\n</code></pre></p> </li> <li> <p>Use user-writable location: <pre><code>dataset = ParquetDataset(\"~/data/dataset\")\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#memory-issues","title":"Memory Issues","text":""},{"location":"advanced/troubleshooting/#out-of-memory-errors","title":"Out of memory errors","text":"<p>Symptoms: <pre><code>MemoryError: Unable to allocate X GiB for an array\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Process data in chunks: <pre><code># Use batch reader\nreader = dataset.to_batch_reader(batch_size=10000)\nfor batch in reader:\n    process_batch(batch)\n</code></pre></p> </li> <li> <p>Set memory limits: <pre><code>from pydala import set_config\n\nset_config({\n    'max_memory': 4 * 1024 * 1024 * 1024,  # 4GB\n    'n_workers': 2\n})\n</code></pre></p> </li> <li> <p>Use lazy evaluation: <pre><code># Use Polars lazy mode\nlazy_df = dataset.read(backend=\"polars\", lazy=True)\nresult = (\n    lazy_df\n    .filter(pl.col(\"amount\") &gt; 100)\n    .group_by(\"category\")\n    .agg(pl.sum(\"amount\"))\n    .collect()\n)\n</code></pre></p> </li> <li> <p>Clear caches: <pre><code>dataset.clear_cache()\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#memory-leaks","title":"Memory leaks","text":"<p>Symptoms: Memory usage grows continuously over time.</p> <p>Solutions:</p> <ol> <li> <p>Monitor memory usage: <pre><code>import psutil\nimport gc\n\ndef check_memory():\n    process = psutil.Process()\n    mem_info = process.memory_info()\n    print(f\"RSS: {mem_info.rss / 1024 / 1024:.1f} MB\")\n    return mem_info.rss\n</code></pre></p> </li> <li> <p>Explicit cleanup: <pre><code># Delete large objects\ndel large_dataframe\ngc.collect()\n</code></pre></p> </li> <li> <p>Use context managers: <pre><code>with dataset.read(backend=\"polars\") as df:\n    # df is automatically cleaned up\n    process_data(df)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"advanced/troubleshooting/#slow-readswrites","title":"Slow reads/writes","text":"<p>Symptoms: Operations taking longer than expected.</p> <p>Diagnostics:</p> <ol> <li> <p>Check filesystem performance: <pre><code>import time\n\nstart = time.time()\ndata = dataset.read()\nread_time = time.time() - start\nprint(f\"Read time: {read_time:.2f} seconds\")\n</code></pre></p> </li> <li> <p>Profile the operation: <pre><code>import cProfile\n\ndef profile_operation():\n    dataset.read(filters=\"date &gt; '2023-01-01'\")\n\nprofiler = cProfile.Profile()\nprofiler.enable()\nprofile_operation()\nprofiler.disable()\nprofiler.print_stats(sort='cumulative')\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li> <p>Enable caching: <pre><code>dataset.enable_cache(max_size=1024*1024*1024)  # 1GB\n</code></pre></p> </li> <li> <p>Optimize file layout: <pre><code># Compact small files\ndataset.compact_small_files(min_file_size=10*1024*1024)\n\n# Repartition for better pruning\ndataset.repartition(partition_cols=['year', 'month', 'day'])\n</code></pre></p> </li> <li> <p>Use appropriate compression: <pre><code>dataset.write(\n    data,\n    compression='zstd',\n    compression_level=3,\n    row_group_size=1000000\n)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#schema-issues","title":"Schema Issues","text":""},{"location":"advanced/troubleshooting/#schema-mismatch-errors","title":"Schema mismatch errors","text":"<p>Symptoms: <pre><code>pyarrow.lib.ArrowInvalid: Schema at index 0 was different\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check schema evolution: <pre><code># Compare schemas\nold_schema = old_dataset.schema\nnew_schema = new_dataset.schema\n\nfor field in old_schema:\n    if field.name not in new_schema.names:\n        print(f\"Missing column: {field.name}\")\n    elif new_schema.field(field.name).type != field.type:\n        print(f\"Type mismatch for {field.name}\")\n</code></pre></p> </li> <li> <p>Use schema validation: <pre><code>from pydala import validate_schema\n\nresult = validate_schema(data, expected_schema)\nif not result.valid:\n    print(\"Validation errors:\")\n    for error in result.errors:\n        print(f\"  - {error}\")\n</code></pre></p> </li> <li> <p>Handle schema evolution: <pre><code>def safe_write(dataset, data):\n    \"\"\"Write data with schema handling.\"\"\"\n    if dataset.exists():\n        # Merge schemas\n        existing_schema = dataset.schema\n        new_schema = pa.unify_schemas([existing_schema, data.schema])\n\n        # Add missing columns with nulls\n        for field in new_schema:\n            if field.name not in data.column_names:\n                data = data.append_column(\n                    field.name,\n                    pa.nulls(len(data)).cast(field.type)\n                )\n\n    dataset.write(data)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#cloud-storage-issues","title":"Cloud Storage Issues","text":""},{"location":"advanced/troubleshooting/#s3-connection-errors","title":"S3 connection errors","text":"<p>Symptoms: <pre><code>NoCredentialsError: Unable to locate credentials\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check credentials: <pre><code># AWS CLI\naws configure list\n\n# Environment variables\necho $AWS_ACCESS_KEY_ID\necho $AWS_SECRET_ACCESS_KEY\n</code></pre></p> </li> <li> <p>Configure credentials properly: <pre><code># Using environment variables\nimport os\nos.environ['AWS_ACCESS_KEY_ID'] = 'your_key'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'your_secret'\n\n# Or using filesystem parameters\ns3_fs = FileSystem(\n    protocol=\"s3\",\n    key=\"your_key\",\n    secret=\"your_secret\"\n)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#slow-cloud-operations","title":"Slow cloud operations","text":"<p>Solutions:</p> <ol> <li> <p>Enable caching: <pre><code># Local cache for cloud data\nfs = FileSystem(\n    protocol=\"s3\",\n    cached=True,\n    cache_storage=\"/tmp/s3_cache\",\n    cache_options={\n        'expiry_time': 3600  # 1 hour\n    }\n)\n</code></pre></p> </li> <li> <p>Use multipart uploads: <pre><code>dataset.write(\n    data,\n    filesystem_kwargs={\n        'max_upload_parts': 10000,\n        'multipart_threshold': 64 * 1024 * 1024  # 64MB\n    }\n)\n</code></pre></p> </li> <li> <p>Optimize region selection: <pre><code># Use same region as data\nfs = FileSystem(\n    protocol=\"s3\",\n    client_kwargs={\n        \"region_name\": \"us-east-1\"\n    }\n)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#catalog-issues","title":"Catalog Issues","text":""},{"location":"advanced/troubleshooting/#catalog-not-found-errors","title":"Catalog not found errors","text":"<p>Symptoms: <pre><code>FileNotFoundError: catalog.yaml not found\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check catalog path: <pre><code># Use absolute path\ncatalog = Catalog(\"/absolute/path/to/catalog.yaml\")\n\n# Or check current directory\nimport os\nprint(f\"Current directory: {os.getcwd()}\")\n</code></pre></p> </li> <li> <p>Create catalog if missing: <pre><code>def ensure_catalog(path):\n    \"\"\"Create catalog if it doesn't exist.\"\"\"\n    if not os.path.exists(path):\n        with open(path, 'w') as f:\n            yaml.safe_dump({\n                'tables': {},\n                'filesystems': {}\n            }, f)\n    return Catalog(path)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#dataset-not-registered","title":"Dataset not registered","text":"<p>Symptoms: <pre><code>ValueError: Dataset 'my_dataset' not found in catalog\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>List available datasets: <pre><code>catalog = Catalog(\"catalog.yaml\")\nprint(\"Available datasets:\", catalog.all_tables())\n</code></pre></p> </li> <li> <p>Register dataset: <pre><code># Register existing dataset\ncatalog.load_parquet(\n    \"my_dataset\",\n    \"/path/to/dataset\",\n    partitioning=[\"year\", \"month\"]\n)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#backend-specific-issues","title":"Backend-Specific Issues","text":""},{"location":"advanced/troubleshooting/#polars-issues","title":"Polars issues","text":"<p>Symptoms: <pre><code>PanicException: ...\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Update Polars: <pre><code>pip install --upgrade polars\n</code></pre></p> </li> <li> <p>Check data types: <pre><code># Some operations don't support certain types\nprint(df.dtypes)\n\n# Convert if necessary\ndf = df.with_columns([\n    pl.col('string_col').cast(pl.Utf8),\n    pl.col('numeric_col').cast(pl.Float64)\n])\n</code></pre></p> </li> <li> <p>Use eager mode for debugging: <pre><code># Switch from lazy to eager\ndf = dataset.read(backend=\"polars\", lazy=False)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#duckdb-issues","title":"DuckDB issues","text":"<p>Symptoms: <pre><code>duckdb.OperationalError: Parser Error\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Validate SQL syntax: <pre><code># Test SQL in DuckDB directly\nimport duckdb\ncon = duckdb.connect()\ncon.execute(\"SELECT * FROM test LIMIT 1\")\n</code></pre></p> </li> <li> <p>Escape identifiers: <pre><code>from pydala import escape_sql_identifier\n\nsafe_column = escape_sql_identifier(column_name)\nsql = f\"SELECT {safe_column} FROM table\"\n</code></pre></p> </li> <li> <p>Check data types: <pre><code># DuckDB has stricter type rules\n# Convert to compatible types\nimport pyarrow as pa\ntable = table.cast(pa.schema([\n    ('date_col', pa.timestamp('ns')),\n    ('amount_col', pa.float64())\n]))\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for detailed information:</p> <pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or for specific modules\nlogging.getLogger('pydala').setLevel(logging.DEBUG)\nlogging.getLogger('fsspec').setLevel(logging.DEBUG)\n\n# Your code here\n</code></pre>"},{"location":"advanced/troubleshooting/#common-error-patterns","title":"Common Error Patterns","text":""},{"location":"advanced/troubleshooting/#pattern-1-invalid-filter-expressions","title":"Pattern 1: Invalid filter expressions","text":"<pre><code># Wrong\ndataset.read(\"column &gt; 'value' AND column2 &lt; 100\")\n\n# Right\ndataset.read(filters=\"column &gt; 'value' AND column2 &lt; 100\")\n\n# Or use Arrow expressions\nimport pyarrow.dataset as pds\nfilter_expr = (\n    (pds.field('column') &gt; 'value') &amp;\n    (pds.field('column2') &lt; 100)\n)\ndataset.read(filter=filter_expr)\n</code></pre>"},{"location":"advanced/troubleshooting/#pattern-2-mixed-data-types","title":"Pattern 2: Mixed data types","text":"<pre><code># Check for mixed types before writing\ndef check_data_types(df):\n    \"\"\"Check for inconsistent data types.\"\"\"\n    for col in df.columns:\n        unique_types = df[col].apply(type).nunique()\n        if unique_types &gt; 1:\n            print(f\"Warning: Column {col} has mixed types\")\n            print(df[col].apply(type).value_counts())\n</code></pre>"},{"location":"advanced/troubleshooting/#pattern-3-large-file-operations","title":"Pattern 3: Large file operations","text":"<pre><code># Process large files incrementally\ndef process_large_dataset(dataset, chunk_size=100000):\n    \"\"\"Process large dataset in chunks.\"\"\"\n    total_rows = 0\n    for i, chunk in enumerate(dataset.stream_read(chunk_size=chunk_size)):\n        print(f\"Processing chunk {i+1}\")\n        process_chunk(chunk)\n        total_rows += len(chunk)\n        del chunk  # Explicit cleanup\n    return total_rows\n</code></pre>"},{"location":"advanced/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"advanced/troubleshooting/#before-asking","title":"Before Asking","text":"<ol> <li> <p>Check the logs: <pre><code># Enable verbose logging\nexport PYDALA2_LOG_LEVEL=DEBUG\n\n# Or in code\nimport logging\nlogging.getLogger('pydala').setLevel(logging.DEBUG)\n</code></pre></p> </li> <li> <p>Create minimal reproducible example: <pre><code># Isolate the issue\nimport pandas as pd\nfrom pydala import ParquetDataset\n\n# Create minimal test case\ntest_data = pd.DataFrame({\n    'id': range(100),\n    'value': [i * 2 for i in range(100)]\n})\n\n# Reproduce issue\ndataset = ParquetDataset(\"test_dataset\")\ndataset.write(test_data)\nresult = dataset.read(filters=\"value &gt; 100\")  # What fails?\n</code></pre></p> </li> <li> <p>Gather system information: <pre><code>import sys\nimport platform\nimport pydala\n\nprint(\"Python version:\", sys.version)\nprint(\"Platform:\", platform.platform())\nprint(\"PyDala2 version:\", pydala.__version__)\nprint(\"PyArrow version:\", pa.__version__)\nprint(\"Polars version:\", pl.__version__)\n</code></pre></p> </li> </ol>"},{"location":"advanced/troubleshooting/#where-to-get-help","title":"Where to Get Help","text":"<ol> <li>Documentation: Check the latest documentation</li> <li>GitHub Issues: Search existing issues or create a new one</li> <li>Stack Overflow: Use the <code>pydala2</code> tag</li> <li>Discord/Slack: Join community channels</li> </ol>"},{"location":"advanced/troubleshooting/#reporting-issues","title":"Reporting Issues","text":"<p>When reporting issues, include:</p> <ol> <li>Minimal reproducible example</li> <li>Expected vs actual behavior</li> <li>Full error traceback</li> <li>System information</li> <li>Data sample (if possible)</li> </ol> <p>Example issue template: <pre><code>## Issue Description\nBrief description of the issue\n\n## Minimal Example\n```python\n# Code that reproduces the issue\n</code></pre></p>"},{"location":"advanced/troubleshooting/#expected-behavior","title":"Expected Behavior","text":"<p>What you expected to happen</p>"},{"location":"advanced/troubleshooting/#actual-behavior","title":"Actual Behavior","text":"<p>What actually happened</p>"},{"location":"advanced/troubleshooting/#error-traceback","title":"Error Traceback","text":"<pre><code>Full error message\n</code></pre>"},{"location":"advanced/troubleshooting/#system-information","title":"System Information","text":"<ul> <li>PyDala2 version: X.Y.Z</li> <li>Python version: X.Y.Z</li> <li>OS: Linux/Windows/MacOS ```</li> </ul>"},{"location":"api/catalog/","title":"Catalog System","text":"<p>This document describes the <code>Catalog</code> class which provides centralized management of datasets across your data lake.</p>"},{"location":"api/catalog/#catalog","title":"Catalog","text":"<pre><code>class Catalog\n</code></pre> <p>The <code>Catalog</code> class provides a centralized way to manage multiple datasets across different namespaces and filesystems. It maintains a YAML configuration file that tracks table locations, formats, and other metadata.</p>"},{"location":"api/catalog/#constructor","title":"Constructor","text":"<pre><code>Catalog(\n    path: str,\n    namespace: str | None = None,\n    ddb_con: duckdb.DuckDBPyConnection | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    **fs_kwargs\n) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code> (str): Path to the catalog configuration file (YAML) - <code>namespace</code> (str, optional): Namespace to scope table operations to - <code>ddb_con</code> (duckdb.DuckDBPyConnection, optional): Existing DuckDB connection - <code>filesystem</code> (AbstractFileSystem, optional): Default filesystem for catalog operations - <code>bucket</code> (str, optional): Bucket name for cloud storage - <code>**fs_kwargs</code>: Additional filesystem configuration</p> <p>Example: <pre><code>from pydala import Catalog\n\n# Create catalog\ncatalog = Catalog(\"catalog.yaml\", namespace=\"sales\")\n\n# The catalog.yaml file structure:\n# tables:\n#   sales:\n#     daily:\n#       path: /data/sales/daily\n#       format: parquet\n#       options:\n#         partitioning: [year, month]\n#     monthly:\n#       path: /data/sales/monthly\n#       format: parquet\n</code></pre></p>"},{"location":"api/catalog/#properties","title":"Properties","text":""},{"location":"api/catalog/#namespace","title":"namespace","text":"<p><pre><code>@property\ndef namespace(self) -&gt; str | None\n</code></pre> Get the current namespace.</p> <p>Returns: - <code>str | None</code>: Current namespace or None if not set</p>"},{"location":"api/catalog/#methods","title":"Methods","text":""},{"location":"api/catalog/#load_catalog","title":"load_catalog","text":"<p><pre><code>def load_catalog(self, namespace: str | None = None) -&gt; Munch\n</code></pre> Load catalog configuration from YAML file.</p> <p>Parameters: - <code>namespace</code> (str, optional): If specified, loads only tables from this namespace</p> <p>Returns: - <code>Munch</code>: The loaded catalog configuration</p> <p>Raises: - <code>FileNotFoundError</code>: If the catalog file doesn't exist - <code>yaml.YAMLError</code>: If the catalog file is invalid YAML</p> <p>Example: <pre><code># Load full catalog\ncatalog.load_catalog()\n\n# Load specific namespace\ncatalog.load_catalog(namespace=\"sales\")\n</code></pre></p>"},{"location":"api/catalog/#list_namespaces","title":"list_namespaces","text":"<p><pre><code>def list_namespaces(self) -&gt; list[str]\n</code></pre> List all namespaces in the catalog.</p> <p>Returns: - <code>list[str]</code>: List of namespace names</p> <p>Example: <pre><code>namespaces = catalog.list_namespaces()\nprint(namespaces)  # ['sales', 'customers', 'products']\n</code></pre></p>"},{"location":"api/catalog/#all_tables","title":"all_tables","text":"<p><pre><code>def all_tables(self) -&gt; list[str]\n</code></pre> List all tables in the current namespace.</p> <p>Returns: - <code>list[str]</code>: List of table names</p> <p>Example: <pre><code>tables = catalog.all_tables()\nprint(tables)  # ['daily', 'monthly', 'quarterly']\n</code></pre></p>"},{"location":"api/catalog/#get","title":"get","text":"<p><pre><code>def get(self, table_name: str) -&gt; Munch\n</code></pre> Get table configuration.</p> <p>Parameters: - <code>table_name</code> (str): Name of the table</p> <p>Returns: - <code>Munch</code>: Table configuration</p> <p>Example: <pre><code>config = catalog.get(\"daily\")\nprint(config.path)  # /data/sales/daily\nprint(config.format)  # parquet\n</code></pre></p>"},{"location":"api/catalog/#show","title":"show","text":"<p><pre><code>def show(self, table_name: str) -&gt; None\n</code></pre> Display table configuration.</p> <p>Parameters: - <code>table_name</code> (str): Name of the table</p> <p>Example: <pre><code>catalog.show(\"daily\")\n# Output:\n# path: /data/sales/daily\n# format: parquet\n# partitioning: [year, month]\n# ...\n</code></pre></p>"},{"location":"api/catalog/#load_parquet","title":"load_parquet","text":"<p><pre><code>def load_parquet(\n    self,\n    table_name: str,\n    path: str,\n    name: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    **table_options\n) -&gt; ParquetDataset\n</code></pre> Load a Parquet dataset and register it in the catalog.</p> <p>Parameters: - <code>table_name</code> (str): Name to register the table as - <code>path</code> (str): Path to the Parquet dataset - <code>name</code> (str, optional): Dataset name - <code>filesystem</code> (AbstractFileSystem, optional): Filesystem for data access - <code>bucket</code> (str, optional): Bucket name for cloud storage - <code>partitioning</code> (str | list[str]): Partitioning scheme - <code>cached</code> (bool): Whether to use caching - <code>timestamp_column</code> (str, optional): Timestamp column name - <code>**table_options</code>: Additional table options</p> <p>Returns: - <code>ParquetDataset</code>: The loaded Parquet dataset</p> <p>Example: <pre><code># Load and register Parquet dataset\ndataset = catalog.load_parquet(\n    \"sales_2023\",\n    \"/data/sales/2023\",\n    partitioning=[\"year\", \"month\"],\n    cached=True\n)\n</code></pre></p>"},{"location":"api/catalog/#load_csv","title":"load_csv","text":"<p><pre><code>def load_csv(\n    self,\n    table_name: str,\n    path: str,\n    name: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    **table_options\n) -&gt; CSVDataset\n</code></pre> Load a CSV dataset and register it in the catalog.</p> <p>Parameters: - <code>table_name</code> (str): Name to register the table as - <code>path</code> (str): Path to the CSV file/directory - <code>name</code> (str, optional): Dataset name - <code>filesystem</code> (AbstractFileSystem, optional): Filesystem for data access - <code>bucket</code> (str, optional): Bucket name for cloud storage - <code>partitioning</code> (str | list[str]): Partitioning scheme - <code>cached</code> (bool): Whether to use caching - <code>timestamp_column</code> (str, optional): Timestamp column name - <code>**table_options</code>: Additional table options</p> <p>Returns: - <code>CSVDataset</code>: The loaded CSV dataset</p> <p>Example: <pre><code># Load CSV with custom options\ndataset = catalog.load_csv(\n    \"customers\",\n    \"/data/customers.csv\",\n    parse_options={'delimiter': ',', 'header': True},\n    convert_options={'column_types': {'id': 'int64'}}\n)\n</code></pre></p>"},{"location":"api/catalog/#load_json","title":"load_json","text":"<p><pre><code>def load_json(\n    self,\n    table_name: str,\n    path: str,\n    name: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    **table_options\n) -&gt; JSONDataset\n</code></pre> Load a JSON dataset and register it in the catalog.</p> <p>Parameters: - <code>table_name</code> (str): Name to register the table as - <code>path</code> (str): Path to the JSON file/directory - <code>name</code> (str, optional): Dataset name - <code>filesystem</code> (AbstractFileSystem, optional): Filesystem for data access - <code>bucket</code> (str, optional): Bucket name for cloud storage - <code>partitioning</code> (str | list[str]): Partitioning scheme - <code>cached</code> (bool): Whether to use caching - <code>timestamp_column</code> (str, optional): Timestamp column name - <code>**table_options</code>: Additional table options</p> <p>Returns: - <code>JSONDataset</code>: The loaded JSON dataset</p> <p>Example: <pre><code># Load JSON dataset\ndataset = catalog.load_json(\n    \"events\",\n    \"/data/events\",\n    read_options={'block_size': 4096}\n)\n</code></pre></p>"},{"location":"api/catalog/#load","title":"load","text":"<p><pre><code>def load(\n    self,\n    table_name: str,\n    path: str,\n    format: str,\n    name: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    **table_options\n) -&gt; BaseDataset\n</code></pre> Load a dataset with specified format and register it in the catalog.</p> <p>Parameters: - <code>table_name</code> (str): Name to register the table as - <code>path</code> (str): Path to the dataset - <code>format</code> (str): File format ('parquet', 'csv', 'json') - <code>name</code> (str, optional): Dataset name - <code>filesystem</code> (AbstractFileSystem, optional): Filesystem for data access - <code>bucket</code> (str, optional): Bucket name for cloud storage - <code>partitioning</code> (str | list[str]): Partitioning scheme - <code>cached</code> (bool): Whether to use caching - <code>timestamp_column</code> (str, optional): Timestamp column name - <code>**table_options</code>: Additional table options</p> <p>Returns: - <code>BaseDataset</code>: The loaded dataset</p> <p>Example: <pre><code># Load with explicit format\ndataset = catalog.load(\n    \"logs\",\n    \"/data/logs\",\n    format=\"parquet\",\n    partitioning=[\"date\", \"level\"]\n)\n</code></pre></p>"},{"location":"api/catalog/#files","title":"files","text":"<p><pre><code>def files(self, table_name: str) -&gt; list[str]\n</code></pre> List files for a table.</p> <p>Parameters: - <code>table_name</code> (str): Name of the table</p> <p>Returns: - <code>list[str]</code>: List of file paths</p> <p>Example: <pre><code>files = catalog.files(\"daily\")\nprint(files)  # ['/data/sales/daily/...', ...]\n</code></pre></p>"},{"location":"api/catalog/#show_filesystem","title":"show_filesystem","text":"<p><pre><code>def show_filesystem(self, table_name: str) -&gt; None\n</code></pre> Display filesystem configuration for a table.</p> <p>Parameters: - <code>table_name</code> (str): Name of the table</p> <p>Example: <pre><code>catalog.show_filesystem(\"daily\")\n# Output:\n# protocol: file\n# ...\n</code></pre></p>"},{"location":"api/catalog/#all_filesystems","title":"all_filesystems","text":"<p><pre><code>def all_filesystems(self) -&gt; list[str]\n</code></pre> List all filesystems in the catalog.</p> <p>Returns: - <code>list[str]</code>: List of filesystem names</p> <p>Example: <pre><code>filesystems = catalog.all_filesystems()\nprint(filesystems)  # ['local', 's3', 'gcs']\n</code></pre></p>"},{"location":"api/catalog/#query-operations","title":"Query Operations","text":"<p>The catalog supports querying across registered datasets:</p> <pre><code># Simple query\nresult = catalog.query(\"SELECT * FROM sales_2023 WHERE amount &gt; 1000\")\n\n# Join across datasets\nresult = catalog.query(\"\"\"\n    SELECT\n        s.*,\n        c.customer_name,\n        c.customer_segment\n    FROM sales_2023 s\n    JOIN customers c ON s.customer_id = c.id\n    WHERE s.date &gt;= '2023-01-01'\n\"\"\")\n\n# Aggregation\nresult = catalog.query(\"\"\"\n    SELECT\n        category,\n        COUNT(*) as orders,\n        SUM(amount) as revenue\n    FROM sales_2023\n    GROUP BY category\n\"\"\")\n</code></pre>"},{"location":"api/catalog/#configuration-management","title":"Configuration Management","text":""},{"location":"api/catalog/#update-table-configuration","title":"Update Table Configuration","text":"<pre><code># Add metadata to table\ncatalog._set_table_params(\n    \"sales_2023\",\n    description=\"Sales data for 2023\",\n    owner=\"sales_team\",\n    refresh_frequency=\"daily\"\n)\n</code></pre>"},{"location":"api/catalog/#namespace-operations","title":"Namespace Operations","text":"<pre><code># Switch namespace\ncatalog.load_catalog(namespace=\"finance\")\n\n# Get current namespace\ncurrent_ns = catalog.namespace\n</code></pre>"},{"location":"api/catalog/#catalog-configuration-file","title":"Catalog Configuration File","text":"<p>The catalog uses a YAML configuration file with the following structure:</p> <pre><code># catalog.yaml\ntables:\n  sales:\n    daily:\n      path: /data/sales/daily\n      format: parquet\n      options:\n        partitioning: [year, month, day]\n        cached: true\n      metadata:\n        description: Daily sales transactions\n        owner: sales_team\n        refresh_frequency: daily\n\n    monthly:\n      path: /data/sales/monthly\n      format: parquet\n      options:\n        partitioning: [year, month]\n\n  customers:\n    active:\n      path: /data/customers/active\n      format: parquet\n      options:\n        cached: true\n\n    all:\n      path: /data/customers/all\n      format: csv\n      options:\n        delimiter: ','\n        header: true\n\nfilesystems:\n  s3_prod:\n    protocol: s3\n    key: your_access_key\n    secret: your_secret_key\n    client_kwargs:\n      region_name: us-east-1\n\n  gcs_archive:\n    protocol: gcs\n    token: /path/to/service-account.json\n</code></pre>"},{"location":"api/catalog/#best-practices","title":"Best Practices","text":""},{"location":"api/catalog/#organization","title":"Organization","text":"<pre><code># Use hierarchical namespaces\ncatalog.load_catalog(namespace=\"sales/daily\")\ncatalog.load_catalog(namespace=\"sales/monthly\")\ncatalog.load_catalog(namespace=\"finance/revenue\")\n\n# Use descriptive table names\ncatalog.load_parquet(\"transactions_2023\", \"/data/transactions/2023\")\ncatalog.load_parquet(\"transactions_2024\", \"/data/transactions/2024\")\n</code></pre>"},{"location":"api/catalog/#metadata-management","title":"Metadata Management","text":"<pre><code># Document your datasets\ncatalog._set_table_params(\n    \"sales_2023\",\n    description=\"Complete sales transactions for fiscal year 2023\",\n    business_unit=\"sales\",\n    data_owner=\"sales_analytics_team\",\n    contact=\"sales-data@company.com\",\n    sla=\"Available by 9:00 AM daily\",\n    retention_policy=\"7_years\",\n    classification=\"internal\",\n    tags=[\"sales\", \"transactions\", \"fy2023\"]\n)\n</code></pre>"},{"location":"api/catalog/#performance","title":"Performance","text":"<pre><code># Enable caching for frequently accessed data\ncatalog.load_parquet(\n    \"hot_data\",\n    \"/data/hot\",\n    cached=True,\n    cache_options={'max_size': '2GB'}\n)\n\n# Use appropriate partitioning\ncatalog.load_parquet(\n    \"time_series\",\n    \"/data/time_series\",\n    partitioning=[\"date\", \"category\"]\n)\n</code></pre>"},{"location":"api/catalog/#security","title":"Security","text":"<pre><code># Store credentials securely\n# Use environment variables or secret management\ncatalog._set_table_params(\n    \"sensitive_data\",\n    access_level=\"restricted\",\n    encryption=\"aes256\",\n    audit_required=True\n)\n</code></pre>"},{"location":"api/catalog/#error-handling","title":"Error Handling","text":"<pre><code>from pydala import PydalaException\n\ntry:\n    dataset = catalog.load_parquet(\"nonexistent\", \"/invalid/path\")\nexcept PydalaException as e:\n    print(f\"Failed to load dataset: {e}\")\n\ntry:\n    result = catalog.query(\"INVALID SQL QUERY\")\nexcept Exception as e:\n    print(f\"Query failed: {e}\")\n</code></pre>"},{"location":"api/core/","title":"Core Classes","text":"<p>This document describes the core classes that form the foundation of PyDala2.</p>"},{"location":"api/core/#basedataset","title":"BaseDataset","text":"<pre><code>class BaseDataset\n</code></pre> <p>The <code>BaseDataset</code> class provides the foundation for all dataset operations in PyDala2. It implements common functionality for working with datasets across different file formats using a dual-engine architecture (PyArrow + DuckDB).</p>"},{"location":"api/core/#constructor","title":"Constructor","text":"<pre><code>BaseDataset(\n    path: str,\n    name: str | None = None,\n    schema: pa.Schema | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    format: str | None = \"parquet\",\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    ddb_con: duckdb.DuckDBPyConnection | None = None,\n    **fs_kwargs\n) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code> (str): The path to the dataset - <code>name</code> (str, optional): The name of the dataset. If None, uses basename of path - <code>schema</code> (pa.Schema, optional): The schema for the dataset. If None, inferred from data - <code>filesystem</code> (AbstractFileSystem, optional): The filesystem to use for data access - <code>bucket</code> (str, optional): The bucket name for cloud storage - <code>partitioning</code> (str | list[str], optional): The partitioning scheme ('hive', 'ignore', or list of columns) - <code>format</code> (str, optional): The file format ('parquet', 'csv', 'json') - <code>cached</code> (bool): Whether to use caching for filesystem operations - <code>timestamp_column</code> (str, optional): The column to use as timestamp for time operations - <code>ddb_con</code> (duckdb.DuckDBPyConnection, optional): Existing DuckDB connection - <code>**fs_kwargs</code>: Additional arguments for filesystem configuration</p> <p>Example: <pre><code>from pydala import BaseDataset\nimport pyarrow as pa\n\n# Define schema\nschema = pa.schema([\n    ('id', pa.int64()),\n    ('name', pa.string()),\n    ('value', pa.float64())\n])\n\n# Create dataset\ndataset = BaseDataset(\n    path=\"data/my_dataset\",\n    schema=schema,\n    partitioning=['year', 'month'],\n    cached=True\n)\n</code></pre></p>"},{"location":"api/core/#properties","title":"Properties","text":""},{"location":"api/core/#path","title":"path","text":"<p><pre><code>@property\ndef path(self) -&gt; str\n</code></pre> Get the path of the dataset.</p> <p>Returns: - <code>str</code>: The path to the dataset</p>"},{"location":"api/core/#name","title":"name","text":"<p><pre><code>@property\ndef name(self) -&gt; str\n</code></pre> Get the name of the dataset.</p> <p>Returns: - <code>str</code>: The dataset name</p>"},{"location":"api/core/#table","title":"table","text":"<p><pre><code>@property\ndef table(self) -&gt; PydalaTable\n</code></pre> Get the PydalaTable for data operations.</p> <p>Returns: - <code>PydalaTable</code>: The table interface for data export and operations</p> <p>Example: <pre><code># Access the table interface\ntable = dataset.table\n\n# Export to Polars\ndf = table.to_polars()\n\n# Use shortcut notation\ndf = dataset.t.to_polars()  # Same as above\n</code></pre></p>"},{"location":"api/core/#t","title":"t","text":"<p><pre><code>@property\ndef t(self) -&gt; PydalaTable\n</code></pre> Shortcut property for accessing the table interface.</p> <p>Returns: - <code>PydalaTable</code>: The table interface</p>"},{"location":"api/core/#ddb_con","title":"ddb_con","text":"<p><pre><code>@property\ndef ddb_con(self) -&gt; duckdb.DuckDBPyConnection\n</code></pre> Get the DuckDB connection for SQL operations.</p> <p>Returns: - <code>duckdb.DuckDBPyConnection</code>: The DuckDB connection</p>"},{"location":"api/core/#schema","title":"schema","text":"<p><pre><code>@property\ndef schema(self) -&gt; pa.Schema\n</code></pre> Get the schema of the dataset.</p> <p>Returns: - <code>pa.Schema</code>: The schema of the dataset</p>"},{"location":"api/core/#files","title":"files","text":"<p><pre><code>@property\ndef files(self) -&gt; list[str]\n</code></pre> Get a list of files in the dataset.</p> <p>Returns: - <code>list[str]</code>: List of file paths</p>"},{"location":"api/core/#columns","title":"columns","text":"<p><pre><code>@property\ndef columns(self) -&gt; list[str]\n</code></pre> Get a list of column names.</p> <p>Returns: - <code>list[str]</code>: List of column names</p>"},{"location":"api/core/#partitions","title":"partitions","text":"<p><pre><code>@property\ndef partitions(self) -&gt; list[str]\n</code></pre> Get partition information for the dataset.</p> <p>Returns: - <code>list[str]</code>: List of partition values</p>"},{"location":"api/core/#metadata_file","title":"metadata_file","text":"<p><pre><code>@property\ndef metadata_file(self) -&gt; str | None\n</code></pre> Get the path to the metadata file if it exists.</p> <p>Returns: - <code>str | None</code>: Path to the _metadata file or None</p>"},{"location":"api/core/#methods","title":"Methods","text":""},{"location":"api/core/#load","title":"load","text":"<p><pre><code>def load(self) -&gt; None\n</code></pre> Load the dataset from the specified path and initialize the PydalaTable.</p> <p>This method creates a PyArrow dataset from the files at the specified path, initializes a PydalaTable for data operations, and registers the dataset with DuckDB for SQL queries.</p> <p>Example: <pre><code>dataset = BaseDataset(\"data/existing_dataset\")\ndataset.load()\n</code></pre></p>"},{"location":"api/core/#exists","title":"exists","text":"<p><pre><code>def exists(self) -&gt; bool\n</code></pre> Check if the dataset exists.</p> <p>Returns: - <code>bool</code>: True if the dataset exists, False otherwise</p> <p>Example: <pre><code>if dataset.exists():\n    print(f\"Dataset found with {dataset.count_rows()} rows\")\n</code></pre></p>"},{"location":"api/core/#count_rows","title":"count_rows","text":"<p><pre><code>def count_rows(self) -&gt; int\n</code></pre> Returns the number of rows in the dataset.</p> <p>Returns: - <code>int</code>: The number of rows in the dataset</p>"},{"location":"api/core/#filter","title":"filter","text":"<p><pre><code>def filter(self, filters: str) -&gt; BaseDataset\n</code></pre> Filter the dataset using the specified filters.</p> <p>Parameters: - <code>filters</code> (str): Filter expression</p> <p>Returns: - <code>BaseDataset</code>: A new dataset instance with the filters applied</p> <p>Example: <pre><code># Filter dataset\nfiltered = dataset.filter(\"date &gt; '2023-01-01' AND category IN ('A', 'B')\")\n\n# Export filtered data\nresult = filtered.table.to_polars()\n</code></pre></p>"},{"location":"api/core/#scan","title":"scan","text":"<p><pre><code>def scan(self, filters: str | None = None) -&gt; pa.dataset.Scanner\n</code></pre> Create a scanner for efficient data access.</p> <p>Parameters: - <code>filters</code> (str, optional): Filter expression</p> <p>Returns: - <code>pa.dataset.Scanner</code>: A PyArrow dataset scanner</p> <p>Example: <pre><code># Create scanner\nscanner = dataset.scan(\"value &gt; 100\")\n\n# Read data in batches\nfor batch in scanner.to_batches():\n    process_batch(batch)\n</code></pre></p>"},{"location":"api/core/#clear_cache","title":"clear_cache","text":"<p><pre><code>def clear_cache(self) -&gt; None\n</code></pre> Clear the cache for the dataset.</p> <p>This method clears the cache for both the filesystem and base filesystem.</p>"},{"location":"api/core/#delete_files","title":"delete_files","text":"<p><pre><code>def delete_files(self, files: str | list[str]) -&gt; None\n</code></pre> Delete specified files from the dataset.</p> <p>Parameters: - <code>files</code> (str | list[str]): The file(s) to be deleted</p> <p>Example: <pre><code># Delete specific files\ndataset.delete_files([\"data/file1.parquet\", \"data/file2.parquet\"])\n\n# Delete all files\ndataset.delete_files(dataset.files)\n</code></pre></p>"},{"location":"api/core/#vacuum","title":"vacuum","text":"<p><pre><code>def vacuum(self) -&gt; None\n</code></pre> Delete all files in the dataset.</p> <p>This method removes all files from the dataset directory but keeps the directory structure intact.</p>"},{"location":"api/core/#optimize","title":"Optimize","text":"<pre><code>class Optimize\n</code></pre> <p>The <code>Optimize</code> class provides dataset optimization and compaction operations.</p>"},{"location":"api/core/#methods_1","title":"Methods","text":""},{"location":"api/core/#compact_partitions","title":"compact_partitions","text":"<p><pre><code>def compact_partitions(self, target_file_size: int | None = None) -&gt; None\n</code></pre> Compact partitions within the dataset.</p> <p>Parameters: - <code>target_file_size</code> (int, optional): Target size for compacted files in bytes</p> <p>Example: <pre><code># Compact partitions with default size\ndataset.optimize.compact_partitions()\n\n# Compact with specific target size\ndataset.optimize.compact_partitions(target_file_size=100 * 1024 * 1024)  # 100MB\n</code></pre></p>"},{"location":"api/core/#compact_by_timeperiod","title":"compact_by_timeperiod","text":"<p><pre><code>def compact_by_timeperiod(\n    self,\n    timestamp_column: str,\n    timeperiod: str = \"day\",\n    target_file_size: int | None = None\n) -&gt; None\n</code></pre> Compact data by time period.</p> <p>Parameters: - <code>timestamp_column</code> (str): Column containing timestamps - <code>timeperiod</code> (str): Time period for compaction ('day', 'week', 'month', 'year') - <code>target_file_size</code> (int, optional): Target size for compacted files</p> <p>Example: <pre><code># Compact by month\ndataset.optimize.compact_by_timeperiod(\n    timestamp_column=\"date\",\n    timeperiod=\"month\"\n)\n</code></pre></p>"},{"location":"api/core/#compact_by_rows","title":"compact_by_rows","text":"<p><pre><code>def compact_by_rows(self, target_rows: int) -&gt; None\n</code></pre> Compact data to achieve target row count per file.</p> <p>Parameters: - <code>target_rows</code> (int): Target number of rows per file</p> <p>Example: <pre><code># Compact to 1M rows per file\ndataset.optimize.compact_by_rows(target_rows=1000000)\n</code></pre></p>"},{"location":"api/core/#repartition","title":"repartition","text":"<p><pre><code>def repartition(self, partition_cols: list[str]) -&gt; None\n</code></pre> Repartition the dataset.</p> <p>Parameters: - <code>partition_cols</code> (list[str]): New partitioning columns</p> <p>Example: <pre><code># Change partitioning\ndataset.optimize.repartition(partition_cols=['year', 'month', 'day'])\n</code></pre></p>"},{"location":"api/core/#optimize_dtypes","title":"optimize_dtypes","text":"<p><pre><code>def optimize_dtypes(self) -&gt; None\n</code></pre> Optimize data types to reduce memory usage.</p> <p>This method analyzes the data and converts columns to more efficient data types where possible.</p> <p>Example: <pre><code># Optimize data types\ndataset.optimize.optimize_dtypes()\n</code></pre></p>"},{"location":"api/core/#writer","title":"Writer","text":"<pre><code>class Writer\n</code></pre> <p>The <code>Writer</code> class provides data writing and transformation utilities.</p>"},{"location":"api/core/#methods_2","title":"Methods","text":""},{"location":"api/core/#write_to_dataset","title":"write_to_dataset","text":"<p><pre><code>@staticmethod\ndef write_to_dataset(\n    data: pa.Table | pd.DataFrame | pl.DataFrame,\n    dataset: BaseDataset,\n    partition_cols: list[str] | None = None,\n    basename_template: str | None = None,\n    **kwargs\n) -&gt; None\n</code></pre> Write data to a dataset.</p> <p>Parameters: - <code>data</code> (pa.Table | pd.DataFrame | pl.DataFrame): Data to write - <code>dataset</code> (BaseDataset): Target dataset - <code>partition_cols</code> (list[str], optional): Columns to partition by - <code>basename_template</code> (str, optional): Template for file names - <code>**kwargs</code>: Additional arguments for writing</p> <p>Example: <pre><code># Write data to dataset\nWriter.write_to_dataset(\n    data=dataframe,\n    dataset=dataset,\n    partition_cols=['category'],\n    basename_template=\"data-{i}.parquet\"\n)\n</code></pre></p>"},{"location":"api/core/#config","title":"Config","text":"<pre><code>class Config\n</code></pre> <p>The <code>Config</code> class manages configuration settings for PyDala2 operations.</p>"},{"location":"api/core/#class-attributes","title":"Class Attributes","text":""},{"location":"api/core/#default_config","title":"default_config","text":"<pre><code>default_config: dict = {\n    'default_backend': 'polars',\n    'cache_enabled': True,\n    'cache_max_size': 1024 * 1024 * 1024,  # 1GB\n    'cache_ttl': 3600,  # 1 hour\n    'log_level': 'INFO',\n    'max_memory': 4 * 1024 * 1024 * 1024,  # 4GB\n    'n_workers': 4,\n    'compression': 'zstd',\n    'compression_level': 3,\n    'row_group_size': 1000000,\n    'auto_discover_partitions': True,\n    'validate_schema': True,\n    'enable_profiling': False\n}\n</code></pre> <p>Default configuration values for PyDala2.</p>"},{"location":"api/core/#methods_3","title":"Methods","text":""},{"location":"api/core/#get","title":"get","text":"<p><pre><code>@classmethod\ndef get(cls, key: str, default=None)\n</code></pre> Get a configuration value.</p> <p>Parameters: - <code>key</code> (str): The configuration key - <code>default</code>: Default value if key not found</p> <p>Returns: - The configuration value</p> <p>Example: <pre><code># Get default backend\nbackend = Config.get('default_backend')\n\n# Get with default\ncache_size = Config.get('cache_max_size', 512 * 1024 * 1024)\n</code></pre></p>"},{"location":"api/core/#set","title":"set","text":"<p><pre><code>@classmethod\ndef set(cls, key: str, value) -&gt; None\n</code></pre> Set a configuration value.</p> <p>Parameters: - <code>key</code> (str): The configuration key - <code>value</code>: The value to set</p> <p>Example: <pre><code># Set default backend\nConfig.set('default_backend', 'duckdb')\n\n# Enable caching\nConfig.set('cache_enabled', True)\n</code></pre></p>"},{"location":"api/core/#update","title":"update","text":"<p><pre><code>@classmethod\ndef update(cls, config: dict) -&gt; None\n</code></pre> Update multiple configuration values.</p> <p>Parameters: - <code>config</code> (dict): Dictionary of configuration values</p> <p>Example: <pre><code>Config.update({\n    'default_backend': 'polars',\n    'cache_enabled': True,\n    'cache_max_size': 2 * 1024 * 1024 * 1024,\n    'log_level': 'DEBUG'\n})\n</code></pre></p>"},{"location":"api/core/#reset","title":"reset","text":"<p><pre><code>@classmethod\ndef reset(cls) -&gt; None\n</code></pre> Reset all configuration values to defaults.</p> <p>Example: <pre><code># Reset to defaults\nConfig.reset()\n</code></pre></p>"},{"location":"api/core/#configuration-management","title":"Configuration Management","text":"<p>PyDala2 provides several ways to manage configuration:</p>"},{"location":"api/core/#environment-variables","title":"Environment Variables","text":"<p>Configuration can be set via environment variables:</p> <pre><code>export PYDALA2_DEFAULT_BACKEND=polars\nexport PYDALA2_CACHE_ENABLED=true\nexport PYDALA2_CACHE_MAX_SIZE=1073741824\nexport PYDALA2_LOG_LEVEL=INFO\n</code></pre>"},{"location":"api/core/#configuration-file","title":"Configuration File","text":"<p>Configuration can be loaded from a YAML file:</p> <pre><code>from pydala import load_config\n\n# Load from file\nconfig = load_config(\"config.yaml\")\nConfig.update(config)\n</code></pre>"},{"location":"api/core/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from pydala import Config, set_config\n\n# Set individual values\nConfig.set('default_backend', 'polars')\n\n# Update multiple values\nset_config({\n    'cache_enabled': True,\n    'cache_max_size': 1024 * 1024 * 1024,\n    'log_level': 'INFO'\n})\n</code></pre>"},{"location":"api/core/#context-manager","title":"Context Manager","text":"<p>Configuration can be temporarily changed using a context manager:</p> <pre><code>from pydala import config_context\n\nwith config_context({'default_backend': 'duckdb'}):\n    # Use DuckDB for operations in this block\n    result = dataset.read(backend=None)  # Uses DuckDB\n\n# Back to previous configuration\n</code></pre>"},{"location":"api/core/#configuration-reference","title":"Configuration Reference","text":"Key Type Default Description <code>default_backend</code> str 'polars' Default backend for operations <code>cache_enabled</code> bool True Enable filesystem caching <code>cache_max_size</code> int 1073741824 Maximum cache size in bytes <code>cache_ttl</code> int 3600 Cache time-to-live in seconds <code>log_level</code> str 'INFO' Logging level <code>max_memory</code> int 4294967296 Maximum memory usage in bytes <code>n_workers</code> int 4 Number of worker threads <code>compression</code> str 'zstd' Default compression codec <code>compression_level</code> int 3 Default compression level <code>row_group_size</code> int 1000000 Default row group size <code>auto_discover_partitions</code> bool True Auto-discover partitioning <code>validate_schema</code> bool True Validate schema on operations <code>enable_profiling</code> bool False Enable performance profiling"},{"location":"api/datasets/","title":"Dataset Classes","text":"<p>This document describes the dataset classes that provide file format-specific operations.</p>"},{"location":"api/datasets/#parquetdataset","title":"ParquetDataset","text":"<pre><code>class ParquetDataset(PydalaDatasetMetadata, BaseDataset)\n</code></pre> <p>The <code>ParquetDataset</code> class inherits from both <code>PydalaDatasetMetadata</code> (for metadata management) and <code>BaseDataset</code> (for dataset operations). It provides comprehensive Parquet file operations with advanced metadata support, optimization features, and efficient querying.</p>"},{"location":"api/datasets/#constructor","title":"Constructor","text":"<pre><code>ParquetDataset(\n    path: str,\n    name: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    ddb_con: duckdb.DuckDBPyConnection | None = None,\n    **fs_kwargs\n) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code> (str): The path to the Parquet dataset - <code>name</code> (str, optional): The name of the dataset - <code>filesystem</code> (AbstractFileSystem, optional): Filesystem for data access - <code>bucket</code> (str, optional): Bucket name for cloud storage - <code>partitioning</code> (str | list[str], optional): Partitioning scheme - <code>cached</code> (bool): Whether to use caching - <code>timestamp_column</code> (str, optional): Timestamp column for time operations - <code>ddb_con</code> (duckdb.DuckDBPyConnection, optional): DuckDB connection - <code>**fs_kwargs</code>: Additional filesystem arguments</p> <p>Example: <pre><code>from pydala import ParquetDataset\n\n# Create dataset\ndataset = ParquetDataset(\n    path=\"data/sales\",\n    partitioning=['year', 'month'],\n    cached=True\n)\n\n# Write data\ndataset.write_to_dataset(\n    dataframe,\n    partition_cols=['year', 'month'],\n    basename_template=\"data-{i}.parquet\"\n)\n\n# Export data with filters\nresult = dataset.filter(\n    \"year = 2023 AND month IN (1, 2, 3)\"\n).table.to_polars()\n</code></pre></p>"},{"location":"api/datasets/#methods","title":"Methods","text":""},{"location":"api/datasets/#write_to_dataset","title":"write_to_dataset","text":"<p><pre><code>def write_to_dataset(\n    self,\n    data: pa.Table | pd.DataFrame | pl.DataFrame,\n    partition_cols: list[str] | None = None,\n    basename_template: str | None = None,\n    max_rows_per_file: int | None = None,\n    min_rows_per_group: int | None = None,\n    max_rows_per_group: int | None = None,\n    compression: str | None = None,\n    compression_level: int | None = None,\n    use_threads: bool | None = None,\n    **kwargs\n) -&gt; None\n</code></pre> Write data to the Parquet dataset.</p> <p>Parameters: - <code>data</code> (pa.Table | pd.DataFrame | pl.DataFrame): Data to write - <code>partition_cols</code> (list[str], optional): Columns to partition by - <code>basename_template</code> (str, optional): Template for file names - <code>max_rows_per_file</code> (int, optional): Maximum rows per output file - <code>min_rows_per_group</code> (int, optional): Minimum rows per row group - <code>max_rows_per_group</code> (int, optional): Maximum rows per row group - <code>compression</code> (str, optional): Compression codec - <code>compression_level</code> (int, optional): Compression level - <code>use_threads</code> (bool, optional): Whether to use threading - <code>**kwargs</code>: Additional write options</p> <p>Example: <pre><code># Write with partitioning\ndataset.write_to_dataset(\n    data=df,\n    partition_cols=['category', 'date'],\n    basename_template=\"data-{i}.parquet\",\n    max_rows_per_file=1000000,\n    compression='zstd',\n    compression_level=3\n)\n</code></pre></p>"},{"location":"api/datasets/#load","title":"load","text":"<p><pre><code>def load(\n    self,\n    update_metadata: bool = False,\n    reload_metadata: bool = False,\n    **kwargs\n) -&gt; None\n</code></pre> Load the dataset using PyArrow's <code>parquet_dataset()</code> function with the <code>_metadata</code> file for efficient loading.</p> <p>Parameters: - <code>update_metadata</code> (bool): Whether to update metadata before loading - <code>reload_metadata</code> (bool): Whether to reload metadata from disk - <code>**kwargs</code>: Additional arguments</p> <p>Example: <pre><code># Load dataset (uses _metadata file for instant loading)\ndataset.load()\n\n# Load with metadata update\ndataset.load(update_metadata=True)\n\n# Access the loaded Arrow dataset\narrow_dataset = dataset._arrow_dataset\n</code></pre></p>"},{"location":"api/datasets/#scan","title":"scan","text":"<p><pre><code>def scan(\n    self,\n    filters: str | list[str] | dict | None = None,\n    files: list[str] | None = None,\n    columns: list[str] | None = None,\n    verbose: bool = False,\n    return_table: bool = False,\n    **kwargs\n) -&gt; list[str] | pa.Table\n</code></pre> Scan files based on metadata statistics without reading their contents.</p> <p>Parameters: - <code>filters</code> (str | list[str] | dict, optional): Filter conditions - <code>files</code> (list[str], optional): Specific files to scan - <code>columns</code> (list[str], optional): Columns to consider in filters - <code>verbose</code> (bool): Whether to print verbose output - <code>return_table</code> (bool): Whether to return results as a table - <code>**kwargs</code>: Additional arguments</p> <p>Returns: - <code>list[str] | pa.Table</code>: List of matching files or results table</p> <p>Example: <pre><code># Scan files based on date range\nmatching_files = dataset.scan(\n    filters=\"date &gt;= '2023-01-01' AND date &lt;= '2023-12-31'\",\n    verbose=True\n)\n\n# Get results as a table\nresults = dataset.scan(\n    filters=\"status = 'completed'\",\n    return_table=True\n)\n</code></pre></p>"},{"location":"api/datasets/#optimize_dtypes","title":"optimize_dtypes","text":"<p><pre><code>def optimize_dtypes(self) -&gt; None\n</code></pre> Optimize data types to reduce memory usage.</p> <p>This method is accessed through the optimize attribute:</p> <p>Example: <pre><code># Optimize data types\ndataset.optimize.optimize_dtypes()\n</code></pre></p>"},{"location":"api/datasets/#compact_partitions","title":"compact_partitions","text":"<p><pre><code>def compact_partitions(self, target_file_size: int | None = None) -&gt; None\n</code></pre> Compact partitions within the dataset.</p> <p>This method is accessed through the optimize attribute:</p> <p>Parameters: - <code>target_file_size</code> (int, optional): Target size for compacted files in bytes</p> <p>Example: <pre><code># Compact partitions\ndataset.optimize.compact_partitions(target_file_size=100 * 1024 * 1024)\n</code></pre></p>"},{"location":"api/datasets/#compact_by_timeperiod","title":"compact_by_timeperiod","text":"<p><pre><code>def compact_by_timeperiod(\n    self,\n    timestamp_column: str,\n    timeperiod: str = \"day\",\n    target_file_size: int | None = None\n) -&gt; None\n</code></pre> Compact data by time period.</p> <p>This method is accessed through the optimize attribute:</p> <p>Parameters: - <code>timestamp_column</code> (str): Column containing timestamps - <code>timeperiod</code> (str): Time period for compaction ('day', 'week', 'month', 'year') - <code>target_file_size</code> (int, optional): Target size for compacted files</p> <p>Example: <pre><code># Compact by month\ndataset.optimize.compact_by_timeperiod(\n    timestamp_column=\"date\",\n    timeperiod=\"month\"\n)\n</code></pre></p>"},{"location":"api/datasets/#compact_by_rows","title":"compact_by_rows","text":"<p><pre><code>def compact_by_rows(self, target_rows: int) -&gt; None\n</code></pre> Compact data to achieve target row count per file.</p> <p>This method is accessed through the optimize attribute:</p> <p>Parameters: - <code>target_rows</code> (int): Target number of rows per file</p> <p>Example: <pre><code># Compact to 1M rows per file\ndataset.optimize.compact_by_rows(target_rows=1000000)\n</code></pre></p>"},{"location":"api/datasets/#repartition","title":"repartition","text":"<p><pre><code>def repartition(self, partition_cols: list[str]) -&gt; None\n</code></pre> Repartition the dataset.</p> <p>This method is accessed through the optimize attribute:</p> <p>Parameters: - <code>partition_cols</code> (list[str]): New partitioning columns</p> <p>Example: <pre><code># Change partitioning\ndataset.optimize.repartition(partition_cols=['year', 'month', 'day'])\n</code></pre></p>"},{"location":"api/datasets/#pyarrowdataset","title":"PyarrowDataset","text":"<pre><code>class PyarrowDataset(BaseDataset)\n</code></pre> <p>Dataset implementation using PyArrow's native capabilities.</p>"},{"location":"api/datasets/#constructor_1","title":"Constructor","text":"<pre><code>PyarrowDataset(\n    path: str,\n    name: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    ddb_con: duckdb.DuckDBPyConnection | None = None,\n    **fs_kwargs\n) -&gt; None\n</code></pre>"},{"location":"api/datasets/#methods_1","title":"Methods","text":""},{"location":"api/datasets/#load_1","title":"load","text":"<p><pre><code>def load(self) -&gt; None\n</code></pre> Load the dataset using PyArrow's dataset reading capabilities.</p> <p>Example: <pre><code>from pydala import PyarrowDataset\n\ndataset = PyarrowDataset(\"data/dataset\")\ndataset.load()\n\n# Access as Arrow dataset\narrow_dataset = dataset.to_arrow_dataset()\n</code></pre></p>"},{"location":"api/datasets/#csvdataset","title":"CSVDataset","text":"<pre><code>class CSVDataset(BaseDataset)\n</code></pre> <p>Dataset implementation for CSV files with support for various CSV formats and options.</p>"},{"location":"api/datasets/#constructor_2","title":"Constructor","text":"<pre><code>CSVDataset(\n    path: str,\n    name: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    ddb_con: duckdb.DuckDBPyConnection | None = None,\n    **fs_kwargs\n) -&gt; None\n</code></pre>"},{"location":"api/datasets/#methods_2","title":"Methods","text":""},{"location":"api/datasets/#load_2","title":"load","text":"<p><pre><code>def load(self) -&gt; None\n</code></pre> Load CSV files using PyArrow's CSV reading capabilities.</p> <p>Example: <pre><code>from pydala import CSVDataset\n\ndataset = CSVDataset(\"data/sales.csv\")\ndataset.load()\n\n# Read with CSV options\ndata = dataset.read(\n    parse_options={'delimiter': ',', 'quote_char': '\"'},\n    convert_options={'column_types': {'id': pa.int64()}}\n)\n</code></pre></p>"},{"location":"api/datasets/#jsondataset","title":"JSONDataset","text":"<pre><code>class JSONDataset(BaseDataset)\n</code></pre> <p>Dataset implementation for JSON files with support for nested JSON structures.</p>"},{"location":"api/datasets/#constructor_3","title":"Constructor","text":"<pre><code>JSONDataset(\n    path: str,\n    name: str | None = None,\n    filesystem: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    partitioning: str | list[str] | None = None,\n    cached: bool = False,\n    timestamp_column: str | None = None,\n    ddb_con: duckdb.DuckDBPyConnection | None = None,\n    **fs_kwargs\n) -&gt; None\n</code></pre>"},{"location":"api/datasets/#methods_3","title":"Methods","text":""},{"location":"api/datasets/#load_3","title":"load","text":"<p><pre><code>def load(self) -&gt; None\n</code></pre> Load JSON files using PyArrow's JSON reading capabilities.</p> <p>Example: <pre><code>from pydala import JSONDataset\n\ndataset = JSONDataset(\"data/events\")\ndataset.load()\n\n# Read JSON data\ndata = dataset.read(\n    read_options={'block_size': 4096},\n    parse_options={'explicit_schema': schema}\n)\n</code></pre></p>"},{"location":"api/datasets/#common-dataset-operations","title":"Common Dataset Operations","text":""},{"location":"api/datasets/#writing-data","title":"Writing Data","text":"<p>All dataset classes support writing data using the <code>write_to_dataset</code> method:</p> <pre><code># Write with partitioning\ndataset.write_to_dataset(\n    data=dataframe,\n    partition_cols=['year', 'month'],\n    basename_template=\"data-{i}.parquet\"\n)\n\n# Write with compression\ndataset.write_to_dataset(\n    data=dataframe,\n    compression='zstd',\n    compression_level=3\n)\n</code></pre>"},{"location":"api/datasets/#readingexporting-data","title":"Reading/Exporting Data","text":"<p>Data is accessed through the <code>table</code> property or its shortcut <code>t</code>:</p> <pre><code># Export to different formats\ndf_polars = dataset.table.to_polars()  # LazyFrame\ndf_pandas = dataset.table.df  # Pandas DataFrame\ntable_arrow = dataset.table.arrow  # PyArrow Table\n\n# Use shortcut notation\ndf_polars = dataset.t.to_polars()  # Same as above\n</code></pre>"},{"location":"api/datasets/#filtering-data","title":"Filtering Data","text":"<pre><code># Filter with automatic backend selection\nfiltered = dataset.filter(\"date &gt; '2023-01-01' AND category = 'premium'\")\n\n# Export filtered data\nresult = filtered.table.to_polars()\nresult = filtered.t.df  # Using shortcut\n</code></pre>"},{"location":"api/datasets/#using-sql-with-duckdb","title":"Using SQL with DuckDB","text":"<pre><code># Execute SQL queries\nresult = dataset.ddb_con.sql(\"\"\"\n    SELECT category, AVG(amount) as avg_amount\n    FROM dataset\n    WHERE date &gt; '2023-01-01'\n    GROUP BY category\n\"\"\").to_arrow()\n</code></pre>"},{"location":"api/datasets/#schema-operations","title":"Schema Operations","text":"<pre><code># View schema\nprint(dataset.schema)\n\n# View columns\nprint(dataset.columns)\n\n# Check if dataset exists\nif dataset.exists():\n    print(f\"Dataset has {dataset.count_rows()} rows\")\n</code></pre>"},{"location":"api/datasets/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Compact partitions\ndataset.optimize.compact_partitions(target_file_size=100 * 1024 * 1024)\n\n# Compact by time period\ndataset.optimize.compact_by_timeperiod(\n    timestamp_column=\"date\",\n    timeperiod=\"month\"\n)\n\n# Optimize data types\ndataset.optimize.optimize_dtypes()\n\n# Repartition dataset\ndataset.optimize.repartition(partition_cols=['year', 'month', 'day'])\n</code></pre>"},{"location":"api/datasets/#dataset-selection-guide","title":"Dataset Selection Guide","text":"Dataset Type Use Case Features <code>ParquetDataset</code> Columnar data, analytics Compression, partitioning, metadata <code>PyarrowDataset</code> Arrow-native operations Maximum Arrow compatibility <code>CSVDataset</code> CSV data interchange Flexible CSV parsing options <code>JSONDataset</code> Nested/semi-structured data JSON schema support <p>Choose the appropriate dataset type based on your data format and use case requirements.</p>"},{"location":"api/filesystem/","title":"Filesystem","text":"<p>This document describes the <code>FileSystem</code> class which provides advanced filesystem operations with caching capabilities.</p>"},{"location":"api/filesystem/#filesystem_1","title":"FileSystem","text":"<pre><code>class FileSystem\n</code></pre> <p>The <code>FileSystem</code> class wraps fsspec filesystems with added caching and monitoring capabilities. It provides a unified interface for working with different storage backends including local filesystem, S3, GCS, and others.</p>"},{"location":"api/filesystem/#constructor","title":"Constructor","text":"<pre><code>FileSystem(\n    fs: AbstractFileSystem | None = None,\n    bucket: str | None = None,\n    cached: bool = False,\n    cache_storage: str | None = None,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Parameters: - <code>fs</code> (AbstractFileSystem, optional): Existing fsspec filesystem instance - <code>bucket</code> (str, optional): Bucket name for cloud storage - <code>cached</code> (bool): Whether to enable caching - <code>cache_storage</code> (str, optional): Path to cache storage directory - <code>**kwargs</code>: Additional filesystem configuration</p> <p>Example: <pre><code>from pydala import FileSystem\n\n# Local filesystem with caching\nfs = FileSystem(cached=True, cache_storage=\"/tmp/pydala2_cache\")\n\n# S3 filesystem\nfs = FileSystem(\n    protocol=\"s3\",\n    bucket=\"my-bucket\",\n    key=\"access_key\",\n    secret=\"secret_key\",\n    cached=True\n)\n\n# GCS filesystem\nfs = FileSystem(\n    protocol=\"gcs\",\n    token=\"/path/to/service-account.json\",\n    cached=True\n)\n</code></pre></p>"},{"location":"api/filesystem/#properties","title":"Properties","text":""},{"location":"api/filesystem/#fs","title":"fs","text":"<p><pre><code>@property\ndef fs(self) -&gt; AbstractFileSystem\n</code></pre> Get the underlying fsspec filesystem.</p> <p>Returns: - <code>AbstractFileSystem</code>: The underlying filesystem instance</p>"},{"location":"api/filesystem/#methods","title":"Methods","text":""},{"location":"api/filesystem/#open","title":"open","text":"<p><pre><code>def open(\n    self,\n    path: str,\n    mode: str = \"rb\",\n    block_size: int | None = None,\n    cache_options: dict | None = None,\n    compression: str | None = None,\n    **kwargs\n) -&gt; fsspec.core.OpenFile\n</code></pre> Open a file for reading or writing.</p> <p>Parameters: - <code>path</code> (str): Path to the file - <code>mode</code> (str): File mode ('r', 'w', 'rb', 'wb', etc.) - <code>block_size</code> (int, optional): Block size for reading/writing - <code>cache_options</code> (dict, optional): Cache options - <code>compression</code> (str, optional): Compression type - <code>**kwargs</code>: Additional arguments</p> <p>Returns: - <code>fsspec.core.OpenFile</code>: File-like object</p> <p>Example: <pre><code># Read file\nwith fs.open(\"data/file.parquet\", \"rb\") as f:\n    data = f.read()\n\n# Write file\nwith fs.open(\"data/output.parquet\", \"wb\") as f:\n    f.write(data)\n</code></pre></p>"},{"location":"api/filesystem/#glob","title":"glob","text":"<p><pre><code>def glob(self, path: str, **kwargs) -&gt; list[str]\n</code></pre> Find files matching a pattern.</p> <p>Parameters: - <code>path</code> (str): Glob pattern - <code>**kwargs</code>: Additional glob options</p> <p>Returns: - <code>list[str]</code>: List of matching file paths</p> <p>Example: <pre><code># Find all Parquet files\nfiles = fs.glob(\"data/**/*.parquet\")\n\n# Find files with date pattern\nfiles = fs.glob(\"data/sales/2023-*.parquet\")\n</code></pre></p>"},{"location":"api/filesystem/#exists","title":"exists","text":"<p><pre><code>def exists(self, path: str) -&gt; bool\n</code></pre> Check if a path exists.</p> <p>Parameters: - <code>path</code> (str): Path to check</p> <p>Returns: - <code>bool</code>: True if path exists, False otherwise</p> <p>Example: <pre><code>if fs.exists(\"data/dataset\"):\n    print(\"Dataset exists\")\n</code></pre></p>"},{"location":"api/filesystem/#ls","title":"ls","text":"<p><pre><code>def ls(self, path: str, detail: bool = False, **kwargs) -&gt; list | dict\n</code></pre> List contents of a directory.</p> <p>Parameters: - <code>path</code> (str): Directory path - <code>detail</code> (bool): Whether to return detailed information - <code>**kwargs</code>: Additional options</p> <p>Returns: - <code>list | dict</code>: List of file names or detailed info</p> <p>Example: <pre><code># Simple listing\ncontents = fs.ls(\"data/\")\n\n# Detailed listing\ndetails = fs.ls(\"data/\", detail=True)\nfor item in details:\n    print(f\"{item['name']}: {item['size']} bytes\")\n</code></pre></p>"},{"location":"api/filesystem/#mkdirs","title":"mkdirs","text":"<p><pre><code>def mkdirs(self, path: str, exist_ok: bool = True) -&gt; None\n</code></pre> Create directories recursively.</p> <p>Parameters: - <code>path</code> (str): Directory path to create - <code>exist_ok</code> (bool): Whether to ignore if directory exists</p> <p>Example: <pre><code># Create directory structure\nfs.mkdirs(\"data/sales/2023/01\")\n</code></pre></p>"},{"location":"api/filesystem/#rm","title":"rm","text":"<p><pre><code>def rm(self, path: str, recursive: bool = False) -&gt; None\n</code></pre> Remove files or directories.</p> <p>Parameters: - <code>path</code> (str): Path to remove - <code>recursive</code> (bool): Whether to remove recursively</p> <p>Example: <pre><code># Remove single file\nfs.rm(\"data/old_file.parquet\")\n\n# Remove directory\nfs.rm(\"data/old_dataset\", recursive=True)\n</code></pre></p>"},{"location":"api/filesystem/#mv","title":"mv","text":"<p><pre><code>def mv(self, path1: str, path2: str, **kwargs) -&gt; None\n</code></pre> Move/rename files or directories.</p> <p>Parameters: - <code>path1</code> (str): Source path - <code>path2</code> (str): Destination path - <code>**kwargs</code>: Additional options</p> <p>Example: <pre><code># Rename file\nfs.mv(\"data/old_name.parquet\", \"data/new_name.parquet\")\n\n# Move directory\nfs.mv(\"data/temp\", \"data/archive\")\n</code></pre></p>"},{"location":"api/filesystem/#cp","title":"cp","text":"<p><pre><code>def cp(self, path1: str, path2: str, recursive: bool = False, **kwargs) -&gt; None\n</code></pre> Copy files or directories.</p> <p>Parameters: - <code>path1</code> (str): Source path - <code>path2</code> (str): Destination path - <code>recursive</code> (bool): Whether to copy recursively - <code>**kwargs</code>: Additional options</p> <p>Example: <pre><code># Copy file\nfs.cp(\"data/source.parquet\", \"data/backup.parquet\")\n\n# Copy directory\nfs.cp(\"data/dataset\", \"data/backup\", recursive=True)\n</code></pre></p>"},{"location":"api/filesystem/#touch","title":"touch","text":"<p><pre><code>def touch(self, path: str, truncate: bool = True, **kwargs) -&gt; None\n</code></pre> Create an empty file or update timestamp.</p> <p>Parameters: - <code>path</code> (str): File path - <code>truncate</code> (bool): Whether to truncate if file exists - <code>**kwargs</code>: Additional options</p> <p>Example: <pre><code># Create empty file\nfs.touch(\"data/placeholder.txt\")\n</code></pre></p>"},{"location":"api/filesystem/#info","title":"info","text":"<p><pre><code>def info(self, path: str, **kwargs) -&gt; dict\n</code></pre> Get file information.</p> <p>Parameters: - <code>path</code> (str): File path - <code>**kwargs</code>: Additional options</p> <p>Returns: - <code>dict</code>: File information dictionary</p> <p>Example: <pre><code>info = fs.info(\"data/file.parquet\")\nprint(f\"Size: {info['size']} bytes\")\nprint(f\"Modified: {info['modified']}\")\n</code></pre></p>"},{"location":"api/filesystem/#size","title":"size","text":"<p><pre><code>def size(self, path: str) -&gt; int\n</code></pre> Get file size in bytes.</p> <p>Parameters: - <code>path</code> (str): File path</p> <p>Returns: - <code>int</code>: File size in bytes</p> <p>Example: <pre><code>size = fs.size(\"data/large_file.parquet\")\nprint(f\"File size: {size / 1024 / 1024:.1f} MB\")\n</code></pre></p>"},{"location":"api/filesystem/#cat","title":"cat","text":"<p><pre><code>def cat(self, path: str, **kwargs) -&gt; bytes\n</code></pre> Read entire file contents.</p> <p>Parameters: - <code>path</code> (str): File path - <code>**kwargs</code>: Additional options</p> <p>Returns: - <code>bytes</code>: File contents</p> <p>Example: <pre><code>contents = fs.cat(\"data/metadata.json\")\nmetadata = json.loads(contents)\n</code></pre></p>"},{"location":"api/filesystem/#cat_file","title":"cat_file","text":"<p><pre><code>def cat_file(self, path, start=None, end=None, **kwargs)\n</code></pre> Read a range of bytes from a file.</p> <p>Parameters: - <code>path</code> (str): File path - <code>start</code> (int, optional): Start byte offset - <code>end</code> (int, optional): End byte offset - <code>**kwargs</code>: Additional options</p> <p>Returns: - <code>bytes</code>: File contents in range</p> <p>Example: <pre><code># Read first 1KB\nheader = fs.cat_file(\"data/file.parquet\", end=1024)\n</code></pre></p>"},{"location":"api/filesystem/#put","title":"put","text":"<p><pre><code>def put(self, lpath, rpath, recursive=False, **kwargs)\n</code></pre> Upload local files to remote filesystem.</p> <p>Parameters: - <code>lpath</code> (str): Local path - <code>rpath</code> (str): Remote path - <code>recursive</code> (bool): Whether to upload recursively - <code>**kwargs</code>: Additional options</p> <p>Example: <pre><code># Upload single file\nfs.put(\"local/data.parquet\", \"remote/data.parquet\")\n\n# Upload directory\nfs.put(\"local/dataset\", \"remote/dataset\", recursive=True)\n</code></pre></p>"},{"location":"api/filesystem/#get","title":"get","text":"<p><pre><code>def get(self, rpath, lpath, recursive=False, **kwargs)\n</code></pre> Download files from remote filesystem.</p> <p>Parameters: - <code>rpath</code> (str): Remote path - <code>lpath</code> (str): Local path - <code>recursive</code> (bool): Whether to download recursively - <code>**kwargs</code>: Additional options</p> <p>Example: <pre><code># Download single file\nfs.get(\"remote/data.parquet\", \"local/data.parquet\")\n\n# Download directory\nfs.get(\"remote/dataset\", \"local/dataset\", recursive=True)\n</code></pre></p>"},{"location":"api/filesystem/#expand_path","title":"expand_path","text":"<p><pre><code>def expand_path(self, path, **kwargs)\n</code></pre> Expand a path with wildcards.</p> <p>Parameters: - <code>path</code> (str): Path with possible wildcards - <code>**kwargs</code>: Additional options</p> <p>Returns: - <code>list</code>: Expanded paths</p> <p>Example: <pre><code>paths = fs.expand_path(\"data/sales/*.parquet\")\n</code></pre></p>"},{"location":"api/filesystem/#caching","title":"Caching","text":"<p>The FileSystem class supports caching for improved performance:</p> <pre><code># Enable caching\nfs = FileSystem(\n    protocol=\"s3\",\n    bucket=\"my-bucket\",\n    cached=True,\n    cache_storage=\"/tmp/pydala2_cache\",\n    cache_options={\n        'expiry_time': 3600,  # 1 hour\n        'cache_check': False  # Don't check if cached file is stale\n    }\n)\n\n# Subsequent reads will be faster\ndata1 = fs.cat(\"large_file.parquet\")  # First read - from remote\ndata2 = fs.cat(\"large_file.parquet\")  # Second read - from cache\n</code></pre>"},{"location":"api/filesystem/#cloud-storage-examples","title":"Cloud Storage Examples","text":""},{"location":"api/filesystem/#s3-configuration","title":"S3 Configuration","text":"<pre><code># S3 with credentials\nfs = FileSystem(\n    protocol=\"s3\",\n    bucket=\"my-bucket\",\n    key=\"your-access-key\",\n    secret=\"your-secret-key\",\n    client_kwargs={\n        \"region_name\": \"us-east-1\"\n    },\n    cached=True\n)\n\n# S3 with IAM role\nfs = FileSystem(\n    protocol=\"s3\",\n    bucket=\"my-bucket\",\n    client_kwargs={\n        \"region_name\": \"us-east-1\"\n    }\n)\n</code></pre>"},{"location":"api/filesystem/#gcs-configuration","title":"GCS Configuration","text":"<pre><code># GCS with service account\nfs = FileSystem(\n    protocol=\"gcs\",\n    token=\"/path/to/service-account.json\",\n    bucket=\"my-bucket\",\n    cached=True\n)\n\n# GCS with default credentials\nfs = FileSystem(\n    protocol=\"gcs\",\n    bucket=\"my-bucket\"\n)\n</code></pre>"},{"location":"api/filesystem/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code># Azure with connection string\nfs = FileSystem(\n    protocol=\"abfs\",\n    account_name=\"myaccount\",\n    account_key=\"mykey\",\n    bucket=\"mycontainer\"\n)\n</code></pre>"},{"location":"api/filesystem/#performance-monitoring","title":"Performance Monitoring","text":"<p>The FileSystem class includes basic performance monitoring:</p> <pre><code>import time\n\n# Time operations\nstart = time.time()\nfs.cat(\"large_file.parquet\")\nduration = time.time() - start\nprint(f\"Read took {duration:.2f} seconds\")\n\n# Check cache statistics\nif hasattr(fs, 'cache_stats'):\n    print(f\"Cache hits: {fs.cache_stats['hits']}\")\n    print(f\"Cache misses: {fs.cache_stats['misses']}\")\n</code></pre>"},{"location":"api/filesystem/#error-handling","title":"Error Handling","text":"<pre><code>from fsspec.exceptions import FileNotFoundError\n\ntry:\n    data = fs.cat(\"nonexistent.file\")\nexcept FileNotFoundError:\n    print(\"File not found\")\n\ntry:\n    fs.open(\"/protected/file.txt\", \"w\")\nexcept PermissionError:\n    print(\"Permission denied\")\n</code></pre>"},{"location":"api/filesystem/#best-practices","title":"Best Practices","text":"<ol> <li>Enable caching for frequently accessed data</li> <li>Use appropriate block sizes for large files</li> <li>Batch operations when possible</li> <li>Handle connection errors gracefully</li> <li>Monitor cache usage and clean up periodically</li> </ol>"},{"location":"api/filesystem/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>cached</code> bool False Enable filesystem caching <code>cache_storage</code> str None Cache storage directory <code>cache_options</code> dict {} Cache configuration <code>block_size</code> int None Default block size <code>client_kwargs</code> dict {} Client configuration"},{"location":"api/metadata/","title":"Metadata Management","text":"<p>This document describes the metadata classes in PyDala2 that handle collection, storage, and management of dataset metadata.</p>"},{"location":"api/metadata/#overview","title":"Overview","text":"<p>PyDala2 implements a sophisticated two-tier metadata system that leverages Apache Arrow's metadata capabilities:</p> <ol> <li>ParquetDatasetMetadata: Base class that manages <code>_metadata</code> and <code>_file_metadata</code> files</li> <li>PydalaDatasetMetadata: Extends the base class with DuckDB integration and advanced scanning capabilities</li> </ol> <p>The metadata system creates consolidated metadata files that contain schema and row group information, enabling efficient dataset operations without directory crawling. PyArrow's <code>parquet_dataset()</code> function uses the <code>_metadata</code> file to instantly create datasets without scanning individual files.</p>"},{"location":"api/metadata/#parquetdatasetmetadata","title":"ParquetDatasetMetadata","text":"<pre><code>class ParquetDatasetMetadata\n</code></pre> <p>The <code>ParquetDatasetMetadata</code> class is the foundation of PyDala2's metadata system, managing consolidated and individual file metadata for Parquet datasets.</p>"},{"location":"api/metadata/#architecture","title":"Architecture","text":"<p>The class maintains two types of metadata files:</p> <ul> <li><code>_metadata</code>: Consolidated metadata from all files, used by PyArrow's <code>parquet_dataset()</code> for efficient dataset loading</li> <li><code>_file_metadata</code>: Detailed metadata for individual files, stored with brotli compression</li> </ul>"},{"location":"api/metadata/#constructor","title":"Constructor","text":"<pre><code>ParquetDatasetMetadata(\n    path: str,\n    filesystem: AbstractFileSystem | pfs.FileSystem | None = None,\n    use_cache: bool = True,\n    cache_key: str | None = None,\n    partitioning: str | list[str] | None = None,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code> (str): Path to the Parquet dataset - <code>filesystem</code> (AbstractFileSystem | pfs.FileSystem, optional): Filesystem for data access - <code>use_cache</code> (bool): Whether to cache metadata (default: True) - <code>cache_key</code> (str, optional): Key for caching - <code>partitioning</code> (str | list[str], optional): Partitioning scheme - <code>**kwargs</code>: Additional arguments</p>"},{"location":"api/metadata/#properties","title":"Properties","text":""},{"location":"api/metadata/#path","title":"path","text":"<p><pre><code>@property\ndef path(self) -&gt; str\n</code></pre> Get the dataset path.</p> <p>Returns: - <code>str</code>: Dataset path</p>"},{"location":"api/metadata/#filesystem","title":"filesystem","text":"<p><pre><code>@property\ndef filesystem(self) -&gt; AbstractFileSystem | pfs.FileSystem\n</code></pre> Get the filesystem instance.</p> <p>Returns: - <code>AbstractFileSystem | pfs.FileSystem</code>: Filesystem instance</p>"},{"location":"api/metadata/#metadata_file","title":"metadata_file","text":"<p><pre><code>@property\ndef metadata_file(self) -&gt; str\n</code></pre> Get the path to the _metadata file.</p> <p>Returns: - <code>str</code>: Path to _metadata file</p>"},{"location":"api/metadata/#file_metadata_file","title":"file_metadata_file","text":"<p><pre><code>@property\ndef file_metadata_file(self) -&gt; str\n</code></pre> Get the path to the _file_metadata file.</p> <p>Returns: - <code>str</code>: Path to _file_metadata file</p>"},{"location":"api/metadata/#has_metadata_file","title":"has_metadata_file","text":"<p><pre><code>@property\ndef has_metadata_file(self) -&gt; bool\n</code></pre> Check if _metadata file exists.</p> <p>Returns: - <code>bool</code>: True if _metadata file exists</p>"},{"location":"api/metadata/#has_file_metadata_file","title":"has_file_metadata_file","text":"<p><pre><code>@property\ndef has_file_metadata_file(self) -&gt; bool\n</code></pre> Check if _file_metadata file exists.</p> <p>Returns: - <code>bool</code>: True if _file_metadata file exists</p>"},{"location":"api/metadata/#metadata","title":"metadata","text":"<p><pre><code>@property\ndef metadata(self) -&gt; pa.Table\n</code></pre> Get the consolidated metadata as a PyArrow Table.</p> <p>Returns: - <code>pa.Table</code>: Metadata table</p>"},{"location":"api/metadata/#file_metadata","title":"file_metadata","text":"<p><pre><code>@property\ndef file_metadata(self) -&gt; list[pyarrow.parquet.FileMetaData]\n</code></pre> Get individual file metadata objects.</p> <p>Returns: - <code>list[pyarrow.parquet.FileMetaData]</code>: List of file metadata objects</p>"},{"location":"api/metadata/#files","title":"files","text":"<p><pre><code>@property\ndef files(self) -&gt; list[str]\n</code></pre> Get list of Parquet files in the dataset.</p> <p>Returns: - <code>list[str]</code>: List of file paths</p>"},{"location":"api/metadata/#num_files","title":"num_files","text":"<p><pre><code>@property\ndef num_files(self) -&gt; int\n</code></pre> Get the number of files in the dataset.</p> <p>Returns: - <code>int</code>: Number of files</p>"},{"location":"api/metadata/#num_row_groups","title":"num_row_groups","text":"<p><pre><code>@property\ndef num_row_groups(self) -&gt; int\n</code></pre> Get the total number of row groups.</p> <p>Returns: - <code>int</code>: Number of row groups</p>"},{"location":"api/metadata/#schema","title":"schema","text":"<p><pre><code>@property\ndef schema(self) -&gt; pa.Schema\n</code></pre> Get the unified schema of the dataset.</p> <p>Returns: - <code>pa.Schema</code>: Dataset schema</p>"},{"location":"api/metadata/#columns","title":"columns","text":"<p><pre><code>@property\ndef columns(self) -&gt; list[str]\n</code></pre> Get the list of column names.</p> <p>Returns: - <code>list[str]</code>: Column names</p>"},{"location":"api/metadata/#size_bytes","title":"size_bytes","text":"<p><pre><code>@property\ndef size_bytes(self) -&gt; int\n</code></pre> Get the total size of the dataset in bytes.</p> <p>Returns: - <code>int</code>: Size in bytes</p>"},{"location":"api/metadata/#num_rows","title":"num_rows","text":"<p><pre><code>@property\ndef num_rows(self) -&gt; int\n</code></pre> Get the total number of rows in the dataset.</p> <p>Returns: - <code>int</code>: Total row count</p>"},{"location":"api/metadata/#methods","title":"Methods","text":""},{"location":"api/metadata/#update","title":"update","text":"<p><pre><code>def update(\n    self,\n    reload: bool = False,\n    verbose: bool = False,\n    update_file_metadata: bool = True,\n    update_metadata: bool = True,\n    repair: bool = True,\n    **kwargs\n) -&gt; None\n</code></pre> Update metadata files. This is the main method for metadata management.</p> <p>Parameters: - <code>reload</code> (bool): Whether to reload existing metadata - <code>verbose</code> (bool): Whether to print verbose output - <code>update_file_metadata</code> (bool): Whether to update _file_metadata - <code>update_metadata</code> (bool): Whether to update _metadata - <code>repair</code> (bool): Whether to repair file schemas - <code>**kwargs</code>: Additional arguments</p> <p>Example: <pre><code># Update all metadata\nmetadata.update(verbose=True)\n\n# Update only consolidated metadata\nmetadata.update(update_file_metadata=False)\n</code></pre></p>"},{"location":"api/metadata/#update_file_metadata","title":"update_file_metadata","text":"<p><pre><code>def update_file_metadata(\n    self,\n    files: list[str] | None = None,\n    verbose: bool = False,\n    **kwargs\n) -&gt; None\n</code></pre> Update the _file_metadata file with individual file metadata.</p> <p>Parameters: - <code>files</code> (list[str], optional): Specific files to update. If None, updates all files - <code>verbose</code> (bool): Whether to print verbose output</p> <p>Example: <pre><code># Update file metadata for specific files\nmetadata.update_file_metadata(files=[\"file1.parquet\", \"file2.parquet\"])\n</code></pre></p>"},{"location":"api/metadata/#_update_metadata","title":"_update_metadata","text":"<p><pre><code>def _update_metadata(self, **kwargs) -&gt; None\n</code></pre> Internal method to update the consolidated _metadata file.</p> <p>Parameters: - <code>**kwargs</code>: Additional arguments</p>"},{"location":"api/metadata/#_read_metadata","title":"_read_metadata","text":"<p><pre><code>def _read_metadata(self) -&gt; pa.Table\n</code></pre> Internal method to read metadata from files.</p> <p>Returns: - <code>pa.Table</code>: Metadata table</p>"},{"location":"api/metadata/#_repair_file_schemas","title":"_repair_file_schemas","text":"<p><pre><code>def _repair_file_schemas(self, verbose: bool = False, **kwargs) -&gt; None\n</code></pre> Internal method to repair inconsistent file schemas.</p> <p>Parameters: - <code>verbose</code> (bool): Whether to print verbose output - <code>**kwargs</code>: Additional arguments</p>"},{"location":"api/metadata/#pydaladatasetmetadata","title":"PydalaDatasetMetadata","text":"<pre><code>class PydalaDatasetMetadata(ParquetDatasetMetadata)\n</code></pre> <p>The <code>PydalaDatasetMetadata</code> class extends <code>ParquetDatasetMetadata</code> with DuckDB integration and advanced scanning capabilities for metadata-based filtering.</p>"},{"location":"api/metadata/#constructor_1","title":"Constructor","text":"<pre><code>PydalaDatasetMetadata(\n    path: str,\n    filesystem: AbstractFileSystem | pfs.FileSystem | None = None,\n    use_cache: bool = True,\n    cache_key: str | None = None,\n    partitioning: str | list[str] | None = None,\n    ddb_con: duckdb.DuckDBPyConnection | None = None,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code> (str): Path to the Parquet dataset - <code>filesystem</code> (AbstractFileSystem | pfs.FileSystem, optional): Filesystem for data access - <code>use_cache</code> (bool): Whether to cache metadata (default: True) - <code>cache_key</code> (str, optional): Key for caching - <code>partitioning</code> (str | list[str], optional): Partitioning scheme - <code>ddb_con</code> (duckdb.DuckDBPyConnection, optional): DuckDB connection. If None, creates a new connection - <code>**kwargs</code>: Additional arguments</p>"},{"location":"api/metadata/#properties_1","title":"Properties","text":""},{"location":"api/metadata/#ddb_con","title":"ddb_con","text":"<p><pre><code>@property\ndef ddb_con(self) -&gt; duckdb.DuckDBPyConnection\n</code></pre> Get the DuckDB connection instance.</p> <p>Returns: - <code>duckdb.DuckDBPyConnection</code>: DuckDB connection</p>"},{"location":"api/metadata/#metadata_table","title":"metadata_table","text":"<p><pre><code>@property\ndef metadata_table(self) -&gt; duckdb.DuckDBPyRelation\n</code></pre> Get the metadata as a DuckDB table.</p> <p>Returns: - <code>duckdb.DuckDBPyRelation</code>: Metadata table relation</p>"},{"location":"api/metadata/#scan_table","title":"scan_table","text":"<p><pre><code>@property\ndef scan_table(self) -&gt; duckdb.DuckDBPyRelation\n</code></pre> Get the scan results as a DuckDB table.</p> <p>Returns: - <code>duckdb.DuckDBPyRelation</code>: Scan results table</p>"},{"location":"api/metadata/#methods_1","title":"Methods","text":""},{"location":"api/metadata/#reset_scan","title":"reset_scan","text":"<p><pre><code>def reset_scan(self) -&gt; None\n</code></pre> Reset scan results.</p> <p>Example: <pre><code># Reset scan to start fresh\nmetadata.reset_scan()\n</code></pre></p>"},{"location":"api/metadata/#scan","title":"scan","text":"<p><pre><code>def scan(\n    self,\n    filters: str | list[str] | dict | None = None,\n    files: list[str] | None = None,\n    columns: list[str] | None = None,\n    verbose: bool = False,\n    return_table: bool = False,\n    **kwargs\n) -&gt; list[str] | pa.Table\n</code></pre> Scan files based on metadata statistics. This is a powerful method for filtering files without reading their contents.</p> <p>Parameters: - <code>filters</code> (str | list[str] | dict, optional): Filter conditions - <code>files</code> (list[str], optional): Specific files to scan - <code>columns</code> (list[str], optional): Columns to consider in filters - <code>verbose</code> (bool): Whether to print verbose output - <code>return_table</code> (bool): Whether to return results as a table - <code>**kwargs</code>: Additional arguments</p> <p>Returns: - <code>list[str] | pa.Table</code>: List of matching files or results table</p> <p>Example: <pre><code># Scan files based on date range\nmatching_files = metadata.scan(\n    filters=\"date &gt;= '2023-01-01' AND date &lt;= '2023-12-31'\",\n    verbose=True\n)\n\n# Scan using dictionary filters\nmatching_files = metadata.scan(\n    filters={\n        'amount': {'min': 100, 'max': 1000},\n        'category': ['A', 'B', 'C']\n    }\n)\n\n# Get results as a table\nresults = metadata.scan(\n    filters=\"status = 'completed'\",\n    return_table=True\n)\n</code></pre></p>"},{"location":"api/metadata/#update_metadata_table","title":"update_metadata_table","text":"<p><pre><code>def update_metadata_table(self, update_parquet_file_metadata: bool = False) -&gt; None\n</code></pre> Update the DuckDB metadata table with current metadata.</p> <p>Parameters: - <code>update_parquet_file_metadata</code> (bool): Whether to update from _file_metadata</p> <p>Example: <pre><code># Update metadata table\nmetadata.update_metadata_table()\n</code></pre></p>"},{"location":"api/metadata/#query_metadata","title":"query_metadata","text":"<p><pre><code>def query_metadata(self, query: str, **kwargs) -&gt; duckdb.DuckDBPyRelation\n</code></pre> Execute SQL queries on the metadata table.</p> <p>Parameters: - <code>query</code> (str): SQL query - <code>**kwargs</code>: Additional arguments</p> <p>Returns: - <code>duckdb.DuckDBPyRelation</code>: Query result</p> <p>Example: <pre><code># Query metadata statistics\nresult = metadata.query_metadata(\"\"\"\n    SELECT column_name, min_value, max_value, null_count\n    FROM metadata_table\n    WHERE column_name IN ('amount', 'date')\n\"\"\")\n</code></pre></p>"},{"location":"api/metadata/#the-_metadata-file","title":"The _metadata File","text":"<p>The <code>_metadata</code> file is a special Parquet file that consolidates metadata from all files in the dataset. It contains: - Schema information unified across all files - Row group metadata from all files - Column statistics (min, max, null counts) - File paths and sizes - Compression information</p> <p>This file enables: - Instant dataset loading: PyArrow's <code>parquet_dataset()</code> function uses this file to create datasets without directory crawling - Efficient query planning: Statistics enable predicate pushdown - File pruning: Skip files that don't contain relevant data - Schema evolution: Handle schema changes across files</p>"},{"location":"api/metadata/#two-tier-metadata-system","title":"Two-Tier Metadata System","text":"<p>PyDala2 maintains two metadata files:</p> <ol> <li><code>_metadata</code>: Consolidated metadata used by PyArrow for efficient dataset loading</li> <li><code>_file_metadata</code>: Detailed individual file metadata stored with brotli compression for advanced operations</li> </ol>"},{"location":"api/metadata/#automatic-metadata-management","title":"Automatic Metadata Management","text":"<pre><code>from pydala import ParquetDataset\n\n# Create dataset - metadata is automatically managed\ndataset = ParquetDataset(\"data/sales\")\n\n# Write data - metadata files are automatically created/updated\ndataset.write_to_dataset(data, partition_cols=['year', 'month'])\n\n# The _metadata file is created automatically\n# PyArrow can now load the dataset instantly\n</code></pre>"},{"location":"api/metadata/#manual-metadata-updates","title":"Manual Metadata Updates","text":"<pre><code># Update metadata when files are added/modified\ndataset.update(verbose=True)\n\n# Update only file metadata\ndataset.update(update_metadata=False)\n\n# Update only consolidated metadata\ndataset.update(update_file_metadata=False)\n</code></pre>"},{"location":"api/metadata/#metadata-operations","title":"Metadata Operations","text":""},{"location":"api/metadata/#metadata-based-file-filtering","title":"Metadata-Based File Filtering","text":"<pre><code># Use the scan method to filter files without reading data\nmatching_files = dataset.scan(\n    filters=\"date &gt;= '2023-01-01' AND amount &gt; 1000\",\n    verbose=True\n)\n\n# Only process files that contain matching data\nfor file_path in matching_files:\n    # These files are guaranteed to contain relevant data\n    process_file(file_path)\n</code></pre>"},{"location":"api/metadata/#sql-queries-on-metadata","title":"SQL Queries on Metadata","text":"<pre><code># Query metadata statistics using SQL\nstats = dataset.query_metadata(\"\"\"\n    SELECT\n        column_name,\n        min_value,\n        max_value,\n        null_count,\n        SUM(num_rows) as total_rows\n    FROM metadata_table\n    WHERE column_name IN ('amount', 'date', 'category')\n    GROUP BY column_name\n\"\"\").to_arrow()\n\nprint(stats)\n</code></pre>"},{"location":"api/metadata/#advanced-filtering-with-duckdb","title":"Advanced Filtering with DuckDB","text":"<pre><code># Use DuckDB for complex metadata queries\nresult = dataset.ddb_con.sql(\"\"\"\n    SELECT file_path, num_rows\n    FROM dataset_metadata\n    WHERE\n        date &gt;= '2023-01-01' AND\n        amount &gt; 1000 AND\n        category IN ('A', 'B', 'C')\n\"\"\").to_arrow()\n</code></pre>"},{"location":"api/metadata/#performance-benefits","title":"Performance Benefits","text":"<p>Using the metadata system provides several performance benefits:</p> <ol> <li>Instant Dataset Loading: The <code>_metadata</code> file enables PyArrow to create datasets without directory scanning</li> <li>File Pruning: The <code>scan()</code> method skips files that don't contain relevant data based on statistics</li> <li>Query Optimization: Statistics enable predicate pushdown to individual files</li> <li>Schema Evolution: Automatic schema repair handles schema differences across files</li> <li>Efficient Storage: Brotli compression for <code>_file_metadata</code> reduces storage overhead</li> <li>SQL Access: DuckDB integration enables SQL queries on metadata</li> </ol>"},{"location":"api/metadata/#example-metadata-enabled-workflow","title":"Example: Metadata-Enabled Workflow","text":"<pre><code>from pydala import ParquetDataset\n\n# Create or load dataset\ndataset = ParquetDataset(\"data/sales\")\n\n# Write data - metadata is automatically created\ndataset.write_to_dataset(data, partition_cols=['year', 'month'])\n\n# Dataset automatically uses _metadata for efficient loading\nprint(f\"Dataset info: {dataset.num_rows} rows in {dataset.num_files} files\")\nprint(f\"Total size: {dataset.size_bytes / 1024 / 1024:.1f} MB\")\n\n# Use metadata-based filtering\n# First, scan to find relevant files\nmatching_files = dataset.scan(\n    filters=\"year = 2023 AND month IN (1, 2, 3) AND amount &gt; 1000\"\n)\n\nprint(f\"Found {len(matching_files)} files with matching data\")\n\n# Then read only relevant data\nresult = dataset.table.to_polars()  # Uses _metadata for fast loading\n</code></pre>"},{"location":"api/metadata/#metadata-serialization","title":"Metadata Serialization","text":"<p>PyDala2 uses efficient serialization for metadata storage:</p> <pre><code># Metadata is automatically serialized/deserialized\n# _file_metadata uses brotli compression\n# _metadata is in PyArrow's native format\n\n# The serialization/deserialization is handled automatically:\nimport brotli\nimport base64\nimport pyarrow as pa\n\n# These functions are used internally:\ndef serialize_metadata(metadata: pa.Table) -&gt; bytes:\n    \"\"\"Serialize metadata table to compressed bytes.\"\"\"\n    return base64.b64encode(\n        brotli.compress(\n            metadata.serialize().to_pybytes()\n        )\n    )\n\ndef deserialize_metadata(data: bytes) -&gt; pa.Table:\n    \"\"\"Deserialize metadata from compressed bytes.\"\"\"\n    return pa.Table.deserialize(\n        brotli.decompress(\n            base64.b64decode(data)\n        )\n    )\n</code></pre>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>This section provides comprehensive API documentation for PyDala2. The API is organized into several main modules:</p>"},{"location":"api/overview/#core-components","title":"Core Components","text":""},{"location":"api/overview/#core-classes","title":"Core Classes","text":"<ul> <li><code>BaseDataset</code> - Base class for all dataset operations</li> <li><code>Optimize</code> - Dataset optimization and compaction operations</li> <li><code>Writer</code> - Data writing and transformation utilities</li> </ul>"},{"location":"api/overview/#dataset-classes","title":"Dataset Classes","text":"<ul> <li><code>ParquetDataset</code> - Parquet file operations with metadata support</li> <li><code>PyarrowDataset</code> - PyArrow dataset integration</li> <li><code>CSVDataset</code> - CSV file operations</li> <li><code>JSONDataset</code> - JSON file operations</li> </ul>"},{"location":"api/overview/#table-operations","title":"Table Operations","text":"<ul> <li><code>PydalaTable</code> - Unified table interface with multiple export formats</li> <li>Data export methods for different backends</li> <li>Scanner and batch reader operations</li> </ul>"},{"location":"api/overview/#catalog-system","title":"Catalog System","text":"<ul> <li><code>Catalog</code> - Centralized dataset management</li> <li>Namespace operations</li> <li>Dataset discovery and registration</li> </ul>"},{"location":"api/overview/#filesystem","title":"Filesystem","text":"<ul> <li><code>FileSystem</code> - Advanced filesystem operations with caching</li> <li>Abstract filesystem implementations</li> <li>Storage backends</li> </ul>"},{"location":"api/overview/#metadata-management","title":"Metadata Management","text":"<ul> <li><code>ParquetDatasetMetadata</code> - Parquet metadata collection and management</li> <li><code>PydalaDatasetMetadata</code> - Extended metadata with statistics</li> <li>Schema unification and repair operations</li> </ul>"},{"location":"api/overview/#utilities","title":"Utilities","text":"<ul> <li>Helper functions and utilities</li> <li>SQL operations</li> <li>Data type conversions</li> <li>Security functions</li> </ul>"},{"location":"api/overview/#quick-api-reference","title":"Quick API Reference","text":""},{"location":"api/overview/#creating-a-dataset","title":"Creating a Dataset","text":"<pre><code>from pydala import ParquetDataset\n\n# Create a new dataset\ndataset = ParquetDataset(\"data/my_dataset\")\n</code></pre>"},{"location":"api/overview/#writing-data","title":"Writing Data","text":"<pre><code># Write data to dataset\ndataset.write_to_dataset(\n    dataframe,\n    partition_cols=['category'],\n    basename_template=\"data-{i}.parquet\"\n)\n</code></pre>"},{"location":"api/overview/#reading-data","title":"Reading Data","text":"<pre><code># Export to different formats\ndf_polars = dataset.table.to_polars(lazy=False)  # Eager Polars DataFrame\ndf_pandas = dataset.table.df  # Pandas DataFrame (shortcut)\ntable_arrow = dataset.table.arrow  # PyArrow Table (shortcut)\n\n# Use shortcut notation\ndf_polars = dataset.t.to_polars()  # Same as dataset.table.to_polars()\n</code></pre>"},{"location":"api/overview/#filtering-data","title":"Filtering Data","text":"<pre><code># Filter with automatic backend selection\nfiltered = dataset.filter(\"date &gt; '2023-01-01'\")\n\n# Export filtered data\nresult = filtered.to_pandas()\n</code></pre>"},{"location":"api/overview/#using-the-catalog","title":"Using the Catalog","text":"<pre><code>from pydala import Catalog\n\n# Create catalog from YAML configuration\ncatalog = Catalog(\"catalog.yaml\")\n\n# Query across datasets\nresult = catalog.ddb_con.sql(\"SELECT * FROM sales_data WHERE value &gt; 100\")\n</code></pre>"},{"location":"api/overview/#data-export-methods","title":"Data Export Methods","text":"<p>PyDala2 provides multiple ways to export data:</p>"},{"location":"api/overview/#to-polars","title":"To Polars","text":"<pre><code># Eager DataFrame\ndf = dataset.table.to_polars(lazy=False)\n\n# LazyFrame (default)\nlf = dataset.table.to_polars(lazy=True)\nlf = dataset.table.pl  # Shortcut property\n\n# Execute LazyFrame\ndf = lf.collect()\n</code></pre>"},{"location":"api/overview/#to-pandas","title":"To Pandas","text":"<pre><code>df = dataset.table.to_pandas()\ndf = dataset.table.df  # Shortcut property\n</code></pre>"},{"location":"api/overview/#to-pyarrow","title":"To PyArrow","text":"<pre><code>table = dataset.table.to_arrow()\ntable = dataset.table.arrow  # Shortcut property\n\n# As dataset\nds = dataset.table.to_arrow_dataset()\n</code></pre>"},{"location":"api/overview/#to-duckdb","title":"To DuckDB","text":"<pre><code>rel = dataset.table.to_duckdb()\nrel = dataset.table.ddb  # Shortcut property\n\n# Execute query\nresult = rel.query(\"SELECT category, AVG(value) FROM rel GROUP BY category\")\n</code></pre>"},{"location":"api/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"api/overview/#dataset-operations","title":"Dataset Operations","text":"<pre><code># Check if dataset exists\nif dataset.exists():\n    print(f\"Dataset has {dataset.count_rows()} rows\")\n\n# Get dataset information\nprint(f\"Schema: {dataset.schema}\")\nprint(f\"Columns: {dataset.columns}\")\nprint(f\"Files: {len(dataset.files)}\")\n</code></pre>"},{"location":"api/overview/#optimization-operations","title":"Optimization Operations","text":"<pre><code># Compact partitions\ndataset.optimize.compact_partitions()\n\n# Compact by time period\ndataset.optimize.compact_by_timeperiod(\n    timestamp_column=\"date\",\n    timeperiod=\"month\"\n)\n\n# Optimize data types\ndataset.optimize.optimize_dtypes()\n</code></pre>"},{"location":"api/overview/#scanner-operations","title":"Scanner Operations","text":"<pre><code># Create scanner for efficient reading\nscanner = dataset.table.scanner()\n\n# Read specific columns\nscanner = dataset.table.to_arrow_scanner(columns=['id', 'name'])\n\n# Read in batches\nbatch_reader = dataset.table.to_batch_reader()\nfor batch in batch_reader:\n    process_batch(batch)\n</code></pre>"},{"location":"api/overview/#error-handling","title":"Error Handling","text":"<p>PyDala2 provides comprehensive error handling:</p> <pre><code>from pydala import PydalaException\n\ntry:\n    result = dataset.filter(\"invalid_column &gt; 100\").to_polars()\nexcept PydalaException as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"api/overview/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use partitioning for large datasets</li> <li>Leverage the <code>_metadata</code> file for efficient dataset loading</li> <li>Use lazy evaluation with Polars for complex operations</li> <li>Apply filters before exporting data to reduce memory usage</li> <li>Use appropriate export format for your use case</li> </ul> <p>For detailed information about each component, see the specific API reference pages.</p>"},{"location":"api/table/","title":"Table Operations","text":"<p>This document describes the <code>PydalaTable</code> class which provides a unified interface for table operations across multiple backends.</p>"},{"location":"api/table/#pydalatable","title":"PydalaTable","text":"<pre><code>class PydalaTable\n</code></pre> <p>The <code>PydalaTable</code> class provides a consistent interface for working with tabular data across different backends (PyArrow and DuckDB). It supports conversion to various formats and provides efficient data access methods.</p>"},{"location":"api/table/#shortcut-notation","title":"Shortcut Notation","text":"<p>PyDala2 provides a convenient shortcut notation using the <code>t</code> property to access the table interface:</p> <pre><code># Instead of writing:\ndf = dataset.table.to_polars()\ntable = dataset.table.to_arrow()\nrel = dataset.table.to_duckdb()\n\n# You can use the shortcut:\ndf = dataset.t.to_polars()\ntable = dataset.t.to_arrow()\nrel = dataset.t.to_duckdb()\n\n# The shortcut also provides direct property access:\nlazy_df = dataset.t.pl  # Polars LazyFrame\ndf_pandas = dataset.t.df  # Pandas DataFrame\narrow_table = dataset.t.arrow  # PyArrow Table\nduckdb_rel = dataset.t.ddb  # DuckDB relation\n</code></pre> <p>The shortcut notation is particularly useful when chaining operations:</p> <pre><code># Filter and export using shortcut\nresult = dataset.filter(\"amount &gt; 100\").t.to_pandas()\n\n# Complex operations with shortcut\nresult = (\n    dataset.t.pl\n    .filter(pl.col(\"date\") &gt; \"2023-01-01\")\n    .group_by(\"category\")\n    .agg(pl.mean(\"amount\"))\n    .collect()\n)\n</code></pre>"},{"location":"api/table/#constructor","title":"Constructor","text":"<pre><code>PydalaTable(\n    result: pds.Dataset | duckdb.DuckDBPyRelation,\n    ddb_con: duckdb.DuckDBPyConnection | None = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>result</code>: The data source - either a PyArrow dataset or DuckDB relation - <code>ddb_con</code>: Existing DuckDB connection. If None, creates a new one</p> <p>Example: <pre><code>from pydala import PydalaTable, ParquetDataset\nimport pyarrow.dataset as pds\n\n# From PyArrow dataset\ndataset = pds.dataset(\"data/sales\")\ntable = PydalaTable(dataset)\n\n# From ParquetDataset\nds = ParquetDataset(\"data/sales\")\ntable = PydalaTable(ds._arrow_dataset)\n</code></pre></p>"},{"location":"api/table/#conversion-methods","title":"Conversion Methods","text":""},{"location":"api/table/#to_arrow_table","title":"to_arrow_table","text":"<p><pre><code>def to_arrow_table(\n    self,\n    columns: str | list[str] | None = None,\n    filter: pds.Expression | None = None,\n    batch_size: int = 131072,\n    sort_by: str | list[str] | list[tuple[str, str]] | None = None,\n    distinct: bool = False,\n    batch_readahead: int = 16,\n    fragment_readahead: int = 4,\n    fragment_scan_options: pds.FragmentScanOptions | None = None,\n    use_threads: bool = True,\n    memory_pool: pa.MemoryPool | None = None\n) -&gt; pa.Table\n</code></pre> Convert the table to a PyArrow Table.</p> <p>Parameters: - <code>columns</code> (str | list[str]): Columns to include - <code>filter</code> (pds.Expression): Filter expression to apply - <code>batch_size</code> (int): Batch size for reading - <code>sort_by</code> (str | list[str]): Column(s) to sort by - <code>distinct</code> (bool): Whether to return distinct rows - <code>batch_readahead</code> (int): Number of batches to read ahead - <code>fragment_readahead</code> (int): Number of fragments to read ahead - <code>fragment_scan_options</code> (pds.FragmentScanOptions): Scan options - <code>use_threads</code> (bool): Whether to use threading - <code>memory_pool</code> (pa.MemoryPool): Memory pool for allocation</p> <p>Returns: - <code>pa.Table</code>: The table as a PyArrow Table</p> <p>Example: <pre><code># Convert to Arrow table\ntable = dataset.table.to_arrow_table()\n\n# With filters\nimport pyarrow.dataset as pds\nfilter_expr = (pds.field('amount') &gt; 100) &amp; (pds.field('category') == 'premium')\ntable = dataset.table.to_arrow_table(filter=filter_expr)\n\n# With sorting\ntable = dataset.table.to_arrow_table(\n    sort_by=['date', 'amount DESC'],\n    columns=['id', 'date', 'amount']\n)\n</code></pre></p>"},{"location":"api/table/#to_polars","title":"to_polars","text":"<p><pre><code>def to_polars(\n    self,\n    lazy: bool = True,\n    columns: str | list[str] | None = None,\n    sort_by: str | list[str] | None = None,\n    distinct: bool = False,\n    batch_size: int = 131072,\n    **kwargs\n) -&gt; pl.DataFrame | pl.LazyFrame\n</code></pre> Convert the table to a Polars DataFrame or LazyFrame.</p> <p>Parameters: - <code>lazy</code> (bool): If True, returns a LazyFrame; if False, returns a DataFrame - <code>columns</code> (str | list[str]): Columns to include - <code>sort_by</code> (str | list[str]): Column(s) to sort by - <code>distinct</code> (bool): Whether to return distinct rows - <code>batch_size</code> (int): Batch size for scanning - <code>**kwargs</code>: Additional arguments</p> <p>Returns: - <code>pl.DataFrame | pl.LazyFrame</code>: Polars DataFrame or LazyFrame</p> <p>Example: <pre><code>import polars as pl\n\n# Get LazyFrame for lazy evaluation\nlazy_df = dataset.table.to_polars(lazy=True)\n\n# Perform operations\nresult = (\n    lazy_df\n    .filter(pl.col(\"amount\") &gt; 100)\n    .group_by(\"category\")\n    .agg([\n        pl.count(\"id\").alias(\"count\"),\n        pl.mean(\"amount\").alias(\"avg_amount\")\n    ])\n    .collect()\n)\n\n# Get DataFrame directly\ndf = dataset.table.to_polars(lazy=False)\n</code></pre></p>"},{"location":"api/table/#to_pandas","title":"to_pandas","text":"<p><pre><code>def to_pandas(\n    self,\n    columns: str | list[str] | None = None,\n    sort_by: str | list[str] | None = None,\n    distinct: bool = False,\n    **kwargs\n) -&gt; pd.DataFrame\n</code></pre> Convert the table to a pandas DataFrame.</p> <p>Parameters: - <code>columns</code> (str | list[str]): Columns to include - <code>sort_by</code> (str | list[str]): Column(s) to sort by - <code>distinct</code> (bool): Whether to return distinct rows - <code>**kwargs</code>: Additional arguments</p> <p>Returns: - <code>pd.DataFrame</code>: The table as a pandas DataFrame</p> <p>Example: <pre><code># Convert to pandas DataFrame\ndf = dataset.table.to_pandas()\n\n# With column selection\ndf = dataset.table.to_pandas(columns=['id', 'name', 'amount'])\n\n# With sorting\ndf = dataset.table.to_pandas(\n    sort_by='date DESC',\n    distinct=False\n)\n</code></pre></p>"},{"location":"api/table/#to_duckdb","title":"to_duckdb","text":"<p><pre><code>def to_duckdb(\n    self,\n    lazy: bool = True,\n    columns: str | list[str] | None = None,\n    batch_size: int = 131072,\n    sort_by: str | list[str] | list[tuple[str, str]] | None = None,\n    distinct: bool = False,\n    **kwargs\n) -&gt; duckdb.DuckDBPyRelation\n</code></pre> Convert the table to a DuckDB relation.</p> <p>Parameters: - <code>lazy</code> (bool): If True, returns a lazy relation; if False, materializes - <code>columns</code> (str | list[str]): Columns to include - <code>batch_size</code> (int): Batch size for scanning - <code>sort_by</code> (str | list[str]): Column(s) to sort by - <code>distinct</code> (bool): Whether to return distinct rows - <code>**kwargs</code>: Additional arguments</p> <p>Returns: - <code>duckdb.DuckDBPyRelation</code>: DuckDB relation</p> <p>Example: <pre><code># Get DuckDB relation\nrel = dataset.table.to_duckdb()\n\n# Use SQL\nresult = rel.execute(\"SELECT category, AVG(amount) FROM rel GROUP BY category\").df()\n\n# Use relation API\nresult = rel.filter(\"amount &gt; 100\").project(\"category, amount\").df()\n</code></pre></p>"},{"location":"api/table/#to_arrow_scanner","title":"to_arrow_scanner","text":"<p><pre><code>def to_arrow_scanner(\n    self,\n    columns: str | list[str] | None = None,\n    filter: pds.Expression | None = None,\n    batch_size: int | None = None,\n    sort_by: str | list[str] | list[tuple[str, str]] | None = None,\n    batch_readahead: int | None = None,\n    fragment_readahead: int | None = None,\n    fragment_scan_options: pds.FragmentScanOptions | None = None,\n    use_threads: bool | None = None,\n    memory_pool: pa.MemoryPool | None = None\n) -&gt; pds.Scanner\n</code></pre> Convert the table to an Arrow scanner for efficient data scanning.</p> <p>Returns: - <code>pds.Scanner</code>: Arrow scanner object</p> <p>Example: <pre><code># Create scanner\nscanner = dataset.table.to_arrow_scanner(\n    columns=['id', 'name', 'amount'],\n    filter=pds.field('amount') &gt; 100\n)\n\n# Read in batches\nfor batch in scanner.to_batches():\n    process_batch(batch)\n</code></pre></p>"},{"location":"api/table/#to_batch_reader","title":"to_batch_reader","text":"<p><pre><code>def to_batch_reader(\n    self,\n    columns: str | list[str] | None = None,\n    filter: pds.Expression | None = None,\n    lazy: bool = True,\n    batch_size: int = 131072,\n    sort_by: str | list[str] | list[tuple[str, str]] | None = None,\n    distinct: bool = False,\n    batch_readahead: int = 16,\n    fragment_readahead: int = 4,\n    fragment_scan_options: pds.FragmentScanOptions | None = None,\n    use_threads: bool = True,\n    memory_pool: pa.MemoryPool | None = None\n) -&gt; pa.RecordBatchReader\n</code></pre> Convert the table to a batch reader for streaming operations.</p> <p>Returns: - <code>pa.RecordBatchReader</code>: RecordBatch reader</p> <p>Example: <pre><code># Stream data in batches\nreader = dataset.table.to_batch_reader(\n    batch_size=10000,\n    columns=['id', 'name', 'amount']\n)\n\nfor batch in reader:\n    process_batch(batch)\n</code></pre></p>"},{"location":"api/table/#properties","title":"Properties","text":""},{"location":"api/table/#arrow","title":"arrow","text":"<p><pre><code>@property\ndef arrow(self) -&gt; pa.Table\n</code></pre> Get the table as a PyArrow Table (shortcut property).</p> <p>Returns: - <code>pa.Table</code>: PyArrow Table</p> <p>Example: <pre><code># Direct access to Arrow table\ntable = dataset.table.arrow\n</code></pre></p>"},{"location":"api/table/#pl","title":"pl","text":"<p><pre><code>@property\ndef pl(self) -&gt; pl.LazyFrame\n</code></pre> Get the table as a Polars LazyFrame (shortcut property).</p> <p>Returns: - <code>pl.LazyFrame</code>: Polars LazyFrame</p> <p>Example: <pre><code># Direct access to Polars LazyFrame\nlazy_df = dataset.table.pl\nresult = lazy_df.filter(pl.col(\"amount\") &gt; 100).collect()\n</code></pre></p>"},{"location":"api/table/#df","title":"df","text":"<p><pre><code>@property\ndef df(self) -&gt; pd.DataFrame\n</code></pre> Get the table as a pandas DataFrame (shortcut property).</p> <p>Returns: - <code>pd.DataFrame</code>: Pandas DataFrame</p> <p>Example: <pre><code># Direct access to pandas DataFrame\ndf = dataset.table.df\n</code></pre></p>"},{"location":"api/table/#ddb","title":"ddb","text":"<p><pre><code>@property\ndef ddb(self) -&gt; duckdb.DuckDBPyRelation\n</code></pre> Get the table as a DuckDB relation (shortcut property).</p> <p>Returns: - <code>duckdb.DuckDBPyRelation</code>: DuckDB relation</p> <p>Example: <pre><code># Direct access to DuckDB relation\nrel = dataset.table.ddb\nresult = rel.sql(\"SELECT COUNT(*) FROM rel\").df()\n</code></pre></p>"},{"location":"api/table/#arrow_dataset","title":"arrow_dataset","text":"<p><pre><code>@property\ndef arrow_dataset(self) -&gt; pds.Dataset\n</code></pre> Get the underlying PyArrow dataset.</p> <p>Returns: - <code>pds.Dataset</code>: PyArrow dataset</p>"},{"location":"api/table/#duckdb_relation","title":"duckdb_relation","text":"<p><pre><code>@property\ndef duckdb_relation(self) -&gt; duckdb.DuckDBPyRelation\n</code></pre> Get the DuckDB relation.</p> <p>Returns: - <code>duckdb.DuckDBPyRelation</code>: DuckDB relation</p>"},{"location":"api/table/#backend-selection","title":"Backend Selection","text":""},{"location":"api/table/#choosing-the-right-backend","title":"Choosing the Right Backend","text":"<pre><code># Use PyArrow for columnar operations\ntable = dataset.table\narrow_table = table.to_arrow_table()\n\n# Use Polars for high-performance transformations\nlazy_df = table.to_polars(lazy=True)\nresult = lazy_df.filter(...).group_by(...).collect()\n\n# Use DuckDB for SQL operations\nrel = table.to_duckdb()\nsql_result = rel.execute(\"SELECT ...\").df()\n\n# Use Pandas for compatibility\ndf = table.to_pandas()\n</code></pre>"},{"location":"api/table/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>PyArrow: Best for columnar operations and Arrow ecosystem integration</li> <li>Polars: Fastest for complex transformations and aggregations</li> <li>DuckDB: Ideal for SQL queries and joins</li> <li>Pandas: Best for compatibility with existing code</li> </ul>"},{"location":"api/table/#memory-management","title":"Memory Management","text":"<pre><code># Use batch readers for large datasets\nreader = table.to_batch_reader(batch_size=10000)\nfor batch in reader:\n    # Process each batch\n    pass\n\n# Use lazy evaluation with Polars\nlazy_df = table.to_polars(lazy=True)\n# Operations are optimized before execution\nresult = lazy_df.filter(...).collect()\n\n# Use DuckDB for out-of-core operations\nrel = table.to_duckdb()\n# DuckDB handles data larger than memory\n</code></pre>"},{"location":"api/table/#common-operations","title":"Common Operations","text":""},{"location":"api/table/#filtering","title":"Filtering","text":"<pre><code># Arrow filtering\nimport pyarrow.dataset as pds\nfilter_expr = (pds.field('amount') &gt; 100) &amp; (pds.field('category') == 'premium')\nresult = table.to_arrow_table(filter=filter_expr)\n\n# Polars filtering\nlazy_df = table.to_polars(lazy=True)\nresult = lazy_df.filter(\n    (pl.col('amount') &gt; 100) &amp; (pl.col('category') == 'premium')\n).collect()\n\n# DuckDB filtering\nrel = table.to_duckdb()\nresult = rel.filter(\"amount &gt; 100 AND category = 'premium'\").df()\n</code></pre>"},{"location":"api/table/#aggregation","title":"Aggregation","text":"<pre><code># Polars aggregation\nlazy_df = table.to_polars(lazy=True)\nresult = (\n    lazy_df\n    .group_by('category')\n    .agg([\n        pl.count('id').alias('count'),\n        pl.sum('amount').alias('total'),\n        pl.mean('amount').alias('average')\n    ])\n    .sort('total', descending=True)\n    .collect()\n)\n\n# DuckDB aggregation\nrel = table.to_duckdb()\nresult = rel.execute(\"\"\"\n    SELECT\n        category,\n        COUNT(*) as count,\n        SUM(amount) as total,\n        AVG(amount) as average\n    FROM table\n    GROUP BY category\n    ORDER BY total DESC\n\"\"\").df()\n</code></pre>"},{"location":"api/table/#joins","title":"Joins","text":"<pre><code># DuckDB join (most efficient)\nrel1 = table1.to_duckdb()\nrel2 = table2.to_duckdb()\nresult = rel1.join(rel2, 'id').df()\n\n# Polars join\ndf1 = table1.to_polars()\ndf2 = table2.to_polars()\nresult = df1.join(df2, on='id')\n</code></pre>"},{"location":"api/table/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/table/#custom-arrow-expressions","title":"Custom Arrow Expressions","text":"<pre><code># Complex Arrow expressions\nimport pyarrow.compute as pc\n\nfilter_expr = (\n    (pc.field('date') &gt;= pc.scalar('2023-01-01')) &amp;\n    (pc.field('amount') &gt; pc.scalar(100)) &amp;\n    (pc.is_in(pc.field('category'), pc.scalar(['A', 'B', 'C'])))\n)\n\nresult = table.to_arrow_table(filter=filter_expr)\n</code></pre>"},{"location":"api/table/#sql-with-duckdb","title":"SQL with DuckDB","text":"<pre><code># Complex SQL queries\nrel = table.to_duckdb()\nresult = rel.execute(\"\"\"\n    WITH monthly_stats AS (\n        SELECT\n            DATE_TRUNC('month', date) as month,\n            category,\n            COUNT(*) as orders,\n            SUM(amount) as revenue\n        FROM table\n        WHERE date &gt;= '2023-01-01'\n        GROUP BY month, category\n    )\n    SELECT\n        month,\n        category,\n        orders,\n        revenue,\n        revenue / orders as avg_order_value,\n        LAG(revenue) OVER (PARTITION BY category ORDER BY month) as prev_month_revenue\n    FROM monthly_stats\n    ORDER BY month, category\n\"\"\").df()\n</code></pre>"},{"location":"api/table/#lazy-evaluation-with-polars","title":"Lazy Evaluation with Polars","text":"<pre><code># Build complex query plan\nquery = (\n    table.to_polars(lazy=True)\n    .filter(pl.col('date') &gt;= '2023-01-01')\n    .group_by(['category', pl.col('date').dt.month()])\n    .agg([\n        pl.count('id').alias('orders'),\n        pl.sum('amount').alias('revenue')\n    ])\n    .filter(pl.col('orders') &gt; 10)\n    .sort('revenue', descending=True)\n)\n\n# Execute once\nresult = query.collect()\n</code></pre>"},{"location":"api/utilities/","title":"Utilities","text":"<p>This document describes utility functions and helper classes available in PyDala2.</p>"},{"location":"api/utilities/#configuration-utilities","title":"Configuration Utilities","text":""},{"location":"api/utilities/#set_config","title":"set_config","text":"<p><pre><code>def set_config(config: dict) -&gt; None\n</code></pre> Set global configuration values.</p> <p>Parameters: - <code>config</code> (dict): Configuration dictionary</p> <p>Example: <pre><code>from pydala import set_config\n\nset_config({\n    'default_backend': 'polars',\n    'cache_enabled': True,\n    'log_level': 'INFO'\n})\n</code></pre></p>"},{"location":"api/utilities/#get_config","title":"get_config","text":"<p><pre><code>def get_config(key: str, default=None)\n</code></pre> Get a configuration value.</p> <p>Parameters: - <code>key</code> (str): Configuration key - <code>default</code>: Default value if key not found</p> <p>Returns: - Configuration value or default</p> <p>Example: <pre><code>from pydala import get_config\n\nbackend = get_config('default_backend', 'polars')\n</code></pre></p>"},{"location":"api/utilities/#config_context","title":"config_context","text":"<p><pre><code>@contextmanager\ndef config_context(config: dict)\n</code></pre> Context manager for temporary configuration changes.</p> <p>Parameters: - <code>config</code> (dict): Configuration to apply within context</p> <p>Example: <pre><code>from pydala import config_context\n\nwith config_context({'default_backend': 'duckdb'}):\n    # Operations here use DuckDB\n    result = dataset.read()\n# Back to previous configuration\n</code></pre></p>"},{"location":"api/utilities/#schema-utilities","title":"Schema Utilities","text":""},{"location":"api/utilities/#infer_schema","title":"infer_schema","text":"<p><pre><code>def infer_schema(data) -&gt; pa.Schema\n</code></pre> Infer Arrow schema from data.</p> <p>Parameters: - <code>data</code>: Data to infer schema from (DataFrame, list of dicts, etc.)</p> <p>Returns: - <code>pa.Schema</code>: Inferred schema</p> <p>Example: <pre><code>from pydala import infer_schema\n\n# From pandas DataFrame\nschema = infer_schema(df)\n\n# From list of dicts\ndata = [{'id': 1, 'name': 'Alice'}, {'id': 2, 'name': 'Bob'}]\nschema = infer_schema(data)\n</code></pre></p>"},{"location":"api/utilities/#convert_schema","title":"convert_schema","text":"<p><pre><code>def convert_schema(schema: pa.Schema, conversions: dict) -&gt; pa.Schema\n</code></pre> Convert schema data types.</p> <p>Parameters: - <code>schema</code> (pa.Schema): Original schema - <code>conversions</code> (dict): Column to type mapping</p> <p>Returns: - <code>pa.Schema</code>: Converted schema</p> <p>Example: <pre><code>from pydala import convert_schema\nimport pyarrow as pa\n\nconversions = {\n    'id': pa.int64(),\n    'price': pa.decimal128(10, 2),\n    'timestamp': pa.timestamp('ns')\n}\n\nnew_schema = convert_schema(old_schema, conversions)\n</code></pre></p>"},{"location":"api/utilities/#validate_schema","title":"validate_schema","text":"<p><pre><code>def validate_schema(data: pa.Table, schema: pa.Schema) -&gt; ValidationResult\n</code></pre> Validate data against schema.</p> <p>Parameters: - <code>data</code> (pa.Table): Data to validate - <code>schema</code> (pa.Schema): Expected schema</p> <p>Returns: - <code>ValidationResult</code>: Validation result object</p> <p>Example: <pre><code>from pydala import validate_schema\n\nresult = validate_schema(table, expected_schema)\nif not result.valid:\n    for error in result.errors:\n        print(f\"Schema error: {error}\")\n</code></pre></p>"},{"location":"api/utilities/#data-type-utilities","title":"Data Type Utilities","text":""},{"location":"api/utilities/#optimize_dtypes","title":"optimize_dtypes","text":"<p><pre><code>def optimize_dtypes(table: pa.Table, backend: str = \"polars\") -&gt; pa.Table\n</code></pre> Optimize data types to reduce memory usage.</p> <p>Parameters: - <code>table</code> (pa.Table): Input table - <code>backend</code> (str): Backend to use for optimization</p> <p>Returns: - <code>pa.Table</code>: Table with optimized types</p> <p>Example: <pre><code>from pydala import optimize_dtypes\n\noptimized = optimize_dtypes(table)\nprint(f\"Memory reduced: {table.nbytes - optimized.nbytes} bytes\")\n</code></pre></p>"},{"location":"api/utilities/#convert_large_types","title":"convert_large_types","text":"<p><pre><code>def convert_large_types(table: pa.Table) -&gt; pa.Table\n</code></pre> Convert large Arrow types to standard types.</p> <p>Parameters: - <code>table</code> (pa.Table): Input table</p> <p>Returns: - <code>pa.Table</code>: Table with standard types</p> <p>Example: <pre><code>from pydala import convert_large_types\n\n# Convert large_string to string, large_list to list\nstandard_table = convert_large_types(large_table)\n</code></pre></p>"},{"location":"api/utilities/#sql-utilities","title":"SQL Utilities","text":""},{"location":"api/utilities/#sql2pyarrow_filter","title":"sql2pyarrow_filter","text":"<p><pre><code>def sql2pyarrow_filter(sql_filter: str) -&gt; pds.Expression\n</code></pre> Convert SQL WHERE clause to PyArrow filter expression.</p> <p>Parameters: - <code>sql_filter</code> (str): SQL filter expression</p> <p>Returns: - <code>pds.Expression</code>: PyArrow filter expression</p> <p>Example: <pre><code>from pydala import sql2pyarrow_filter\n\nfilter_expr = sql2pyarrow_filter(\"date &gt; '2023-01-01' AND amount &gt; 100\")\nresult = dataset.read(filter=filter_expr)\n</code></pre></p>"},{"location":"api/utilities/#escape_sql_identifier","title":"escape_sql_identifier","text":"<p><pre><code>def escape_sql_identifier(identifier: str) -&gt; str\n</code></pre> Escape SQL identifiers to prevent injection.</p> <p>Parameters: - <code>identifier</code> (str): SQL identifier</p> <p>Returns: - <code>str</code>: Escaped identifier</p> <p>Example: <pre><code>from pydala import escape_sql_identifier\n\nsafe_name = escape_sql_identifier(column_name)\nsql = f\"SELECT {safe_name} FROM table\"\n</code></pre></p>"},{"location":"api/utilities/#escape_sql_literal","title":"escape_sql_literal","text":"<p><pre><code>def escape_sql_literal(value) -&gt; str\n</code></pre> Escape SQL literal values.</p> <p>Parameters: - <code>value</code>: Value to escape</p> <p>Returns: - <code>str</code>: Escaped SQL literal</p> <p>Example: <pre><code>from pydala import escape_sql_literal\n\nsafe_value = escape_sql_literal(user_input)\nsql = f\"SELECT * FROM table WHERE name = {safe_value}\"\n</code></pre></p>"},{"location":"api/utilities/#time-utilities","title":"Time Utilities","text":""},{"location":"api/utilities/#get_timestamp_column","title":"get_timestamp_column","text":"<p><pre><code>def get_timestamp_column(df: pd.DataFrame | pl.DataFrame) -&gt; list[str]\n</code></pre> Identify timestamp columns in a DataFrame.</p> <p>Parameters: - <code>df</code>: DataFrame to analyze</p> <p>Returns: - <code>list[str]</code>: Names of timestamp columns</p> <p>Example: <pre><code>from pydala import get_timestamp_column\n\ntimestamp_cols = get_timestamp_column(df)\nprint(f\"Found timestamp columns: {timestamp_cols}\")\n</code></pre></p>"},{"location":"api/utilities/#parse_timestamp_string","title":"parse_timestamp_string","text":"<p><pre><code>def parse_timestamp_string(ts_str: str, fmt: str = None) -&gt; pd.Timestamp\n</code></pre> Parse timestamp string with automatic format detection.</p> <p>Parameters: - <code>ts_str</code> (str): Timestamp string - <code>fmt</code> (str, optional): Expected format</p> <p>Returns: - <code>pd.Timestamp</code>: Parsed timestamp</p> <p>Example: <pre><code>from pydala import parse_timestamp_string\n\n# Auto-detect format\nts = parse_timestamp_string(\"2023-12-01 14:30:00\")\n\n# Specify format\nts = parse_timestamp_string(\"01/12/2023\", fmt=\"%d/%m/%Y\")\n</code></pre></p>"},{"location":"api/utilities/#compression-utilities","title":"Compression Utilities","text":""},{"location":"api/utilities/#get_compression_extension","title":"get_compression_extension","text":"<p><pre><code>def get_compression_extension(compression: str) -&gt; str\n</code></pre> Get file extension for compression type.</p> <p>Parameters: - <code>compression</code> (str): Compression algorithm</p> <p>Returns: - <code>str</code>: File extension</p> <p>Example: <pre><code>from pydala import get_compression_extension\n\next = get_compression_extension('gzip')  # Returns '.gz'\next = get_compression_extension('zstd')  # Returns '.zst'\n</code></pre></p>"},{"location":"api/utilities/#compress_data","title":"compress_data","text":"<p><pre><code>def compress_data(data: bytes, compression: str, level: int = None) -&gt; bytes\n</code></pre> Compress data with specified algorithm.</p> <p>Parameters: - <code>data</code> (bytes): Data to compress - <code>compression</code> (str): Compression algorithm - <code>level</code> (int, optional): Compression level</p> <p>Returns: - <code>bytes</code>: Compressed data</p> <p>Example: <pre><code>from pydala import compress_data\n\ncompressed = compress_data(raw_data, 'zstd', level=3)\n</code></pre></p>"},{"location":"api/utilities/#decompress_data","title":"decompress_data","text":"<p><pre><code>def decompress_data(data: bytes, compression: str) -&gt; bytes\n</code></pre> Decompress data.</p> <p>Parameters: - <code>data</code> (bytes): Compressed data - <code>compression</code> (str): Compression algorithm</p> <p>Returns: - <code>bytes</code>: Decompressed data</p> <p>Example: <pre><code>from pydala import decompress_data\n\ndecompressed = decompress_data(compressed_data, 'zstd')\n</code></pre></p>"},{"location":"api/utilities/#security-utilities","title":"Security Utilities","text":""},{"location":"api/utilities/#validate_path","title":"validate_path","text":"<p><pre><code>def validate_path(path: str) -&gt; bool\n</code></pre> Validate filesystem path for security.</p> <p>Parameters: - <code>path</code> (str): Path to validate</p> <p>Returns: - <code>bool</code>: True if path is safe</p> <p>Example: <pre><code>from pydala import validate_path\n\nif validate_path(user_path):\n    # Path is safe to use\n    fs.open(user_path)\n</code></pre></p>"},{"location":"api/utilities/#sanitize_filter_expression","title":"sanitize_filter_expression","text":"<p><pre><code>def sanitize_filter_expression(expr: str) -&gt; str\n</code></pre> Sanitize filter expressions to prevent injection.</p> <p>Parameters: - <code>expr</code> (str): Filter expression</p> <p>Returns: - <code>str</code>: Sanitized expression</p> <p>Example: <pre><code>from pydala import sanitize_filter_expression\n\nsafe_expr = sanitize_filter_expression(user_filter)\nresult = dataset.read(filters=safe_expr)\n</code></pre></p>"},{"location":"api/utilities/#validate_partition_name","title":"validate_partition_name","text":"<p><pre><code>def validate_partition_name(name: str) -&gt; bool\n</code></pre> Validate partition column names.</p> <p>Parameters: - <code>name</code> (str): Partition name</p> <p>Returns: - <code>bool</code>: True if valid</p> <p>Example: <pre><code>from pydala import validate_partition_name\n\nif validate_partition_name(partition_col):\n    # Safe to use as partition column\n    dataset.write(data, partition_cols=[partition_col])\n</code></pre></p>"},{"location":"api/utilities/#performance-utilities","title":"Performance Utilities","text":""},{"location":"api/utilities/#timer","title":"Timer","text":"<p><pre><code>class Timer\n</code></pre> Context manager for timing operations.</p> <p>Example: <pre><code>from pydala import Timer\n\nwith Timer() as t:\n    # Perform operation\n    dataset.read()\n\nprint(f\"Operation took {t.elapsed:.2f} seconds\")\n</code></pre></p>"},{"location":"api/utilities/#memory-usage","title":"Memory Usage","text":"<p><pre><code>def get_memory_usage() -&gt; dict\n</code></pre> Get current memory usage statistics.</p> <p>Returns: - <code>dict</code>: Memory usage information</p> <p>Example: <pre><code>from pydala import get_memory_usage\n\nmem = get_memory_usage()\nprint(f\"Memory used: {mem['used'] / 1024**3:.1f} GB\")\nprint(f\"Memory available: {mem['available'] / 1024**3:.1f} GB\")\n</code></pre></p>"},{"location":"api/utilities/#progressbar","title":"ProgressBar","text":"<p><pre><code>class ProgressBar\n</code></pre> Progress bar for long-running operations.</p> <p>Example: <pre><code>from pydala import ProgressBar\n\nwith ProgressBar(total=len(files)) as pbar:\n    for file in files:\n        process_file(file)\n        pbar.update(1)\n</code></pre></p>"},{"location":"api/utilities/#logging-utilities","title":"Logging Utilities","text":""},{"location":"api/utilities/#setup_logger","title":"setup_logger","text":"<p><pre><code>def setup_logger(\n    name: str,\n    level: str = \"INFO\",\n    format: str = None,\n    file: str = None\n) -&gt; logging.Logger\n</code></pre> Set up a logger with custom configuration.</p> <p>Parameters: - <code>name</code> (str): Logger name - <code>level</code> (str): Log level - <code>format</code> (str, optional): Log format - <code>file</code> (str, optional): Log file path</p> <p>Returns: - <code>logging.Logger</code>: Configured logger</p> <p>Example: <pre><code>from pydala import setup_logger\n\nlogger = setup_logger(\n    \"my_app\",\n    level=\"DEBUG\",\n    file=\"app.log\"\n)\nlogger.info(\"Application started\")\n</code></pre></p>"},{"location":"api/utilities/#exception-classes","title":"Exception Classes","text":""},{"location":"api/utilities/#pydalaexception","title":"PydalaException","text":"<p><pre><code>class PydalaException(Exception)\n</code></pre> Base exception class for PyDala2.</p>"},{"location":"api/utilities/#validationerror","title":"ValidationError","text":"<p><pre><code>class ValidationError(PydalaException)\n</code></pre> Raised when data validation fails.</p>"},{"location":"api/utilities/#filesystemerror","title":"FileSystemError","text":"<p><pre><code>class FileSystemError(PydalaException)\n</code></pre> Raised for filesystem-related errors.</p>"},{"location":"api/utilities/#catalogerror","title":"CatalogError","text":"<p><pre><code>class CatalogError(PydalaException)\n</code></pre> Raised for catalog-related errors.</p>"},{"location":"api/utilities/#schemaerror","title":"SchemaError","text":"<p><pre><code>class SchemaError(PydalaException)\n</code></pre> Raised for schema-related errors.</p> <p>Example: <pre><code>from pydala import PydalaException, ValidationError\n\ntry:\n    dataset.write(data)\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\nexcept PydalaException as e:\n    print(f\"PyDala2 error: {e}\")\n</code></pre></p>"},{"location":"api/utilities/#helper-functions","title":"Helper Functions","text":""},{"location":"api/utilities/#safe_join","title":"safe_join","text":"<p><pre><code>def safe_join(base_path: str, *paths) -&gt; str\n</code></pre> Safely join filesystem paths.</p> <p>Parameters: - <code>base_path</code> (str): Base path - <code>*paths</code>: Additional path components</p> <p>Returns: - <code>str</code>: Joined path</p> <p>Example: <pre><code>from pydala import safe_join\n\npath = safe_join(\"/data\", \"sales\", \"2023\", \"file.parquet\")\n</code></pre></p>"},{"location":"api/utilities/#generate_unique_name","title":"generate_unique_name","text":"<p><pre><code>def generate_unique_name(prefix: str = \"pydala_\") -&gt; str\n</code></pre> Generate a unique name for temporary objects.</p> <p>Parameters: - <code>prefix</code> (str): Name prefix</p> <p>Returns: - <code>str</code>: Unique name</p> <p>Example: <pre><code>from pydala import generate_unique_name\n\ntemp_name = generate_unique_name(\"temp_\")\nprint(temp_name)  # e.g., \"temp_abc123\"\n</code></pre></p>"},{"location":"api/utilities/#format_bytes","title":"format_bytes","text":"<p><pre><code>def format_bytes(bytes: int) -&gt; str\n</code></pre> Format bytes as human-readable string.</p> <p>Parameters: - <code>bytes</code> (int): Number of bytes</p> <p>Returns: - <code>str</code>: Formatted string</p> <p>Example: <pre><code>from pydala import format_bytes\n\nprint(format_bytes(1024))        # \"1.0 KB\"\nprint(format_bytes(1048576))      # \"1.0 MB\"\nprint(format_bytes(1073741824))   # \"1.0 GB\"\n</code></pre></p>"},{"location":"api/utilities/#format_duration","title":"format_duration","text":"<p><pre><code>def format_duration(seconds: float) -&gt; str\n</code></pre> Format duration as human-readable string.</p> <p>Parameters: - <code>seconds</code> (float): Duration in seconds</p> <p>Returns: - <code>str</code>: Formatted string</p> <p>Example: <pre><code>from pydala import format_duration\n\nprint(format_duration(45))        # \"45.0s\"\nprint(format_duration(3661))      # \"1h 1m 1s\"\nprint(format_duration(90061))     # \"1d 1h 1m 1s\"\n</code></pre></p>"},{"location":"user-guide/basic-usage/","title":"Basic Usage","text":"<p>This guide covers the fundamental operations you'll perform with PyDala2 datasets.</p>"},{"location":"user-guide/basic-usage/#creating-datasets","title":"Creating Datasets","text":""},{"location":"user-guide/basic-usage/#basic-dataset-creation","title":"Basic Dataset Creation","text":"<pre><code>from pydala import ParquetDataset\n\n# Create a new dataset (directory auto-created)\ndataset = ParquetDataset(\"data/my_dataset\")\n\n# The dataset is now ready with:\n# - Automatic DuckDB connection\n# - Object caching enabled\n# - Partitioning inference\n</code></pre>"},{"location":"user-guide/basic-usage/#dataset-with-configuration","title":"Dataset with Configuration","text":"<pre><code># Configure dataset options\ndataset = ParquetDataset(\n    \"data/configured_dataset\",\n    name=\"sales_data\",           # Custom name (defaults to directory name)\n    partitioning=\"hive\",         # Auto-detect hive partitioning\n    cached=True,                 # Enable filesystem caching\n    timestamp_column=\"created_at\" # For time-based operations\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#cloud-storage-dataset","title":"Cloud Storage Dataset","text":"<pre><code># S3 dataset\ns3_dataset = ParquetDataset(\n    \"s3://my-bucket/sales-data\",\n    bucket=\"my-bucket\",\n    key=\"your-access-key\",\n    secret=\"your-secret-key\",\n    cached=True,\n    cache_storage=\"/tmp/s3_cache\"\n)\n\n# The dataset handles S3 authentication and caching automatically\n</code></pre>"},{"location":"user-guide/basic-usage/#writing-data","title":"Writing Data","text":""},{"location":"user-guide/basic-usage/#basic-write","title":"Basic Write","text":"<pre><code>import polars as pl\n\ndata = pl.DataFrame({\n    'id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'value': [100, 200, 300]\n})\n\n# Write with default settings\ndataset.write_to_dataset(data)\n</code></pre>"},{"location":"user-guide/basic-usage/#optimized-write","title":"Optimized Write","text":"<pre><code># Write with optimization options\ndataset.write_to_dataset(\n    data,\n    mode=\"append\",                    # append, delta, overwrite\n    partition_by=[\"category\"],        # Hive-style partitioning\n    max_rows_per_file=1000000,       # Control file size\n    row_group_size=250000,           # Parquet row group size\n    compression=\"zstd\",              # Compression algorithm\n    sort_by=\"id DESC\",               # Sort before writing\n    unique=True,                     # Remove duplicates\n    update_metadata=True             # Update metadata after write\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#partitioned-writes","title":"Partitioned Writes","text":"<pre><code># Partition by multiple columns\ndata = pl.DataFrame({\n    'id': range(100),\n    'date': pl.date_range(start=2023-01-01, periods=100, interval='1d', eager=True),\n    'region': ['US', 'EU', 'APAC'] * 33 + ['US'],\n    'category': ['A', 'B'] * 50,\n    'value': range(100)\n})\n\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"year\", \"month\", \"region\"],  # Creates nested directories\n    max_rows_per_file=50000\n)\n\n# Creates structure:\n# data/my_dataset/\n#   \u251c\u2500\u2500 year=2023/\n#   \u2502   \u251c\u2500\u2500 month=01/\n#   \u2502   \u2502   \u251c\u2500\u2500 region=US/\n#   \u2502   \u2502   \u2502   \u2514\u2500\u2500 data-0.parquet\n#   \u2502   \u2502   \u2514\u2500\u2500 region=EU/\n#   \u2502   \u2502       \u2514\u2500\u2500 data-0.parquet\n</code></pre>"},{"location":"user-guide/basic-usage/#delta-updates","title":"Delta Updates","text":"<pre><code># Efficiently merge new data\nnew_data = pl.DataFrame({\n    'id': [101, 102],\n    'category': ['A', 'B'],\n    'value': [100, 200],\n    'updated_at': [datetime.now(), datetime.now()]\n})\n\n# Delta mode checks for existing data\ndataset.write_to_dataset(\n    new_data,\n    mode=\"delta\",\n    delta_subset=[\"id\"],             # Columns to check for duplicates\n    partition_by=[\"category\"]\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#reading-data","title":"Reading Data","text":""},{"location":"user-guide/basic-usage/#basic-reading","title":"Basic Reading","text":"<pre><code># Read entire dataset as Polars DataFrame\ndf = dataset.table.pl.collect()\n\n# Or as PyArrow Table\ntable = dataset.table.arrow()\n</code></pre>"},{"location":"user-guide/basic-usage/#filtering-with-automatic-backend-selection","title":"Filtering with Automatic Backend Selection","text":"<pre><code># Simple filters use PyArrow\nsimple_filtered = dataset.filter(\"category = 'A'\")\n\n# Complex filters automatically use DuckDB\ncomplex_filtered = dataset.filter(\"\"\"\n    category IN ('A', 'B')\n    AND value &gt; 100\n    AND name LIKE '%test%'\n    OR (category = 'C' AND value &lt; 50)\n\"\"\")\n\n# Get results\nresult = complex_filtered.collect()  # Returns Polars DataFrame\n</code></pre>"},{"location":"user-guide/basic-usage/#using-duckdb-directly","title":"Using DuckDB Directly","text":"<pre><code># Dataset is automatically registered in DuckDB\nsql_result = dataset.ddb_con.sql(\"\"\"\n    SELECT\n        category,\n        COUNT(*) as count,\n        AVG(value) as avg_value,\n        MIN(value) as min_value,\n        MAX(value) as max_value\n    FROM dataset\n    GROUP BY category\n    ORDER BY count DESC\n\"\"\").pl()  # Convert to Polars\n</code></pre>"},{"location":"user-guide/basic-usage/#partition-pruning","title":"Partition Pruning","text":"<pre><code># For partitioned datasets, PyDala2 automatically prunes partitions\n# when filtering on partition columns\nresult = dataset.filter(\"year = 2023 AND month = 01\")\n# Only reads files in year=2023/month=01/ directory\n</code></pre>"},{"location":"user-guide/basic-usage/#dataset-information","title":"Dataset Information","text":""},{"location":"user-guide/basic-usage/#basic-properties","title":"Basic Properties","text":"<pre><code># Dataset information\nprint(f\"Dataset name: {dataset.name}\")\nprint(f\"Dataset path: {dataset.path}\")\nprint(f\"Exists: {dataset.exists()}\")\n\n# Partition information\nprint(f\"Partition columns: {dataset.partition_names}\")\nprint(f\"Partition values: {dataset.partition_values}\")\n</code></pre>"},{"location":"user-guide/basic-usage/#schema-information","title":"Schema Information","text":"<pre><code># Get schema\nschema = dataset.schema\nprint(f\"Schema: {schema}\")\n\n# Check if column exists\nif 'value' in schema.names:\n    print(\"Value column exists\")\n\n# Get column type\nvalue_type = schema.field('value').type\nprint(f\"Value type: {value_type}\")\n</code></pre>"},{"location":"user-guide/basic-usage/#row-count-and-statistics","title":"Row Count and Statistics","text":"<pre><code># Count rows (efficient - uses metadata)\nrow_count = dataset.count_rows()\nprint(f\"Total rows: {row_count}\")\n\n# Get partition information\nif dataset.partitions is not None:\n    print(f\"Partitions: {dataset.partitions}\")\n</code></pre>"},{"location":"user-guide/basic-usage/#working-with-partitions","title":"Working with Partitions","text":""},{"location":"user-guide/basic-usage/#hive-partitioning","title":"Hive Partitioning","text":"<pre><code># Dataset with hive partitioning\nhive_dataset = ParquetDataset(\n    \"data/hive_data\",\n    partitioning=\"hive\"  # Auto-detects partition columns from paths\n)\n\n# Partition columns are automatically extracted\nprint(f\"Auto-detected partitions: {hive_dataset.partition_names}\")\n\n# Access partition values\nprint(f\"Available values: {hive_dataset.partition_values}\")\n</code></pre>"},{"location":"user-guide/basic-usage/#manual-partitioning","title":"Manual Partitioning","text":"<pre><code># Specify partition columns manually\nmanual_dataset = ParquetDataset(\n    \"data/manual\",\n    partitioning=[\"year\", \"month\", \"day\"]\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#metadata-management","title":"Metadata Management","text":""},{"location":"user-guide/basic-usage/#understanding-pydala2-metadata","title":"Understanding PyDala2 Metadata","text":"<p>PyDala2 automatically maintains two metadata files:</p> <ol> <li>_metadata: Combined Parquet metadata for all files</li> <li>_file_metadata: Per-file statistics (JSON, brotli compressed)</li> </ol>"},{"location":"user-guide/basic-usage/#working-with-metadata","title":"Working with Metadata","text":"<pre><code># Metadata is automatically managed\n# After writing, files are created:\n# - data/_metadata\n# - data/_file_metadata\n\n# Update metadata if files changed manually\ndataset.update_metadata()\n\n# Check metadata version\nprint(f\"Metadata version: {dataset.metadata_version}\")\n</code></pre>"},{"location":"user-guide/basic-usage/#custom-metadata","title":"Custom Metadata","text":"<pre><code># Add custom metadata to dataset\n# This is stored in the _file_metadata\ndataset.write_to_dataset(\n    data,\n    custom_metadata={\n        'description': 'Sales data for 2023',\n        'owner': 'sales_team',\n        'update_frequency': 'daily'\n    }\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/basic-usage/#time-series-data","title":"Time Series Data","text":"<pre><code># Time-based dataset\ntime_dataset = ParquetDataset(\n    \"metrics/daily\",\n    timestamp_column=\"timestamp\",\n    partitioning=[\"year\", \"month\", \"day\"]\n)\n\n# Write time series data\ntime_dataset.write_to_dataset(\n    time_data,\n    partition_by=[\"year\", \"month\", \"day\"],\n    sort_by=\"timestamp\"\n)\n\n# Time-based compaction\ntime_dataset.compact_by_timeperiod(\n    interval=\"1 month\",\n    timestamp_column=\"timestamp\"\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#schema-evolution","title":"Schema Evolution","text":"<pre><code># Add new column to existing dataset\nnew_data_with_col = data.with_columns(\n    pl.col(\"value\").alias(\"double_value\") * 2\n)\n\n# Allow schema changes\ndataset.write_to_dataset(\n    new_data_with_col,\n    mode=\"append\",\n    alter_schema=True\n)\n</code></pre>"},{"location":"user-guide/basic-usage/#data-type-optimization","title":"Data Type Optimization","text":"<pre><code># Optimize data types to reduce storage\ndataset.optimize_dtypes(\n    exclude=[\"id\"],  # Don't optimize ID columns\n    ts_unit=\"ms\"    # Convert timestamps to milliseconds\n)\n\n# This converts:\n# - int64 to int32/int16 where possible\n# - float64 to float32 where precision allows\n# - string to categorical for low cardinality\n</code></pre>"},{"location":"user-guide/basic-usage/#working-with-large-datasets","title":"Working with Large Datasets","text":"<pre><code># Process in batches using DuckDB\nbatch_size = 100000\noffset = 0\n\nwhile True:\n    batch = dataset.ddb_con.sql(f\"\"\"\n        SELECT * FROM dataset\n        ORDER BY id\n        LIMIT {batch_size} OFFSET {offset}\n    \"\"\").pl()\n\n    if len(batch) == 0:\n        break\n\n    process_batch(batch)\n    offset += batch_size\n</code></pre>"},{"location":"user-guide/basic-usage/#error-handling","title":"Error Handling","text":"<pre><code># PyDala2 handles many errors gracefully\ntry:\n    # This will work even if directory doesn't exist\n    dataset = ParquetDataset(\"new/dataset\")\n    dataset.write_to_dataset(data)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n# Check dataset before operations\nif not dataset.exists():\n    print(\"Dataset is empty - first write will create it\")\n</code></pre>"},{"location":"user-guide/basic-usage/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate partitioning:</li> <li>Partition on columns frequently used in filters</li> <li>Keep cardinality moderate (100-1000 values per partition)</li> <li> <p>Avoid over-partitioning (too many small files)</p> </li> <li> <p>Optimize file sizes:</p> </li> <li>Target 100MB-1GB per file</li> <li>Use <code>max_rows_per_file</code> to control size</li> <li> <p>Compact small files periodically</p> </li> <li> <p>Leverage automatic features:</p> </li> <li>Let PyDala2 choose between PyArrow/DuckDB</li> <li>Use <code>filter()</code> for automatic partition pruning</li> <li> <p>Enable caching for remote filesystems</p> </li> <li> <p>Monitor and maintain:</p> </li> <li>Update metadata after manual file operations</li> <li>Use <code>optimize_dtypes()</code> to reduce storage</li> <li> <p>Compact partitions when files become too small</p> </li> <li> <p>Use appropriate write modes:</p> </li> <li><code>append</code> for adding new data</li> <li><code>delta</code> for merging with change detection</li> <li><code>overwrite</code> for complete replacement</li> </ol>"},{"location":"user-guide/catalog-management/","title":"Catalog Management","text":"<p>The catalog system provides YAML-based configuration for managing datasets across different namespaces and filesystems.</p>"},{"location":"user-guide/catalog-management/#introduction-to-catalogs","title":"Introduction to Catalogs","text":"<p>A catalog acts as a centralized registry for your datasets, storing their locations, formats, and configurations in a YAML file. This makes it easy to:</p> <ul> <li>Organize datasets by namespace or project</li> <li>Manage different filesystems (local, S3, etc.)</li> <li>Share dataset configurations across teams</li> <li>Execute SQL queries across multiple datasets</li> </ul>"},{"location":"user-guide/catalog-management/#catalog-configuration","title":"Catalog Configuration","text":""},{"location":"user-guide/catalog-management/#basic-catalog-structure","title":"Basic Catalog Structure","text":"<pre><code># catalog.yaml\nfilesystem:\n  local:\n    protocol: file\n    bucket: ./data\n  s3_storage:\n    protocol: s3\n    bucket: my-bucket\n    key: your-access-key\n    secret: your-secret-key\n\ntables:\n  sales:\n    orders:\n      path: sales/orders\n      format: parquet\n      filesystem: local\n      partitioning: [year, month]\n      write_args:\n        compression: zstd\n        max_rows_per_file: 1000000\n\n    customers:\n      path: sales/customers\n      format: parquet\n      filesystem: local\n      write_args:\n        compression: snappy\n\n  finance:\n    revenue:\n      path: finance/revenue\n      format: parquet\n      filesystem: s3_storage\n      write_args:\n        compression: zstd\n        partition_by: [region, year]\n</code></pre>"},{"location":"user-guide/catalog-management/#creating-a-catalog","title":"Creating a Catalog","text":"<pre><code>from pydala import Catalog\n\n# Create catalog from YAML file\ncatalog = Catalog(\"catalog.yaml\")\n\n# Create catalog with specific namespace\nsales_catalog = Catalog(\"catalog.yaml\", namespace=\"sales\")\n\n# Use existing DuckDB connection\nimport duckdb\ncon = duckdb.connect()\ncatalog = Catalog(\"catalog.yaml\", ddb_con=con)\n</code></pre>"},{"location":"user-guide/catalog-management/#dataset-operations","title":"Dataset Operations","text":""},{"location":"user-guide/catalog-management/#loading-datasets","title":"Loading Datasets","text":"<pre><code># Load Parquet dataset with metadata\norders = catalog.load(\"sales.orders\", with_metadata=True)\n\n# Load as DataFrame instead of dataset\ncustomers_df = catalog.load(\"sales.customers\", as_dataset=False)\n\n# Auto-detect format and load\nrevenue = catalog.load(\"finance.revenue\")\n\n# Force reload from disk\norders = catalog.load(\"sales.orders\", reload=True)\n</code></pre>"},{"location":"user-guide/catalog-management/#creating-tables","title":"Creating Tables","text":"<pre><code>import polars as pl\n\n# Create table from DataFrame\ndata = pl.DataFrame({\n    'id': range(100),\n    'name': [f'item_{i}' for i in range(100)],\n    'value': [i * 10 for i in range(100)]\n})\n\ncatalog.create_table(\n    data=data,\n    table_name=\"inventory.items\",\n    format=\"parquet\",\n    filesystem=\"local\",\n    partitioning=[\"category\"],\n    write_args={\n        \"compression\": \"zstd\",\n        \"max_rows_per_file\": 50000\n    }\n)\n\n# Create table from existing dataset\ncatalog.create_table(\n    data=existing_dataset,\n    table_name=\"archive.old_data\",\n    path=\"archive/2023/data\",\n    overwrite=True\n)\n\n# Create placeholder table (no data written yet)\ncatalog.create_table(\n    table_name=\"future.placeholder\",\n    path=\"future/data\",\n    format=\"parquet\",\n    filesystem=\"s3_storage\"\n)\n</code></pre>"},{"location":"user-guide/catalog-management/#writing-data","title":"Writing Data","text":"<pre><code># Write to catalog table\nnew_orders = pl.read_csv(\"new_orders.csv\")\ncatalog.write_table(\n    data=new_orders,\n    table_name=\"sales.orders\",\n    mode=\"append\",\n    partition_by=[\"year\", \"month\"]\n)\n\n# Update catalog with new write arguments\ncatalog.write_table(\n    data=quarterly_data,\n    table_name=\"finance.revenue\",\n    compression=\"brotli\",\n    update_catalog=True\n)\n</code></pre>"},{"location":"user-guide/catalog-management/#updating-table-configuration","title":"Updating Table Configuration","text":"<pre><code># Update table metadata\ncatalog.update(\n    table_name=\"sales.orders\",\n    description=\"Customer order data\",\n    owner=\"sales_team\",\n    refresh_frequency=\"daily\",\n    write_args={\n        \"compression\": \"zstd\",\n        \"max_rows_per_file\": 2000000\n    }\n)\n\n# Change filesystem\ncatalog.update(\n    table_name=\"finance.revenue\",\n    filesystem=\"s3_storage\",\n    path=\"finance/processed/revenue\"\n)\n</code></pre>"},{"location":"user-guide/catalog-management/#deleting-tables","title":"Deleting Tables","text":"<pre><code># Remove from catalog only\ncatalog.delete_table(\"temp.data\")\n\n# Remove and delete all data files\ncatalog.delete_table(\"old.archive\", vacuum=True)\n\n# Delete entire namespace\ncatalog.delete_table(\"legacy\", vacuum=True)\n</code></pre>"},{"location":"user-guide/catalog-management/#namespace-management","title":"Namespace Management","text":""},{"location":"user-guide/catalog-management/#working-with-namespaces","title":"Working with Namespaces","text":"<pre><code># List available namespaces\nprint(catalog.list_namespaces)\n# Output: ['sales', 'finance', 'inventory']\n\n# Create catalog scoped to namespace\nsales_catalog = Catalog(\"catalog.yaml\", namespace=\"sales\")\n\n# Access tables in namespace\norders = sales_catalog.load(\"orders\")  # Loads sales.orders\ncustomers = sales_catalog.load(\"customers\")  # Loads sales.customers\n\n# Create new namespace\ncatalog.create_namespace(\"analytics\")\ncatalog.create_table(\n    data=metrics_data,\n    table_name=\"analytics.metrics\",\n    path=\"analytics/metrics\"\n)\n</code></pre>"},{"location":"user-guide/catalog-management/#cross-namespace-operations","title":"Cross-Namespace Operations","text":"<pre><code># Copy table between namespaces\ndef copy_table(source_catalog, dest_catalog, source_table, dest_table):\n    data = source_catalog.load(source_table, as_dataset=False)\n    dest_catalog.create_table(\n        data=data,\n        table_name=dest_table,\n        write_args={\"mode\": \"overwrite\"}\n    )\n\n# Usage\ncopy_table(\n    source_catalog=catalog,\n    dest_catalog=catalog,\n    source_table=\"sales.orders\",\n    dest_table=\"analytics.orders_archive\"\n)\n</code></pre>"},{"location":"user-guide/catalog-management/#sql-operations","title":"SQL Operations","text":""},{"location":"user-guide/catalog-management/#querying-multiple-tables","title":"Querying Multiple Tables","text":"<pre><code># SQL with automatic table loading\nresult = catalog.sql(\"\"\"\n    SELECT\n        o.order_id,\n        o.order_date,\n        c.customer_name,\n        o.total_amount\n    FROM sales.orders o\n    JOIN sales.customers c ON o.customer_id = c.id\n    WHERE o.order_date &gt;= '2023-01-01'\n    ORDER BY o.total_amount DESC\n    LIMIT 1000\n\"\"\")\n\n# Get as Polars DataFrame\ndf = result.pl()\n\n# Tables are automatically loaded and registered\nprint(catalog.registered_tables)\n</code></pre>"},{"location":"user-guide/catalog-management/#complex-analytics-queries","title":"Complex Analytics Queries","text":"<pre><code># Time-based analytics\nanalytics = catalog.sql(\"\"\"\n    WITH monthly_sales AS (\n        SELECT\n            DATE_TRUNC('month', order_date) as month,\n            region,\n            COUNT(*) as order_count,\n            SUM(total_amount) as revenue\n        FROM sales.orders\n        WHERE order_date &gt;= '2023-01-01'\n        GROUP BY month, region\n    )\n    SELECT\n        month,\n        region,\n        revenue,\n        LAG(revenue, 1) OVER (PARTITION BY region ORDER BY month) as prev_revenue,\n        (revenue - LAG(revenue, 1) OVER (PARTITION BY region ORDER BY month)) /\n            LAG(revenue, 1) OVER (PARTITION BY region ORDER BY month) * 100 as growth_rate\n    FROM monthly_sales\n    ORDER BY region, month\n\"\"\").pl()\n</code></pre>"},{"location":"user-guide/catalog-management/#filesystem-management","title":"Filesystem Management","text":""},{"location":"user-guide/catalog-management/#configuring-multiple-filesystems","title":"Configuring Multiple Filesystems","text":"<pre><code># Access configured filesystems\nfor fs_name in catalog.all_filesystems:\n    print(f\"Filesystem: {fs_name}\")\n    catalog.show_filesystem(fs_name)\n\n# Files are accessed through configured filesystems\norders_files = catalog.files(\"sales.orders\")\nprint(f\"Orders files: {orders_files[:5]}...\")  # Show first 5 files\n</code></pre>"},{"location":"user-guide/catalog-management/#cloud-storage-integration","title":"Cloud Storage Integration","text":"<pre><code># catalog.yaml\nfilesystem:\n  production:\n    protocol: s3\n    bucket: my-company-data\n    key: ${S3_ACCESS_KEY}\n    secret: ${S3_SECRET_KEY}\n    region: us-west-2\n\n  archive:\n    protocol: s3\n    bucket: my-company-archive\n    key: ${S3_ACCESS_KEY}\n    secret: ${S3_SECRET_KEY}\n    region: us-east-1\n\ntables:\n  production:\n    current:\n      path: sales/current\n      format: parquet\n      filesystem: production\n      write_args:\n        compression: zstd\n        partition_by: [date, region]\n\n    archived:\n      path: sales/historical\n      format: parquet\n      filesystem: archive\n      write_args:\n        compression: zstd\n        partition_by: [year, quarter]\n</code></pre>"},{"location":"user-guide/catalog-management/#catalog-inspection","title":"Catalog Inspection","text":""},{"location":"user-guide/catalog-management/#viewing-table-information","title":"Viewing Table Information","text":"<pre><code># Show table configuration\ncatalog.show(\"sales.orders\")\n# Output:\n# path: sales/orders\n# format: parquet\n# filesystem: local\n# partitioning: [year, month]\n# write_args:\n#   compression: zstd\n#   max_rows_per_file: 1000000\n\n# Get table configuration as object\nconfig = catalog.get(\"finance.revenue\")\nprint(f\"Table path: {config.path}\")\nprint(f\"Compression: {config.write_args.compression}\")\n\n# List all tables\nprint(\"All tables:\", catalog.all_tables)\n\n# Get table schema\nschema = catalog.schema(\"sales.orders\")\nprint(f\"Schema: {schema}\")\n</code></pre>"},{"location":"user-guide/catalog-management/#working-with-duckdb","title":"Working with DuckDB","text":"<pre><code># See registered tables\nprint(\"Registered tables:\")\ncatalog.registered_tables.show()\n\n# Manual table registration\ndataset = catalog.load(\"sales.orders\")\ncatalog.ddb_con.register(\"orders_temp\", dataset.table.pl.collect())\n\n# Use registered tables in queries\nresult = catalog.ddb_con.sql(\"\"\"\n    SELECT region, AVG(total_amount) as avg_order\n    FROM orders_temp\n    GROUP BY region\n\"\"\").pl()\n</code></pre>"},{"location":"user-guide/catalog-management/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"user-guide/catalog-management/#data-pipeline-with-catalog","title":"Data Pipeline with Catalog","text":"<pre><code>def run_pipeline(catalog, input_table, output_table, transform_func):\n    \"\"\"Run a data transformation pipeline\"\"\"\n    # Load input data\n    input_data = catalog.load(input_table, as_dataset=False)\n\n    # Apply transformation\n    output_data = transform_func(input_data)\n\n    # Create output table if needed\n    if output_table not in catalog.all_tables:\n        catalog.create_table(\n            table_name=output_table,\n            format=\"parquet\",\n            filesystem=\"local\"\n        )\n\n    # Write results\n    catalog.write_table(\n        data=output_data,\n        table_name=output_table,\n        mode=\"overwrite\"\n    )\n\n# Usage\ndef aggregate_daily_orders(df):\n    return df.groupby([\"date\", \"region\"]).agg([\n        pl.count(\"order_id\").alias(\"order_count\"),\n        pl.sum(\"total_amount\").alias(\"revenue\")\n    ])\n\nrun_pipeline(\n    catalog=catalog,\n    input_table=\"sales.orders\",\n    output_table=\"analytics.daily_summary\",\n    transform_func=aggregate_daily_orders\n)\n</code></pre>"},{"location":"user-guide/catalog-management/#catalog-validation","title":"Catalog Validation","text":"<pre><code>def validate_catalog(catalog):\n    \"\"\"Validate catalog configuration\"\"\"\n    issues = []\n\n    # Check all table paths exist\n    for table_name in catalog.all_tables:\n        try:\n            files = catalog.files(table_name)\n            if not files:\n                issues.append(f\"No files found for {table_name}\")\n        except Exception as e:\n            issues.append(f\"Error accessing {table_name}: {e}\")\n\n    # Check filesystem configurations\n    for fs_name in catalog.all_filesystems:\n        try:\n            fs = catalog.params.filesystem[fs_name]\n            if fs.protocol not in [\"file\", \"s3\"]:\n                issues.append(f\"Unknown protocol for {fs_name}: {fs.protocol}\")\n        except Exception as e:\n            issues.append(f\"Error in filesystem {fs_name}: {e}\")\n\n    return issues\n\n# Usage\nissues = validate_catalog(catalog)\nif issues:\n    print(\"Catalog validation issues:\")\n    for issue in issues:\n        print(f\"  - {issue}\")\nelse:\n    print(\"Catalog is valid\")\n</code></pre>"},{"location":"user-guide/catalog-management/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/catalog-management/#organization","title":"Organization","text":"<pre><code># Use hierarchical namespaces\ntables:\n  raw:\n    source1:\n      path: raw/source1\n      format: json\n    source2:\n      path: raw/source2\n      format: csv\n\n  processed:\n    cleaned:\n      path: processed/cleaned\n      format: parquet\n      write_args:\n        compression: zstd\n        partition_by: [date]\n\n    aggregated:\n      path: processed/aggregated\n      format: parquet\n      write_args:\n        compression: zstd\n\n  analytics:\n    dashboards:\n      path: analytics/dashboards\n      format: parquet\n</code></pre>"},{"location":"user-guide/catalog-management/#performance","title":"Performance","text":"<pre><code># Cache frequently accessed datasets\ncatalog.load(\"sales.orders\", with_metadata=True)  # Load with metadata\n\n# Use SQL for complex operations across tables\nresult = catalog.sql(\"\"\"\n    SELECT * FROM sales.orders\n    WHERE customer_id IN (\n        SELECT id FROM sales.customers\n        WHERE segment = 'Premium'\n    )\n\"\"\")\n\n# Bulk operations\nfor table_name in catalog.all_tables:\n    if table_name.startswith(\"temp.\"):\n        catalog.delete_table(table_name, vacuum=True)\n</code></pre>"},{"location":"user-guide/catalog-management/#security","title":"Security","text":"<pre><code># Use environment variables for credentials\nfilesystem:\n  secure_storage:\n    protocol: s3\n    bucket: sensitive-data\n    key: ${AWS_ACCESS_KEY_ID}\n    secret: ${AWS_SECRET_ACCESS_KEY}\n\n# Control access at catalog level\ntables:\n  restricted:\n    financial:\n      path: secure/financial\n      format: parquet\n      filesystem: secure_storage\n      access_control: [\"finance_team\", \"executives\"]\n</code></pre> <p>The catalog system provides a powerful way to organize and manage your datasets while leveraging PyDala2's full capabilities for data operations.</p>"},{"location":"user-guide/data-operations/","title":"Data Operations","text":"<p>This guide covers advanced data operations and transformations you can perform with PyDala2.</p>"},{"location":"user-guide/data-operations/#filtering-operations","title":"Filtering Operations","text":""},{"location":"user-guide/data-operations/#basic-filtering-with-automatic-backend-selection","title":"Basic Filtering with Automatic Backend Selection","text":"<pre><code>from pydala import ParquetDataset\n\ndataset = ParquetDataset(\"data/sales\")\n\n# Simple filters use PyArrow automatically\nsimple_result = dataset.filter(\"region = 'North America'\")\n\n# Complex filters automatically use DuckDB\ncomplex_result = dataset.filter(\"\"\"\n    (category = 'Electronics' AND price &gt; 500) OR\n    (category = 'Books' AND rating &gt;= 4.5) OR\n    (customer_id IN (SELECT DISTINCT customer_id FROM premium_customers))\n\"\"\")\n\n# String operations use DuckDB\nstring_result = dataset.filter(\"\"\"\n    product_name LIKE '%Laptop%'\n    AND customer_email LIKE '%@company.com'\n    AND NOT product_description LIKE '%refurbished%'\n\"\"\")\n\n# Get results as Polars DataFrame\ndf = complex_result.collect()\n</code></pre>"},{"location":"user-guide/data-operations/#advanced-filter-patterns","title":"Advanced Filter Patterns","text":"<pre><code># Date range filtering\ndate_result = dataset.filter(\"\"\"\n    sale_date BETWEEN '2023-01-01' AND '2023-12-31'\n    AND sale_date &lt; CURRENT_DATE\n\"\"\")\n\n# Using subqueries\nsubquery_result = dataset.filter(\"\"\"\n    customer_id IN (\n        SELECT customer_id\n        FROM customers\n        WHERE segment = 'Premium'\n        AND total_lifetime_value &gt; 10000\n    )\n\"\"\")\n\n# Pattern matching with regex\nregex_result = dataset.filter(\"\"\"\n    product_code ~ '^[A-Z]{2}-\\\\d{4}$'\n    AND product_name !~ '.*(test|demo|sample).*'\n\"\"\")\n\n# NULL handling\nnull_result = dataset.filter(\"\"\"\n    (status IS NOT NULL AND status != 'cancelled')\n    OR (refund_amount IS NULL AND order_date &gt; '2023-06-01')\n\"\"\")\n</code></pre>"},{"location":"user-guide/data-operations/#explicit-backend-selection","title":"Explicit Backend Selection","text":"<pre><code># Force using PyArrow for simple operations\narrow_result = dataset.filter(\"region = 'US'\", use=\"pyarrow\")\n\n# Force using DuckDB for complex operations\nduckdb_result = dataset.filter(\"\"\"\n    product_name LIKE '%test%'\n    OR customer_id IN (SELECT id FROM high_value_customers)\n\"\"\", use=\"duckdb\")\n</code></pre>"},{"location":"user-guide/data-operations/#aggregation-operations","title":"Aggregation Operations","text":""},{"location":"user-guide/data-operations/#using-duckdb-for-aggregations","title":"Using DuckDB for Aggregations","text":"<pre><code># Basic aggregations\nagg_result = dataset.ddb_con.sql(\"\"\"\n    SELECT\n        region,\n        category,\n        COUNT(*) as order_count,\n        SUM(amount) as total_revenue,\n        AVG(amount) as avg_order_value,\n        MIN(amount) as min_order,\n        MAX(amount) as max_order,\n        COUNT(DISTINCT customer_id) as unique_customers\n    FROM dataset\n    WHERE date &gt;= '2023-01-01'\n    GROUP BY region, category\n    HAVING COUNT(*) &gt; 100\n    ORDER BY total_revenue DESC\n\"\"\").pl()\n\n# Window functions\nwindow_result = dataset.ddb_con.sql(\"\"\"\n    SELECT\n        order_id,\n        customer_id,\n        order_date,\n        amount,\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id\n            ORDER BY order_date\n        ) as order_sequence,\n        RANK() OVER (\n            PARTITION BY customer_id\n            ORDER BY amount DESC\n        ) as amount_rank,\n        LAG(amount, 1) OVER (\n            PARTITION BY customer_id\n            ORDER BY order_date\n        ) as prev_amount,\n        SUM(amount) OVER (\n            PARTITION BY customer_id\n            ORDER BY order_date\n            RANGE BETWEEN INTERVAL 30 DAY PRECEDING AND CURRENT ROW\n        ) as rolling_30d_sum\n    FROM dataset\n    ORDER BY customer_id, order_date\n\"\"\").pl()\n</code></pre>"},{"location":"user-guide/data-operations/#time-based-aggregations","title":"Time-based Aggregations","text":"<pre><code># Time series aggregations\ntime_agg = dataset.ddb_con.sql(\"\"\"\n    SELECT\n        DATE_TRUNC('month', order_date) as month,\n        DATE_TRUNC('week', order_date) as week,\n        region,\n        COUNT(*) as orders,\n        SUM(amount) as revenue,\n        AVG(amount) as avg_order,\n        COUNT(DISTINCT customer_id) as customers\n    FROM dataset\n    WHERE order_date &gt;= '2023-01-01'\n    GROUP BY month, week, region\n    ORDER BY month, week\n\"\"\").pl()\n\n# Growth calculations\ngrowth_result = dataset.ddb_con.sql(\"\"\"\n    WITH monthly_data AS (\n        SELECT\n            DATE_TRUNC('month', order_date) as month,\n            SUM(amount) as revenue\n        FROM dataset\n        GROUP BY month\n    )\n    SELECT\n        month,\n        revenue,\n        LAG(revenue, 1) OVER (ORDER BY month) as prev_revenue,\n        (revenue - LAG(revenue, 1) OVER (ORDER BY month)) /\n            LAG(revenue, 1) OVER (ORDER BY month) * 100 as growth_rate\n    FROM monthly_data\n    ORDER BY month\n\"\"\").pl()\n</code></pre>"},{"location":"user-guide/data-operations/#data-transformations","title":"Data Transformations","text":""},{"location":"user-guide/data-operations/#using-polars-for-complex-transformations","title":"Using Polars for Complex Transformations","text":"<pre><code>import polars as pl\n\n# Read data\ndf = dataset.table.pl.collect()\n\n# Chain transformations\ntransformed = (\n    df\n    .filter(pl.col(\"status\") == \"completed\")\n    .with_columns([\n        # Date transformations\n        pl.col(\"order_date\").dt.year().alias(\"year\"),\n        pl.col(\"order_date\").dt.month().alias(\"month\"),\n        pl.col(\"order_date\").dt.day().alias(\"day\"),\n        pl.col(\"order_date\").dt.weekday().alias(\"day_of_week\"),\n\n        # Business calculations\n        (pl.col(\"unit_price\") * pl.col(\"quantity\")).alias(\"total_amount\"),\n        (pl.col(\"unit_price\") * pl.col(\"quantity\") * 0.1).alias(\"tax_amount\"),\n        (pl.col(\"unit_price\") * pl.col(\"quantity\") * 1.1).alias(\"total_with_tax\"),\n\n        # Conditional columns\n        pl.when(pl.col(\"total_amount\") &gt; 1000)\n        .then(\"High Value\")\n        .when(pl.col(\"total_amount\") &gt; 500)\n        .then(\"Medium Value\")\n        .otherwise(\"Low Value\")\n        .alias(\"order_value_category\")\n    ])\n    .group_by([\"year\", \"month\", \"customer_segment\"])\n    .agg([\n        pl.count(\"order_id\").alias(\"order_count\"),\n        pl.sum(\"total_amount\").alias(\"total_revenue\"),\n        pl.mean(\"total_amount\").alias(\"avg_order_value\"),\n        pl.col(\"customer_id\").n_unique().alias(\"unique_customers\"),\n        pl.quantile(\"total_amount\", 0.95).alias(\"p95_order_value\")\n    ])\n)\n\n# Write transformed data\noutput_dataset = ParquetDataset(\"data/transformed_sales\")\noutput_dataset.write_to_dataset(\n    transformed,\n    partition_by=[\"year\", \"month\"],\n    sort_by=\"total_revenue DESC\"\n)\n</code></pre>"},{"location":"user-guide/data-operations/#schema-evolution-and-type-conversions","title":"Schema Evolution and Type Conversions","text":"<pre><code># Add new columns with type conversions\nenhanced_data = df.with_columns([\n    # String to categorical\n    pl.col(\"category\").cast(pl.Categorical),\n\n    # Numeric conversions\n    pl.col(\"price\").cast(pl.Float32),\n    pl.col(\"quantity\").cast(pl.Int32),\n\n    # Date operations\n    pl.col(\"order_date\").cast(pl.Date),\n\n    # New calculated columns\n    (pl.col(\"price\") * pl.col(\"quantity\")).alias(\"total_price\"),\n    pl.col(\"customer_email\").str.to_lowercase().alias(\"email_lower\")\n])\n\n# Write with schema evolution\ndataset.write_to_dataset(\n    enhanced_data,\n    mode=\"append\",\n    alter_schema=True\n)\n</code></pre>"},{"location":"user-guide/data-operations/#join-operations","title":"Join Operations","text":""},{"location":"user-guide/data-operations/#using-duckdb-for-complex-joins","title":"Using DuckDB for Complex Joins","text":"<pre><code># Multiple dataset joins via catalog\nfrom pydala import Catalog\n\ncatalog = Catalog(\"catalog.yaml\")\n\n# Get datasets\norders = catalog.get_table(\"orders\")\ncustomers = catalog.get_table(\"customers\")\nproducts = catalog.get_table(\"products\")\n\n# Complex join with aggregations\njoin_result = orders.ddb_con.sql(\"\"\"\n    SELECT\n        o.order_id,\n        o.order_date,\n        c.customer_name,\n        c.segment,\n        p.product_name,\n        p.category,\n        o.quantity,\n        o.unit_price,\n        (o.quantity * o.unit_price) as total_amount,\n        ROW_NUMBER() OVER (\n            PARTITION BY c.customer_id\n            ORDER BY o.order_date DESC\n        ) as recent_order_rank\n    FROM orders o\n    INNER JOIN customers c ON o.customer_id = c.id\n    INNER JOIN products p ON o.product_id = p.id\n    WHERE o.order_date &gt;= '2023-01-01'\n      AND c.segment IN ('Premium', 'VIP')\n      AND p.category IN ('Electronics', 'Appliances')\n    QUALIFY recent_order_rank &lt;= 5  -- Get only 5 most recent orders per customer\n    ORDER BY c.segment, c.customer_name, o.order_date DESC\n\"\"\").pl()\n</code></pre>"},{"location":"user-guide/data-operations/#self-joins-and-recursive-patterns","title":"Self-Joins and Recursive Patterns","text":"<pre><code># Customer purchase patterns\nself_join_result = dataset.ddb_con.sql(\"\"\"\n    WITH customer_orders AS (\n        SELECT\n            customer_id,\n            order_date,\n            total_amount,\n            LAG(order_date, 1) OVER (\n                PARTITION BY customer_id\n                ORDER BY order_date\n            ) as prev_order_date\n        FROM dataset\n    )\n    SELECT\n        customer_id,\n        COUNT(*) as order_count,\n        AVG(total_amount) as avg_order_value,\n        AVG(DATEDIFF('day', prev_order_date, order_date)) as avg_days_between_orders,\n        COUNT(CASE WHEN DATEDIFF('day', prev_order_date, order_date) &lt;= 7\n             THEN 1 END) as repeat_purchase_count\n    FROM customer_orders\n    WHERE prev_order_date IS NOT NULL\n    GROUP BY customer_id\n    HAVING COUNT(*) &gt;= 2\n\"\"\").pl()\n</code></pre>"},{"location":"user-guide/data-operations/#advanced-write-patterns","title":"Advanced Write Patterns","text":""},{"location":"user-guide/data-operations/#conditional-and-batched-writes","title":"Conditional and Batched Writes","text":"<pre><code># Write in batches with validation\ndef write_in_batches(dataset, data, batch_size=100000):\n    \"\"\"Write data in batches with validation\"\"\"\n    total_rows = len(data)\n\n    for i in range(0, total_rows, batch_size):\n        batch = data.slice(i, batch_size)\n\n        # Validate batch\n        if batch[\"amount\"].min() &lt; 0:\n            print(f\"Warning: Negative amounts found in batch {i//batch_size}\")\n\n        # Write batch\n        dataset.write_to_dataset(\n            batch,\n            mode=\"append\",\n            update_metadata=(i + batch_size &gt;= total_rows)  # Update metadata on last batch\n        )\n\n        print(f\"Written batch {i//batch_size + 1}/{(total_rows-1)//batch_size + 1}\")\n\n# Usage\nwrite_in_batches(dataset, large_dataframe)\n</code></pre>"},{"location":"user-guide/data-operations/#upsert-operations","title":"Upsert Operations","text":"<pre><code>def upsert_data(dataset, new_data, key_columns):\n    \"\"\"Implement upsert functionality\"\"\"\n    # Get existing keys\n    existing_keys = dataset.ddb_con.sql(f\"\"\"\n        SELECT {', '.join(key_columns)}\n        FROM dataset\n    \"\"\").pl()\n\n    # Find new vs existing records\n    new_keys = new_data.select(key_columns)\n\n    # Insert new records\n    new_records = new_data.join(\n        existing_keys,\n        on=key_columns,\n        how=\"anti\"\n    )\n\n    if len(new_records) &gt; 0:\n        dataset.write_to_dataset(\n            new_records,\n            mode=\"append\"\n        )\n        print(f\"Inserted {len(new_records)} new records\")\n\n    # Update existing records using delta mode\n    if len(new_data) &gt; len(new_records):\n        dataset.write_to_dataset(\n            new_data,\n            mode=\"delta\",\n            delta_subset=key_columns\n        )\n        print(f\"Updated existing records\")\n\n# Usage\nupsert_data(dataset, updated_customers, [\"customer_id\"])\n</code></pre>"},{"location":"user-guide/data-operations/#data-quality-operations","title":"Data Quality Operations","text":""},{"location":"user-guide/data-operations/#automated-data-validation","title":"Automated Data Validation","text":"<pre><code>def validate_dataset_quality(dataset):\n    \"\"\"Perform comprehensive data quality checks\"\"\"\n    # Read sample for validation\n    sample = dataset.table.pl.collect().head(10000)\n\n    quality_report = {\n        \"row_count\": dataset.count_rows(),\n        \"column_count\": len(sample.columns),\n        \"null_counts\": {},\n        \"data_types\": {},\n        \"value_ranges\": {},\n        \"duplicate_rows\": 0,\n        \"issues\": []\n    }\n\n    # Check each column\n    for col in sample.columns:\n        # Null counts\n        null_count = sample[col].null_count()\n        quality_report[\"null_counts\"][col] = null_count\n\n        # Data types\n        quality_report[\"data_types\"][col] = str(sample[col].dtype)\n\n        # Value ranges for numeric columns\n        if sample[col].dtype in [pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n            quality_report[\"value_ranges\"][col] = {\n                \"min\": sample[col].min(),\n                \"max\": sample[col].max(),\n                \"mean\": sample[col].mean()\n            }\n\n            # Check for outliers\n            q1 = sample[col].quantile(0.25)\n            q3 = sample[col].quantile(0.75)\n            iqr = q3 - q1\n            outliers = sample.filter(\n                (sample[col] &lt; q1 - 1.5 * iqr) |\n                (sample[col] &gt; q3 + 1.5 * iqr)\n            )\n\n            if len(outliers) &gt; 0:\n                quality_report[\"issues\"].append(\n                    f\"Column {col} has {len(outliers)} potential outliers\"\n                )\n\n    # Check for duplicates\n    quality_report[\"duplicate_rows\"] = sample.is_duplicated().sum()\n\n    return quality_report\n\n# Usage\nquality = validate_dataset_quality(dataset)\nprint(f\"Quality Report: {quality}\")\n</code></pre>"},{"location":"user-guide/data-operations/#automated-data-cleaning","title":"Automated Data Cleaning","text":"<pre><code>def clean_dataset(dataset, rules):\n    \"\"\"Clean dataset based on business rules\"\"\"\n    # Read data\n    df = dataset.table.pl.collect()\n\n    # Apply cleaning rules\n    for rule in rules:\n        if rule[\"type\"] == \"remove_nulls\":\n            df = df.filter(pl.col(rule[\"column\"]).is_not_null())\n\n        elif rule[\"type\"] == \"remove_outliers\":\n            col = rule[\"column\"]\n            q1 = df[col].quantile(0.25)\n            q3 = df[col].quantile(0.75)\n            iqr = q3 - q1\n            df = df.filter(\n                (df[col] &gt;= q1 - 1.5 * iqr) &amp;\n                (df[col] &lt;= q3 + 1.5 * iqr)\n            )\n\n        elif rule[\"type\"] == \"standardize_format\":\n            if rule[\"format\"] == \"uppercase\":\n                df = df.with_columns(\n                    pl.col(rule[\"column\"]).str.to_uppercase()\n                )\n            elif rule[\"format\"] == \"lowercase\":\n                df = df.with_columns(\n                    pl.col(rule[\"column\"]).str.to_lowercase()\n                )\n\n        elif rule[\"type\"] == \"fill_missing\":\n            if rule[\"strategy\"] == \"mean\":\n                fill_value = df[rule[\"column\"]].mean()\n            elif rule[\"strategy\"] == \"median\":\n                fill_value = df[rule[\"column\"]].median()\n            elif rule[\"strategy\"] == \"mode\":\n                fill_value = df[rule[\"column\"]].mode()[0]\n\n            df = df.with_columns(\n                pl.col(rule[\"column\"]).fill_null(fill_value)\n            )\n\n    # Write cleaned data\n    cleaned_dataset = ParquetDataset(f\"{dataset.path}_cleaned\")\n    cleaned_dataset.write_to_dataset(df)\n\n    return cleaned_dataset\n\n# Usage\ncleaning_rules = [\n    {\"type\": \"remove_nulls\", \"column\": \"customer_id\"},\n    {\"type\": \"remove_outliers\", \"column\": \"order_amount\"},\n    {\"type\": \"standardize_format\", \"column\": \"email\", \"format\": \"lowercase\"},\n    {\"type\": \"fill_missing\", \"column\": \"age\", \"strategy\": \"median\"}\n]\n\ncleaned = clean_dataset(dataset, cleaning_rules)\n</code></pre>"},{"location":"user-guide/data-operations/#performance-optimization-patterns","title":"Performance Optimization Patterns","text":""},{"location":"user-guide/data-operations/#query-optimization","title":"Query Optimization","text":"<pre><code># Optimize frequent queries with materialized views\nclass MaterializedView:\n    def __init__(self, base_dataset, view_name, query):\n        self.base_dataset = base_dataset\n        self.view_name = view_name\n        self.query = query\n        self.view_dataset = ParquetDataset(f\"views/{view_name}\")\n\n    def refresh(self):\n        \"\"\"Refresh the materialized view\"\"\"\n        result = self.base_dataset.ddb_con.sql(self.query)\n        self.view_dataset.write_to_dataset(\n            result.pl(),\n            mode=\"overwrite\",\n            update_metadata=True\n        )\n\n    def query(self, additional_filters=\"\"):\n        \"\"\"Query the materialized view\"\"\"\n        full_query = f\"SELECT * FROM {self.view_name}\"\n        if additional_filters:\n            full_query += f\" WHERE {additional_filters}\"\n\n        return self.view_dataset.ddb_con.sql(full_query)\n\n# Usage\nmv = MaterializedView(\n    dataset,\n    \"daily_sales_summary\",\n    \"\"\"\n    SELECT\n        DATE_TRUNC('day', order_date) as day,\n        region,\n        category,\n        COUNT(*) as orders,\n        SUM(amount) as revenue,\n        COUNT(DISTINCT customer_id) as customers\n    FROM dataset\n    GROUP BY day, region, category\n    \"\"\"\n)\n\n# Refresh daily\nmv.refresh()\n\n# Query the view\nresult = mv.query(\"day &gt;= '2023-12-01'\")\n</code></pre>"},{"location":"user-guide/data-operations/#partitioning-optimization","title":"Partitioning Optimization","text":"<pre><code>def optimize_partitioning(dataset, target_rows_per_file=1000000):\n    \"\"\"Analyze and optimize partitioning strategy\"\"\"\n    # Get current statistics\n    total_rows = dataset.count_rows()\n    current_files = len(dataset.files)\n\n    # Analyze partition cardinality\n    if dataset.partition_names:\n        partition_stats = {}\n\n        for part_col in dataset.partition_names:\n            unique_vals = dataset.ddb_con.sql(f\"\"\"\n                SELECT DISTINCT {part_col}\n                FROM dataset\n            \"\"\").pl()\n\n            partition_stats[part_col] = len(unique_vals)\n\n        # Recommend optimal partitioning\n        avg_rows_per_partition = total_rows / max(1, current_files)\n\n        recommendations = []\n\n        if avg_rows_per_partition &lt; target_rows_per_file / 10:\n            recommendations.append(\"Consider reducing partition levels - too many small files\")\n        elif avg_rows_per_partition &gt; target_rows_per_file * 10:\n            recommendations.append(\"Consider adding more partition levels - files too large\")\n\n        return {\n            \"total_rows\": total_rows,\n            \"current_files\": current_files,\n            \"avg_rows_per_file\": avg_rows_per_partition,\n            \"partition_stats\": partition_stats,\n            \"recommendations\": recommendations\n        }\n\n# Usage\nstats = optimize_partitioning(dataset)\nprint(f\"Partitioning analysis: {stats}\")\n</code></pre> <p>These patterns demonstrate the powerful data operations available in PyDala2, leveraging its dual-engine architecture to provide optimal performance for both simple and complex data processing tasks.</p>"},{"location":"user-guide/metadata/","title":"Metadata Management","text":"<p>This guide explains PyDala2's sophisticated metadata system that enables efficient dataset management and query optimization through automatic metadata collection and intelligent file-based operations.</p>"},{"location":"user-guide/metadata/#overview","title":"Overview","text":"<p>PyDala2 implements a two-tier metadata system:</p> <ol> <li>ParquetDatasetMetadata: Base class managing metadata collection and storage</li> <li>PydalaDatasetMetadata: Extended class with DuckDB integration and advanced scanning capabilities</li> </ol> <p>The system maintains two key files: - <code>_metadata</code>: Consolidated metadata used by PyArrow for instant dataset loading - <code>_file_metadata</code>: Detailed individual file metadata with brotli compression</p> <p>Key benefits: - Instant dataset loading via PyArrow's <code>parquet_dataset()</code> function - Metadata-based filtering to skip irrelevant files - Automatic schema evolution and repair - SQL access to metadata statistics through DuckDB</p>"},{"location":"user-guide/metadata/#the-two-tier-metadata-system","title":"The Two-Tier Metadata System","text":""},{"location":"user-guide/metadata/#the-_metadata-file","title":"The _metadata File","text":"<p>The <code>_metadata</code> file is a special Parquet file that consolidates metadata from all files: - Unified schema information across all files - Row group metadata from all files - Column statistics (min, max, null counts) - File paths and sizes - Compression information</p> <p>This file is used directly by PyArrow's <code>parquet_dataset()</code> function to create datasets instantly without directory scanning.</p>"},{"location":"user-guide/metadata/#the-_file_metadata-file","title":"The _file_metadata File","text":"<p>The <code>_file_metadata</code> file stores detailed individual file metadata: - Complete <code>pyarrow.parquet.FileMetaData</code> objects for each file - Serialized with brotli compression for efficient storage - Used for advanced operations and schema repair</p>"},{"location":"user-guide/metadata/#key-benefits","title":"Key Benefits","text":"<ol> <li>Instant Dataset Loading: PyArrow loads datasets using <code>_metadata</code> without scanning</li> <li>File Pruning: Skip files that don't contain relevant data based on statistics</li> <li>Query Optimization: Statistics enable predicate pushdown to individual files</li> <li>Schema Evolution: Automatic detection and repair of schema differences</li> <li>SQL Integration: DuckDB provides SQL access to metadata statistics</li> </ol>"},{"location":"user-guide/metadata/#working-with-metadata","title":"Working with Metadata","text":""},{"location":"user-guide/metadata/#automatic-metadata-management","title":"Automatic Metadata Management","text":"<p>PyDala2 automatically manages metadata files when you work with datasets:</p> <pre><code>from pydala import ParquetDataset\nimport pandas as pd\n\n# Create dataset - metadata is automatically managed\ndataset = ParquetDataset(\"data/sales\")\n\n# Write data - metadata files are automatically created/updated\ndataset.write_to_dataset(\n    data=df,\n    partition_cols=['year', 'month']\n)\n\n# Both _metadata and _file_metadata are created automatically\n</code></pre>"},{"location":"user-guide/metadata/#manual-metadata-updates","title":"Manual Metadata Updates","text":"<pre><code># Update metadata when needed\ndataset.update(verbose=True)\n\n# Update only consolidated metadata\ndataset.update(update_file_metadata=False)\n\n# Update only file metadata\ndataset.update(update_metadata=False)\n</code></pre>"},{"location":"user-guide/metadata/#accessing-metadata-information","title":"Accessing Metadata Information","text":"<pre><code># Access dataset properties (uses metadata)\nprint(f\"Total rows: {dataset.num_rows:,}\")\nprint(f\"Number of files: {dataset.num_files}\")\nprint(f\"Dataset size: {dataset.size_bytes / 1024 / 1024:.1f} MB\")\nprint(f\"Schema: {dataset.schema}\")\n\n# Access metadata properties\nprint(f\"Has _metadata: {dataset.has_metadata_file}\")\nprint(f\"Has _file_metadata: {dataset.has_file_metadata_file}\")\n\n# Access metadata table\nmetadata_table = dataset.metadata\n</code></pre>"},{"location":"user-guide/metadata/#metadata-based-operations","title":"Metadata-Based Operations","text":""},{"location":"user-guide/metadata/#file-scanning","title":"File Scanning","text":"<p>The <code>scan()</code> method is a powerful feature that filters files based on metadata statistics without reading their contents:</p> <pre><code># Scan files based on date range\nmatching_files = dataset.scan(\n    filters=\"date &gt;= '2023-01-01' AND date &lt;= '2023-12-31'\",\n    verbose=True\n)\n\n# Use dictionary filters for complex conditions\nmatching_files = dataset.scan(\n    filters={\n        'amount': {'min': 100, 'max': 1000},\n        'category': ['A', 'B', 'C']\n    }\n)\n\n# Get results as a table with statistics\nresults = dataset.scan(\n    filters=\"status = 'completed'\",\n    return_table=True\n)\n</code></pre>"},{"location":"user-guide/metadata/#sql-queries-on-metadata","title":"SQL Queries on Metadata","text":"<p>PyDala2 integrates with DuckDB to enable SQL queries on metadata:</p> <pre><code># Query metadata statistics\nstats = dataset.query_metadata(\"\"\"\n    SELECT\n        column_name,\n        min_value,\n        max_value,\n        null_count,\n        SUM(num_rows) as total_rows\n    FROM metadata_table\n    WHERE column_name IN ('amount', 'date')\n    GROUP BY column_name\n\"\"\").to_arrow()\n\n# Get file information\nfiles_info = dataset.ddb_con.sql(\"\"\"\n    SELECT file_path, num_rows, size_bytes\n    FROM dataset_metadata\n    WHERE date &gt;= '2023-01-01'\n    ORDER BY num_rows DESC\n\"\"\").to_arrow()\n</code></pre>"},{"location":"user-guide/metadata/#query-optimization","title":"Query Optimization","text":"<p>PyDala2 automatically uses metadata for query optimization:</p> <pre><code># Filters automatically use metadata statistics\nfiltered = dataset.filter(\"date &gt; '2023-01-01' AND amount &gt; 1000\")\n\n# The query engine:\n# 1. Uses metadata to skip irrelevant files\n# 2. Applies predicate pushdown to remaining files\n# 3. Only reads necessary columns\n\n# Column selection also optimized\nresult = dataset.table.to_polars(columns=['id', 'date', 'amount'])\n# Only these columns are read from disk\n</code></pre>"},{"location":"user-guide/metadata/#advanced-metadata-usage","title":"Advanced Metadata Usage","text":""},{"location":"user-guide/metadata/#working-with-scan-results","title":"Working with Scan Results","text":"<pre><code># Reset scan to start fresh\ndataset.reset_scan()\n\n# Perform complex scans\nmatching_files = dataset.scan(\n    filters=[\n        \"date &gt;= '2023-01-01'\",\n        \"amount &gt; 1000\",\n        \"category IN ('A', 'B')\"\n    ],\n    columns=['date', 'amount', 'category'],\n    verbose=True\n)\n\n# Use the matching files list\nfor file_path in matching_files[:5]:  # Process first 5 files\n    print(f\"Processing: {file_path}\")\n</code></pre>"},{"location":"user-guide/metadata/#metadata-table-operations","title":"Metadata Table Operations","text":"<pre><code># Update the metadata table explicitly\ndataset.update_metadata_table()\n\n# Access the DuckDB metadata table\nmetadata_relation = dataset.metadata_table\n\n# Perform complex metadata queries\nresult = metadata_relation.filter(\"num_rows &gt; 10000\").project(\"file_path, num_rows\")\n</code></pre>"},{"location":"user-guide/metadata/#schema-management","title":"Schema Management","text":"<pre><code># Access unified schema\nschema = dataset.schema\n\n# Check schema consistency\nprint(f\"Schema fields: {len(schema)}\")\nfor field in schema:\n    print(f\"  {field.name}: {field.type}\")\n\n# Schema repair happens automatically during update\ndataset.update(repair=True)\n</code></pre>"},{"location":"user-guide/metadata/#pyarrow-integration","title":"PyArrow Integration","text":"<p>PyDala2 leverages PyArrow's native metadata support:</p> <pre><code>import pyarrow.dataset as pds\n\n# PyDala2 uses _metadata file internally\n# When you access the dataset, it loads instantly:\ndataset = ParquetDataset(\"data/sales\")\n\n# This is equivalent to:\n# ds = pds.parquet_dataset(\"data/sales/_metadata\")\n\n# The _metadata file contains all necessary information\n# No directory scanning required\n</code></pre>"},{"location":"user-guide/metadata/#performance-considerations","title":"Performance Considerations","text":""},{"location":"user-guide/metadata/#when-to-update-metadata","title":"When to Update Metadata","text":"<p>Update metadata using <code>dataset.update()</code> when: - After writing new data to the dataset - After compaction operations (<code>compact_partitions</code>, <code>compact_by_timeperiod</code>) - After manually adding/removing files - When query performance seems degraded - After schema changes in source data</p>"},{"location":"user-guide/metadata/#metadata-file-size","title":"Metadata File Size","text":"<p>The metadata files size depends on: - Number of files in the dataset - Complexity of the schema - Number of row groups across all files</p> <p>For very large datasets (1000+ files): - Use partitioning to organize data logically - Consider periodic compaction to reduce file count - The <code>_file_metadata</code> uses brotli compression for efficiency</p>"},{"location":"user-guide/metadata/#storage-efficiency","title":"Storage Efficiency","text":"<pre><code># Metadata storage is optimized:\n# - _metadata: PyArrow native format\n# - _file_metadata: brotli compressed + base64 encoded\n\n# Check metadata file sizes\nimport os\n\nmetadata_size = os.path.getsize(dataset.metadata_file)\nfile_metadata_size = os.path.getsize(dataset.file_metadata_file)\n\nprint(f\"_metadata: {metadata_size / 1024:.1f} KB\")\nprint(f\"_file_metadata: {file_metadata_size / 1024:.1f} KB\")\n</code></pre>"},{"location":"user-guide/metadata/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/metadata/#missing-metadata-files","title":"Missing Metadata Files","text":"<pre><code># Check metadata files exist\nif not dataset.has_metadata_file:\n    print(\"No _metadata file found\")\n    # Update will create both files\n    dataset.update(verbose=True)\n\nif not dataset.has_file_metadata_file:\n    print(\"No _file_metadata file found\")\n    dataset.update(update_metadata=False)\n</code></pre>"},{"location":"user-guide/metadata/#schema-inconsistencies","title":"Schema Inconsistencies","text":"<pre><code># Schema repair is automatic during update\ntry:\n    # This will trigger schema repair if needed\n    dataset.update(repair=True, verbose=True)\nexcept Exception as e:\n    print(f\"Schema repair issue: {e}\")\n</code></pre>"},{"location":"user-guide/metadata/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Monitor metadata operations\nimport time\n\nstart = time.time()\ndataset.update()\nupdate_time = time.time() - start\n\nprint(f\"Metadata update took {update_time:.2f} seconds\")\nprint(f\"Dataset has {dataset.num_files} files\")\n</code></pre>"},{"location":"user-guide/metadata/#best-practices","title":"Best Practices","text":"<ol> <li>Let PyDala2 manage metadata automatically - It creates and updates metadata files during write operations</li> <li>Use <code>dataset.update()</code> after manual file changes - Keeps metadata synchronized</li> <li>Leverage the <code>scan()</code> method for file-based filtering before reading data</li> <li>Use appropriate partitioning - Reduces metadata size and improves query performance</li> <li>Monitor metadata file sizes - Large datasets benefit from compaction</li> <li>Use SQL queries on metadata - DuckDB integration provides powerful analysis capabilities</li> </ol>"},{"location":"user-guide/metadata/#example-complete-metadata-workflow","title":"Example: Complete Metadata Workflow","text":"<pre><code>from pydala import ParquetDataset\nimport pandas as pd\nimport numpy as np\nimport time\n\n# 1. Create dataset - metadata management is automatic\ndataset = ParquetDataset(\"data/sales\")\n\n# 2. Write data - metadata files are created automatically\ndata = pd.DataFrame({\n    'id': range(1000000),\n    'date': pd.date_range('2023-01-01', periods=1000000, freq='H'),\n    'amount': np.random.randn(1000000) * 100 + 50,\n    'category': np.random.choice(['A', 'B', 'C'], 1000000),\n    'status': np.random.choice(['active', 'completed', 'pending'], 1000000)\n})\n\n# Write with partitioning\ndataset.write_to_dataset(\n    data=data,\n    partition_cols=['category', 'status']\n)\n\n# 3. Verify metadata exists and check dataset info\nprint(f\"Dataset info:\")\nprint(f\"  Rows: {dataset.num_rows:,}\")\nprint(f\"  Files: {dataset.num_files}\")\nprint(f\"  Size: {dataset.size_bytes / 1024 / 1024:.1f} MB\")\nprint(f\"  Has metadata: {dataset.has_metadata_file}\")\n\n# 4. Use metadata-based file scanning\nprint(\"\\nScanning for files with specific criteria:\")\nstart = time.time()\nmatching_files = dataset.scan(\n    filters={\n        'date': {'min': '2023-06-01', 'max': '2023-06-30'},\n        'amount': {'min': 100},\n        'status': ['completed']\n    },\n    verbose=True\n)\nscan_time = time.time() - start\n\nprint(f\"Found {len(matching_files)} files with matching data in {scan_time:.3f}s\")\n\n# 5. Query using metadata optimization\nprint(\"\\nExecuting optimized query:\")\nstart = time.time()\nresult = dataset.filter(\n    \"date &gt;= '2023-06-01' AND status = 'completed' AND amount &gt; 100\"\n).table.to_polars()\nquery_time = time.time() - start\n\nprint(f\"Query completed in {query_time:.2f} seconds\")\nprint(f\"Returned {len(result)} rows\")\n\n# 6. Query metadata statistics using SQL\nprint(\"\\nMetadata statistics:\")\nstats = dataset.query_metadata(\"\"\"\n    SELECT\n        status,\n        SUM(num_rows) as total_rows,\n        AVG(amount) as avg_amount,\n        MIN(amount) as min_amount,\n        MAX(amount) as max_amount\n    FROM metadata_table\n    GROUP BY status\n    ORDER BY total_rows DESC\n\"\"\").df()\nprint(stats)\n\n# 7. After compaction, update metadata\nprint(\"\\nAfter compaction:\")\n# dataset.optimize.compact_partitions()  # This would be your actual compaction\ndataset.update(verbose=True)  # Update metadata after operations\nprint(f\"Updated dataset has {dataset.num_files} files\")\n</code></pre>"},{"location":"user-guide/performance/","title":"Performance Optimization","text":"<p>This guide covers techniques and best practices for optimizing PyDala2 performance based on the actual features available in the library.</p>"},{"location":"user-guide/performance/#understanding-performance-factors","title":"Understanding Performance Factors","text":"<p>PyDala2's performance depends on several factors:</p> <ol> <li>Data Layout: How your data is partitioned and organized</li> <li>File Format: Parquet compression and encoding settings</li> <li>Caching: Built-in filesystem caching for remote storage</li> <li>Query Patterns: How you filter and access data</li> <li>Automatic Backend Selection: Letting PyDala2 choose the optimal engine</li> </ol>"},{"location":"user-guide/performance/#partitioning-strategies","title":"Partitioning Strategies","text":""},{"location":"user-guide/performance/#effective-partitioning-with-write_to_dataset","title":"Effective Partitioning with write_to_dataset","text":"<pre><code>from pydala import ParquetDataset\nimport polars as pl\n\ndataset = ParquetDataset(\"data/sales\")\n\n# Good partitioning - low cardinality, frequently filtered\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"year\", \"month\", \"region\"],\n    max_rows_per_file=1_000_000\n)\n\n# Avoid high-cardinality partitioning\n# Bad: partition_by=[\"user_id\", \"timestamp\"]\n</code></pre>"},{"location":"user-guide/performance/#time-based-partitioning","title":"Time-based Partitioning","text":"<pre><code># Time-based partitioning (common pattern)\ndataset.write_to_dataset(\n    time_series_data,\n    partition_by=[\"year\", \"month\", \"day\"],\n    timestamp_column=\"created_at\",\n    max_rows_per_file=500_000\n)\n\n# Hierarchical partitioning\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"region\", \"category\"],\n    sort_by=\"date DESC\"\n)\n</code></pre>"},{"location":"user-guide/performance/#analyzing-partitions","title":"Analyzing Partitions","text":"<pre><code># Check partition information\nprint(f\"Partition columns: {dataset.partition_names}\")\nprint(f\"Partition values: {dataset.partition_values}\")\n\n# Access partitions data\nif dataset.partitions is not None:\n    print(f\"Number of partitions: {len(dataset.partitions)}\")\n</code></pre>"},{"location":"user-guide/performance/#compaction-operations","title":"Compaction Operations","text":""},{"location":"user-guide/performance/#compacting-small-files","title":"Compacting Small Files","text":"<pre><code># Compact partitions with multiple small files\ndataset.compact_partitions(\n    max_rows_per_file=1_000_000,\n    sort_by=\"date DESC\",\n    compression=\"zstd\",\n    row_group_size=100_000,\n    unique=True  # Remove duplicates\n)\n\n# Compact by time period (useful for time series)\ndataset.compact_by_timeperiod(\n    interval=\"1 month\",  # Can be string or timedelta\n    timestamp_column=\"created_at\",\n    max_rows_per_file=500_000,\n    sort_by=\"created_at\"\n)\n\n# Compact based on row count\ndataset.compact_by_rows(\n    max_rows_per_file=2_000_000,\n    sort_by=\"id\",\n    unique=False\n)\n</code></pre>"},{"location":"user-guide/performance/#repartitioning-data","title":"Repartitioning Data","text":"<pre><code># Change partitioning scheme\ndataset.repartition(\n    partitioning_columns=[\"year\", \"quarter\", \"region\"],\n    max_rows_per_file=1_000_000,\n    sort_by=[\"region\", \"date\"],\n    compression=\"zstd\",\n    unique=True\n)\n</code></pre>"},{"location":"user-guide/performance/#data-type-optimization","title":"Data Type Optimization","text":""},{"location":"user-guide/performance/#automatic-type-optimization","title":"Automatic Type Optimization","text":"<pre><code># Optimize data types to reduce storage size\ndataset.optimize_dtypes(\n    exclude=[\"id\"],  # Don't optimize ID columns\n    strict=True,    # Use strict type inference\n    include=None,   # Or specify columns to include\n    ts_unit=\"ms\",   # Convert timestamps to milliseconds\n    tz=\"UTC\"        # Set timezone for timestamp columns\n)\n\n# This automatically:\n# - Converts int64 to int32/int16 where possible\n# - Converts float64 to float32 where precision allows\n# - Converts strings to categorical for low cardinality\n# - Optimizes timestamp precision\n</code></pre>"},{"location":"user-guide/performance/#manual-type-optimization","title":"Manual Type Optimization","text":"<pre><code># Get current schema\nprint(f\"Current schema: {dataset.schema}\")\n\n# Write with optimized types\ndata = data.with_columns([\n    pl.col(\"price\").cast(pl.Float32),\n    pl.col(\"quantity\").cast(pl.Int32),\n    pl.col(\"category\").cast(pl.Categorical),\n    pl.col(\"timestamp\").dt.convert_time_zone(\"UTC\")\n])\n\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"category\"],\n    max_rows_per_file=1_000_000,\n    compression=\"zstd\"\n)\n</code></pre>"},{"location":"user-guide/performance/#write-performance-optimization","title":"Write Performance Optimization","text":""},{"location":"user-guide/performance/#optimal-write-settings","title":"Optimal Write Settings","text":"<pre><code># Write with performance settings\ndataset.write_to_dataset(\n    data,\n    partition_by=[\"region\", \"category\"],\n    max_rows_per_file=1_000_000,    # Target 1M rows per file\n    row_group_size=250_000,        # 250K rows per row group\n    compression=\"zstd\",            # Best compression ratio\n    sort_by=\"date DESC\",           # Pre-sort for query performance\n    unique=True                    # Remove duplicates\n)\n</code></pre>"},{"location":"user-guide/performance/#compression-comparison","title":"Compression Comparison","text":"<pre><code>import time\nimport os\n\n# Test different compression settings\ncompressions = [\"snappy\", \"gzip\", \"zstd\", \"brotli\"]\n\nfor comp in compressions:\n    test_dataset = ParquetDataset(f\"data/test_{comp}\")\n\n    start = time.time()\n    test_dataset.write_to_dataset(\n        data,\n        compression=comp,\n        max_rows_per_file=100_000\n    )\n    write_time = time.time() - start\n\n    # Read performance\n    start = time.time()\n    df = test_dataset.table.pl.collect()\n    read_time = time.time() - start\n\n    # Calculate size\n    size = sum(\n        os.path.getsize(os.path.join(test_dataset.path, f))\n        for f in test_dataset.files\n    )\n\n    print(f\"{comp:10} | Size: {size/1024/1024:6.1f} MB | \"\n          f\"Write: {write_time:4.2f}s | Read: {read_time:4.2f}s\")\n\n    # Clean up\n    test_dataset.vacuum()\n</code></pre>"},{"location":"user-guide/performance/#caching-for-remote-storage","title":"Caching for Remote Storage","text":""},{"location":"user-guide/performance/#enabling-caching","title":"Enabling Caching","text":"<pre><code># Enable caching for remote filesystems\ns3_dataset = ParquetDataset(\n    \"s3://my-bucket/data\",\n    bucket=\"my-bucket\",\n    key=\"your-access-key\",\n    secret=\"your-secret-key\",\n    cached=True,  # Enable caching\n    cache_storage=\"/tmp/pydala_cache\"  # Local cache directory\n)\n\n# Cache is automatically used for reads\ndata = s3_dataset.table.pl.collect()  # First read - from S3\ndata = s3_dataset.table.pl.collect()  # Subsequent reads - from cache\n</code></pre>"},{"location":"user-guide/performance/#cache-management","title":"Cache Management","text":"<pre><code># Clear cache (useful when data changes)\ndataset.clear_cache()\n\n# Cache is automatically managed:\n# - Files are cached on first read\n# - Cache respects file modifications\n# - Cache size depends on available disk space\n</code></pre>"},{"location":"user-guide/performance/#query-optimization","title":"Query Optimization","text":""},{"location":"user-guide/performance/#automatic-backend-selection","title":"Automatic Backend Selection","text":"<p>PyDala2 automatically chooses the best backend for your queries:</p> <pre><code># Simple filters use PyArrow (fast scanning)\nsimple_result = dataset.filter(\"region = 'US'\")\n\n# Complex filters automatically use DuckDB\ncomplex_result = dataset.filter(\"\"\"\n    region IN ('US', 'EU')\n    AND amount &gt; 1000\n    AND customer_id IN (SELECT id FROM premium_customers)\n\"\"\")\n\n# Both return PydalaTable objects\ndf = simple_result.collect()  # Polars DataFrame\n</code></pre>"},{"location":"user-guide/performance/#filter-pushdown","title":"Filter Pushdown","text":"<pre><code># Filters are automatically pushed down when possible\n# Only reads relevant partitions and rows\nresult = dataset.filter(\"\"\"\n    year = 2023\n    AND month IN (1, 2, 3)\n    AND amount &gt; 100\n\"\"\")\n\n# For partitioned datasets, this automatically prunes partitions\n</code></pre>"},{"location":"user-guide/performance/#column-pruning","title":"Column Pruning","text":"<pre><code># Only select needed columns\nresult = dataset.ddb_con.sql(\"\"\"\n    SELECT id, name, email, amount\n    FROM dataset\n    WHERE date &gt;= '2023-01-01'\n\"\"\").pl()  # Convert to Polars\n</code></pre>"},{"location":"user-guide/performance/#using-duckdb-for-complex-queries","title":"Using DuckDB for Complex Queries","text":"<pre><code># Complex aggregations\nagg_result = dataset.ddb_con.sql(\"\"\"\n    SELECT\n        region,\n        category,\n        COUNT(*) as order_count,\n        SUM(amount) as total_revenue,\n        AVG(amount) as avg_order,\n        MIN(amount) as min_order,\n        MAX(amount) as max_order\n    FROM dataset\n    WHERE date &gt;= '2023-01-01'\n    GROUP BY region, category\n    HAVING COUNT(*) &gt; 100\n    ORDER BY total_revenue DESC\n\"\"\").pl()\n\n# Window functions\nwindow_result = dataset.ddb_con.sql(\"\"\"\n    SELECT\n        order_id,\n        customer_id,\n        order_date,\n        amount,\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id\n            ORDER BY order_date\n        ) as order_number,\n        LAG(amount, 1) OVER (\n            PARTITION BY customer_id\n            ORDER BY order_date\n        ) as prev_amount\n    FROM dataset\n\"\"\").pl()\n</code></pre>"},{"location":"user-guide/performance/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"user-guide/performance/#using-explain","title":"Using EXPLAIN","text":"<pre><code># Get query execution plan\nplan = dataset.ddb_con.sql(\"\"\"\n    EXPLAIN SELECT category, AVG(amount)\n    FROM dataset\n    WHERE date &gt;= '2023-01-01'\n    GROUP BY category\n\"\"\").fetchall()\n\nprint(\"Execution plan:\")\nfor row in plan:\n    print(row[0])\n</code></pre>"},{"location":"user-guide/performance/#dataset-statistics","title":"Dataset Statistics","text":"<pre><code># Basic statistics\nprint(f\"Total rows: {dataset.count_rows()}\")\nprint(f\"Number of files: {len(dataset.files)}\")\nprint(f\"Schema: {dataset.schema}\")\n\n# For partitioned datasets\nif dataset.partitions is not None:\n    print(f\"Partitions: {dataset.partitions}\")\n</code></pre>"},{"location":"user-guide/performance/#memory-management","title":"Memory Management","text":"<pre><code># Process large datasets in chunks using DuckDB\nbatch_size = 100000\noffset = 0\n\nwhile True:\n    batch = dataset.ddb_con.sql(f\"\"\"\n        SELECT * FROM dataset\n        ORDER BY id\n        LIMIT {batch_size} OFFSET {offset}\n    \"\"\").pl()\n\n    if len(batch) == 0:\n        break\n\n    process_batch(batch)\n    offset += batch_size\n\n# Clear memory\ndel batch\n</code></pre>"},{"location":"user-guide/performance/#common-performance-patterns","title":"Common Performance Patterns","text":""},{"location":"user-guide/performance/#time-series-optimization","title":"Time Series Optimization","text":"<pre><code># Optimal time series layout\ntime_dataset = ParquetDataset(\"data/metrics\")\ntime_dataset.write_to_dataset(\n    metrics_data,\n    partition_by=[\"metric_name\", \"year\", \"month\", \"day\"],\n    timestamp_column=\"timestamp\",\n    max_rows_per_file=86400,  # One day of minute data\n    sort_by=\"timestamp\"\n)\n\n# Fast time range queries with automatic partition pruning\nresult = time_dataset.filter(\"\"\"\n    metric_name = 'cpu_usage'\n    AND timestamp BETWEEN '2023-01-01' AND '2023-01-31'\n\"\"\")\n\n# Compact old time series data\ntime_dataset.compact_by_timeperiod(\n    interval=\"1 month\",\n    timestamp_column=\"timestamp\",\n    max_rows_per_file=5_000_000\n)\n</code></pre>"},{"location":"user-guide/performance/#large-dataset-processing","title":"Large Dataset Processing","text":"<pre><code># Write in batches for very large datasets\ndef write_large_dataset(dataset, large_df, batch_size=1_000_000):\n    \"\"\"Write large DataFrame in batches\"\"\"\n    total_rows = len(large_df)\n\n    for i in range(0, total_rows, batch_size):\n        batch = large_df.slice(i, batch_size)\n\n        dataset.write_to_dataset(\n            batch,\n            mode=\"append\",\n            partition_by=[\"batch_id\", \"date\"],\n            update_metadata=(i + batch_size &gt;= total_rows)\n        )\n\n        print(f\"Written batch {i//batch_size + 1}\")\n\n# Usage\nwrite_large_dataset(dataset, very_large_dataframe)\n</code></pre>"},{"location":"user-guide/performance/#schema-evolution-performance","title":"Schema Evolution Performance","text":"<pre><code># Add new columns efficiently\nnew_data = existing_data.with_columns([\n    (pl.col(\"amount\") * 0.1).alias(\"tax\"),\n    (pl.col(\"amount\") * 1.1).alias(\"total_with_tax\")\n])\n\n# Use alter_schema for schema evolution\ndataset.write_to_dataset(\n    new_data,\n    mode=\"append\",\n    alter_schema=True,\n    partition_by=[\"category\"]\n)\n</code></pre>"},{"location":"user-guide/performance/#performance-checklist","title":"Performance Checklist","text":"<ul> <li> Partition on low-to-medium cardinality columns (100-1000 values)</li> <li> Use appropriate compression (zstd recommended for best ratio)</li> <li> Set optimal file sizes (1M rows per file typical)</li> <li> Configure row group size (250K rows per group typical)</li> <li> Enable caching for remote filesystems</li> <li> Use <code>compact_partitions()</code> for many small files</li> <li> Run <code>optimize_dtypes()</code> to reduce storage</li> <li> Pre-sort data on frequently queried columns</li> <li> Let PyDala2 automatically select backends</li> <li> Use partition pruning in filters</li> <li> Monitor file sizes and compact regularly</li> <li> Use time-based compaction for time series data</li> </ul>"},{"location":"user-guide/schema-management/","title":"Schema Management","text":"<p>PyDala2 provides comprehensive schema management capabilities to ensure data consistency and enable efficient operations.</p>"},{"location":"user-guide/schema-management/#understanding-schemas-in-pydala2","title":"Understanding Schemas in PyDala2","text":"<p>A schema defines the structure of your data including: - Column names and data types - Nullability constraints - Column metadata and descriptions - Validation rules</p> <pre><code>from pydala2 import ParquetDataset\n\ndataset = ParquetDataset(\"data/sales\")\n\n# View the schema\nschema = dataset.schema\nprint(schema)\n</code></pre>"},{"location":"user-guide/schema-management/#schema-basics","title":"Schema Basics","text":""},{"location":"user-guide/schema-management/#viewing-schema-information","title":"Viewing Schema Information","text":"<pre><code># Get basic schema info\nschema = dataset.schema\n\n# Print column details\nfor field in schema:\n    print(f\"Column: {field.name}\")\n    print(f\"  Type: {field.type}\")\n    print(f\"  Nullable: {field.nullable}\")\n    print(f\"  Metadata: {field.metadata}\")\n    print()\n</code></pre>"},{"location":"user-guide/schema-management/#schema-types","title":"Schema Types","text":"<p>PyDala2 supports all Arrow data types:</p> <pre><code># Numeric types\n'int32', 'int64', 'float32', 'float64', 'decimal128'\n\n# String and binary\n'string', 'binary', 'large_string', 'large_binary'\n\n# Date and time\n'date32', 'date64', 'timestamp', 'time32', 'time64'\n\n# List and struct types\n'list', 'struct', 'large_list'\n\n# Other\n'boolean', 'null'\n</code></pre>"},{"location":"user-guide/schema-management/#schema-operations","title":"Schema Operations","text":"<pre><code># Check if column exists\nhas_id = 'id' in schema.names\n\n# Get column by name\nid_field = schema.field('id')\n\n# Get column type\namount_type = schema.field('amount').type\n\n# Schema size\nnum_columns = len(schema)\n</code></pre>"},{"location":"user-guide/schema-management/#schema-validation","title":"Schema Validation","text":""},{"location":"user-guide/schema-management/#basic-validation","title":"Basic Validation","text":"<pre><code># Validate data against schema\nvalidation_result = dataset.validate_schema()\n\nif validation_result.valid:\n    print(\"Schema is valid\")\nelse:\n    print(\"Schema validation failed:\")\n    for error in validation_result.errors:\n        print(f\"  - {error}\")\n</code></pre>"},{"location":"user-guide/schema-management/#custom-validation-rules","title":"Custom Validation Rules","text":"<pre><code># Define custom validation rules\nrules = {\n    'id': {'required': True, 'type': 'int64'},\n    'email': {'required': True, 'pattern': r'^[^@]+@[^@]+\\.[^@]+$'},\n    'amount': {'required': True, 'min': 0, 'max': 1000000},\n    'status': {'allowed_values': ['active', 'inactive', 'pending']}\n}\n\n# Validate with custom rules\nresult = dataset.validate_data(rules=rules)\n</code></pre>"},{"location":"user-guide/schema-management/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code># Comprehensive data quality validation\nquality_report = dataset.data_quality_check()\n\nprint(f\"Completeness: {quality_report.completeness:.2%}\")\nprint(f\"Consistency: {quality_report.consistency:.2%}\")\nprint(f\"Validity: {quality_report.validity:.2%}\")\n\n# Issues found\nfor issue in quality_report.issues:\n    print(f\"Row {issue.row}: {issue.message}\")\n</code></pre>"},{"location":"user-guide/schema-management/#schema-evolution","title":"Schema Evolution","text":""},{"location":"user-guide/schema-management/#adding-columns","title":"Adding Columns","text":"<pre><code># Add new column to existing dataset\nimport pandas as pd\n\n# Read existing data\ndata = dataset.table.to_pandas()\n\n# Add new column\ndata['discount_percent'] = 0.0\n\n# Update schema metadata\ndataset.update_schema_metadata({\n    'discount_percent': {\n        'description': 'Discount percentage applied',\n        'unit': 'percentage',\n        'default_value': 0.0\n    }\n})\n\n# Write back\ndataset.write(data, mode='overwrite')\n</code></pre>"},{"location":"user-guide/schema-management/#modifying-column-types","title":"Modifying Column Types","text":"<pre><code># Safe type conversion\ndef convert_column_safely(df, column, new_type):\n    try:\n        df[column] = df[column].astype(new_type)\n        return df\n    except Exception as e:\n        print(f\"Cannot convert {column} to {new_type}: {e}\")\n        return None\n\n# Apply conversion\ndata = dataset.table.to_pandas()\ndata = convert_column_safely(data, 'user_id', 'string')\nif data is not None:\n    dataset.write(data, mode='overwrite')\n</code></pre>"},{"location":"user-guide/schema-management/#schema-versioning","title":"Schema Versioning","text":"<pre><code># Track schema versions\ndataset.add_schema_version({\n    'version': '1.1',\n    'changes': ['Added discount_percent column'],\n    'author': 'data_team',\n    'date': '2023-12-01'\n})\n\n# List schema versions\nversions = dataset.list_schema_versions()\nfor v in versions:\n    print(f\"Version {v.version}: {v.changes}\")\n</code></pre>"},{"location":"user-guide/schema-management/#schema-metadata","title":"Schema Metadata","text":""},{"location":"user-guide/schema-management/#column-documentation","title":"Column Documentation","text":"<pre><code># Add descriptive metadata\nschema_metadata = {\n    'id': {\n        'description': 'Unique identifier for the record',\n        'type': 'integer',\n        'primary_key': True\n    },\n    'customer_id': {\n        'description': 'Foreign key to customers table',\n        'type': 'integer',\n        'references': 'customers.id'\n    },\n    'order_date': {\n        'description': 'Date when order was placed',\n        'type': 'datetime',\n        'format': 'YYYY-MM-DD'\n    },\n    'total_amount': {\n        'description': 'Total order amount including tax',\n        'type': 'decimal',\n        'precision': 10,\n        'scale': 2,\n        'currency': 'USD'\n    }\n}\n\ndataset.update_schema_metadata(schema_metadata)\n</code></pre>"},{"location":"user-guide/schema-management/#business-rules","title":"Business Rules","text":"<pre><code># Define business rules in schema\nbusiness_rules = {\n    'order_status': {\n        'allowed_values': ['pending', 'confirmed', 'shipped', 'delivered', 'cancelled'],\n        'default': 'pending'\n    },\n    'quantity': {\n        'min': 1,\n        'max': 1000,\n        'unit': 'items'\n    },\n    'email': {\n        'validation': 'email_format',\n        'required': True\n    }\n}\n\ndataset.set_business_rules(business_rules)\n</code></pre>"},{"location":"user-guide/schema-management/#data-classification","title":"Data Classification","text":"<pre><code># Classify data sensitivity\nclassification = {\n    'ssn': {'classification': 'PII', 'access_level': 'restricted'},\n    'email': {'classification': 'PII', 'access_level': 'internal'},\n    'name': {'classification': 'personal', 'access_level': 'internal'},\n    'amount': {'classification': 'financial', 'access_level': 'confidential'},\n    'ip_address': {'classification': 'technical', 'access_level': 'internal'}\n}\n\ndataset.set_data_classification(classification)\n</code></pre>"},{"location":"user-guide/schema-management/#schema-inference","title":"Schema Inference","text":""},{"location":"user-guide/schema-management/#automatic-schema-detection","title":"Automatic Schema Detection","text":"<pre><code># Infer schema from CSV file\nfrom pydala2 import infer_schema\n\nschema = infer_schema(\"data.csv\")\nprint(schema)\n\n# Infer from sample data\nimport pandas as pd\nsample_data = pd.read_csv(\"sample.csv\", nrows=1000)\nschema = infer_schema(sample_data)\n</code></pre>"},{"location":"user-guide/schema-management/#schema-recommendations","title":"Schema Recommendations","text":"<pre><code># Get schema optimization recommendations\nrecommendations = dataset.get_schema_recommendations()\n\nfor rec in recommendations:\n    print(f\"Column {rec.column}:\")\n    print(f\"  Current type: {rec.current_type}\")\n    print(f\"  Recommended type: {rec.recommended_type}\")\n    print(f\"  Reason: {rec.reason}\")\n    print(f\"  Impact: {rec.impact}\")\n    print()\n</code></pre>"},{"location":"user-guide/schema-management/#schema-drift-detection","title":"Schema Drift Detection","text":"<pre><code># Detect schema changes over time\ndrift_report = dataset.detect_schema_drift()\n\nif drift_report.has_drift:\n    print(\"Schema drift detected:\")\n    for change in drift_report.changes:\n        print(f\"  {change.column}: {change.old_type} -&gt; {change.new_type}\")\n        print(f\"    First seen: {change.first_seen}\")\n        print(f\"    Frequency: {change.frequency}\")\n</code></pre>"},{"location":"user-guide/schema-management/#schema-migration","title":"Schema Migration","text":""},{"location":"user-guide/schema-management/#safe-migration-process","title":"Safe Migration Process","text":"<pre><code>def safe_schema_migration(dataset, migration_func):\n    \"\"\"Safely migrate schema with backup\"\"\"\n\n    # 1. Backup current data\n    backup_path = f\"{dataset.path}_backup_{int(time.time())}\"\n    backup = ParquetDataset(backup_path)\n    backup.write(dataset.table.to_pandas())\n\n    try:\n        # 2. Apply migration\n        data = dataset.table.to_pandas()\n        data = migration_func(data)\n\n        # 3. Validate\n        if validate_migrated_data(data):\n            # 4. Write back\n            dataset.write(data, mode='overwrite')\n            print(\"Migration successful\")\n            return True\n        else:\n            raise ValueError(\"Validation failed\")\n\n    except Exception as e:\n        # 5. Restore from backup\n        print(f\"Migration failed: {e}\")\n        print(\"Restoring from backup...\")\n        dataset.write(backup.table.to_pandas(), mode='overwrite')\n        return False\n</code></pre>"},{"location":"user-guide/schema-management/#common-migrations","title":"Common Migrations","text":"<pre><code># Example migration: Normalize date formats\ndef normalize_dates(df):\n    df['order_date'] = pd.to_datetime(df['order_date']).dt.normalize()\n    return df\n\n# Execute migration\nsuccess = safe_schema_migration(dataset, normalize_dates)\n</code></pre>"},{"location":"user-guide/schema-management/#schema-patterns","title":"Schema Patterns","text":""},{"location":"user-guide/schema-management/#star-schema","title":"Star Schema","text":"<pre><code># Fact table schema\nfact_schema = {\n    'order_id': 'int64',\n    'customer_id': 'int64',\n    'product_id': 'int64',\n    'date_id': 'int32',\n    'quantity': 'int32',\n    'unit_price': 'decimal(10,2)',\n    'total_amount': 'decimal(10,2)',\n    'discount_amount': 'decimal(10,2)'\n}\n\n# Dimension table schemas\ndate_schema = {\n    'date_id': 'int32',\n    'date': 'date32',\n    'day': 'int32',\n    'month': 'int32',\n    'year': 'int32',\n    'quarter': 'int32',\n    'day_of_week': 'int32'\n}\n</code></pre>"},{"location":"user-guide/schema-management/#slowly-changing-dimensions","title":"Slowly Changing Dimensions","text":"<pre><code># Type 2 SCD\ncustomer_schema = {\n    'customer_id': 'int64',\n    'version': 'int32',\n    'name': 'string',\n    'email': 'string',\n    'address': 'string',\n    'valid_from': 'timestamp',\n    'valid_to': 'timestamp',\n    'current_flag': 'boolean'\n}\n</code></pre>"},{"location":"user-guide/schema-management/#nested-data","title":"Nested Data","text":"<pre><code># Schema for nested JSON\nnested_schema = {\n    'order_id': 'int64',\n    'customer': {\n        'id': 'int64',\n        'name': 'string',\n        'address': {\n            'street': 'string',\n            'city': 'string',\n            'country': 'string'\n        }\n    },\n    'items': [\n        {\n            'product_id': 'int64',\n            'name': 'string',\n            'quantity': 'int32',\n            'price': 'decimal(10,2)'\n        }\n    ]\n}\n</code></pre>"},{"location":"user-guide/schema-management/#schema-utilities","title":"Schema Utilities","text":""},{"location":"user-guide/schema-management/#schema-comparison","title":"Schema Comparison","text":"<pre><code># Compare two schemas\ndef compare_schemas(schema1, schema2):\n    added = []\n    removed = []\n    changed = []\n\n    fields1 = {f.name: f for f in schema1}\n    fields2 = {f.name: f for f in schema2}\n\n    # Check for added fields\n    for name in fields2:\n        if name not in fields1:\n            added.append(name)\n\n    # Check for removed fields\n    for name in fields1:\n        if name not in fields2:\n            removed.append(name)\n\n    # Check for type changes\n    for name in fields1:\n        if name in fields2 and fields1[name].type != fields2[name].type:\n            changed.append((name, str(fields1[name].type), str(fields2[name].type)))\n\n    return {\n        'added': added,\n        'removed': removed,\n        'changed': changed\n    }\n\n# Usage\ncomparison = compare_schemas(old_schema, new_schema)\n</code></pre>"},{"location":"user-guide/schema-management/#schema-documentation","title":"Schema Documentation","text":"<pre><code># Generate schema documentation\ndef generate_schema_doc(schema):\n    doc = \"# Schema Documentation\\n\\n\"\n    doc += \"| Column | Type | Nullable | Description |\\n\"\n    doc += \"|--------|------|----------|-------------|\\n\"\n\n    for field in schema:\n        doc += f\"| {field.name} | {field.type} | {field.nullable} | \"\n        doc += f\"{field.metadata.get('description', '')} |\\n\"\n\n    return doc\n\n# Save to file\nwith open(\"schema.md\", \"w\") as f:\n    f.write(generate_schema_doc(dataset.schema))\n</code></pre>"},{"location":"user-guide/schema-management/#best-practices","title":"Best Practices","text":"<ol> <li>Use consistent naming conventions across all datasets</li> <li>Document columns with clear descriptions and business context</li> <li>Validate data before writing to ensure schema compliance</li> <li>Version schemas when making breaking changes</li> <li>Use appropriate data types for your use case</li> <li>Plan for schema evolution from the start</li> <li>Monitor schema drift in automated pipelines</li> <li>Back up data before schema migrations</li> <li>Test migrations in staging environments first</li> <li>Communicate changes to downstream consumers</li> </ol>"}]}